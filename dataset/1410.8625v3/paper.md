# Convergence of Bregman alternating direction method with multipliers for nonconvex composite problems

**arXiv ID:** 1410.8625v3

**Authors:** Fenghui Wang, Zongben Xu, Hong-Kun Xu

**Abstract:** The alternating direction method with multipliers (ADMM) has been one of most powerful and successful methods for solving various convex or nonconvex composite problems that arise in the fields of image & signal processing and machine learning. In convex settings, numerous convergence results have been established for ADMM as well as its varieties. However, due to the absence of convexity, the convergence analysis of nonconvex ADMM is generally very difficult. In this paper we study the Bregman modification of ADMM (BADMM), which includes the conventional ADMM as a special case and often leads to an improvement of the performance of the algorithm. Under certain assumptions, we prove that the iterative sequence generated by BADMM converges to a stationary point of the associated augmented Lagrangian function. The obtained results underline the feasibility of ADMM in applications under nonconvex settings.

---

> **Note:** This text was extracted with pypdf (plain-text fallback). LaTeX formulas may be garbled. Install `marker-pdf` for better results.

<!-- page 1 -->
1
Convergence of Bregman Alternating Direction
Method with Multipliers
for Nonconvex Composite Problems
Fenghui Wang, Zongben Xu∗, and Hong-Kun Xu
Abstract
The alternating direction method with multipliers (ADMM) has been one of most powerful and successful methods for solving
various convex or nonconvex composite problems that arise in the ﬁelds of image & signal processing and machine learning. In
convex settings, numerous convergence results have been established for ADMM as well as its varieties. However, there have been
few studies on the convergence properties of ADMM under nonconvex frameworks, since the convergence analysis of nonconvex
algorithm is generally very difﬁcult. In this paper we study the Bregman modiﬁcation of ADMM (BADMM), which includes the
conventional ADMM as a special case and can signiﬁcantly improve the performance of the algorithm. Under some assumptions,
we show that the iterative sequence generated by BADMM converges to a stationary point of the associated augmented Lagrangian
function. The obtained results underline the feasibility of ADMM in applications under nonconvex settings.
Index Terms
nonconvex regularization, nonconvex sparse minimization, alternating direction method, sub-analytic function, K-L inequality,
Bregman distance.
I. I NTRODUCTION
Many problems arising in the ﬁelds of signal & image processing and machine learning [5], [28] involve ﬁnding a minimizer
of some composite objective functions. More speciﬁcally, such problems can be formulated as:
min f(x) + g(y)
s.t. Ax = By, (1)
where A∈ Rm×n1 and B∈ Rm×n2 are given matrices, f : Rn1→ R is usually a (quadratic, or logistic) loss function, and
g : Rn2→ R is often a regularizer such as the ℓ1 norm or ℓ1/2 quasi-norm.
Because of its separable structure, problem (1) can be efﬁciently solved by the alternating direction method with multipliers
(ADMM), which decomposes the original joint minimization problem into two easily solved subproblems. The standard ADMM
for problem (1) takes the form:
yk+1 = arg min
y∈Rn2
Lα(xk, y, pk) (2)
xk+1 = arg min
x∈Rn1
Lα(x, yk+1, pk) (3)
pk+1 = pk + α(Axk+1− By k+1), (4)
where α is a penalty parameter and
Lα(x, y, p) := f(x) + g(y) +⟨p, Ax− By⟩
+ α
2∥Ax− By∥2
is the associated augmented Lagrangian function with multiplier p. Generally speaking, ADMM is ﬁrst minimized with respect
to y for ﬁxed values of p, x, then with respect to x with p, y ﬁxed, and ﬁnally maximized with respect to p with x, y ﬁxed.
Updating the dual variable pk in the above system is a trivial task, but this is not so simple for the primal variables xk
and yk. Indeed in many cases, the x-subproblem (3) and y-subproblem (2) cannot easily be solved. Recently, the Bregman
∗corresponding author.
F. Wang is with School of Mathematics and Statistics, Xian Jiaotong University, Xian 710049, P R China
Z. Xu is with School of Mathematics and Statistics, Xian Jiaotong University, Xian 710049, P R China.
H.K. Xu is with Department of Applied Mathematics, National Sun Yat-sen University, Kaohsiung 80424, Taiwan.
arXiv:1410.8625v3  [math.OC]  5 Dec 2014

<!-- page 2 -->
2
modiﬁcation of ADMM (BADMM) has been adopted by several researchers to improve the performance of the conventional
ADMM algorithm [16], [35], [36], [47]. BADMM takes the following iterative form: 1
yk+1 = arg min
y∈Rn2
Lα(xk, y, pk) +△ψ(y, yk) (5)
xk+1 = arg min
x∈Rn1
Lα(x, yk+1, pk) +△φ(x, xk) (6)
pk+1 = pk + α(Axk+1− By k+1), (7)
where△ψ and△φ respectively denote the Bregman distance with respect to function ψ and φ. The difference between this
algorithm and the standard ADMM is that the objective function in (2)-(3) is replaced by the sum of a Bregman distance function
and the augmented Lagrangian function. Moreover, as shown in [26], [36], [47] and the following section, an appropriate choice
of Bregman distance does indeed simplify the original subproblems.
ADMM was introduced in the early 1970s [17], [18], and its convergence properties for convex objective functions have
been extensively studied. The convergence of ADMM was ﬁrst established for strongly convex functions [17], [18], before
being extended to general convex functions [13], [14]. It has been shown that ADMM converges at a sublinear rate of O(1/k)
[20], [30], or O(1/k2) for the accelerated version [19]; furthermore, a linear convergence rate was also shown under certain
additional assumptions [12]. The convergence of BADMM for convex objective functions has also been examined with the
Euclidean distance [10], Mahalanobis distance [47], and the general Bregman distance [47].
Recent studies on nonnegative matrix factorization, distributed matrix factorization, distributed clustering, sparse zero variance
discriminant analysis, polynomial optimization, tensor decomposition, and matrix completion have led to growing interest in
ADMM for nonconvex objective functions (see e.g. [21], [27], [38], [44], [46]). It has been shown that the nonconvex ADMM
works extremely well for these particular examples.
However, because the convergence analysis of nonconvex algorithms is generally very difﬁcult, there have been few studies
on the convergence properties of ADMM under nonconvex frameworks. One major difﬁculty is that the F ´ejer monotonicity
of iterative sequences does not hold in the absence of convexity. Very recently, [22] analyzed the convergence of ADMM for
certain nonconvex consensus and sharing problems. They demonstrated that with A and B set to the identity matrices, ADMM
converges to the set of stationary solutions as long as the penalty parameter α is sufﬁciently large. To show the convergence of
ADMM to a stationary point , additional assumptions are required on the functions involved. For example, if f and g are both
semi-algebraic, [26] proved that ADMM converges to a stationary point when B is the identity matrix. This result requires
that function f is strongly convex or matrix A has full-column rank.
In this paper, we study the convergencev of BADMM under nonconvex frameworks. First, we extend the convergence of the
BADMM from semi-algebraic functions to sub-analytic functions. In particular, this implies that BADMM is convergent for
logistic sparse loss functions, which are not semi-algebraic. Second, we establish a global convergence theorem for cases when
B has full-column rank. This allows us to choose φ≡ 0, which covers a recent result in [26]. We also study the case when
B does not have full-column rank. In this instance, a suitable Bregman distance also leads to global BADMM convergence.
This enhanced ﬂexibility of BADMM enables its application to more general cases. More importantly, the main idea of our
convergence analysis is different from that used in [26]. Instead of employing an augmented Lagrangian function at each
iteration, we demonstrate global convergence using the descent property of an auxiliary function.
The paper is organized as follows. In Section 2, we recall the deﬁnitions of subdifferentials, Bregman distance, and
Kurdyka-Łojasiewicz inequality. In Section 3, we establish the global convergence of BADMM to a critical point under
certain assumptions. In Section 4, we conduct experimental studies to verify the convergence of BADMM.
II. P RELIMINARIES
In what follows, Rn will stand for the n-dimensional Euclidean space,
⟨x, y⟩ = x⊤y =
n∑
i=1
xiyi, ∥x∥ =
√
⟨x, x⟩,
where x, y∈ Rn and⊤ stands for the transpose operation.
A. Subdifferentials
Given a function f : Rn→ R we denote by domf the domain of f, namely domf :={x∈ Rn : f(x) < +∞}. A function
f is said to be proper if domf̸=∅; lower semicontinuous at the point x0 if
lim inf
x→x0
f(x)≥ f(x0).
If f is lower semicontinuous at every point of its domain of deﬁnition, then it is simply called a lower semicontinuous function.
1If the solution to the x or y-subproblem is not unique, then xk or yk should be regarded as a selection from their solution sets.

<!-- page 3 -->
3
Deﬁnition II.1. Let f : Rn→ R be a proper lower semi-continuous function.
(i) Given x∈ domf, the Fr´echet subdifferential of f at x, written by ˆ∂f (x), is the set of all elements u∈ Rn which satisfy
lim
y̸=x
inf
y→x
f(y)− f(x)−⟨ u, y− x⟩
∥x− y∥ ≥ 0.
(ii) The limiting subdifferential, or simply subdifferential, of f at x, written by ∂f (x), is deﬁned as
∂f (x) ={u∈ Rn :∃xk→ x, f(xk)→ f(x),
uk∈ ˆ∂f (xk)→ u, k→∞} .
(iii) A critical point or stationary point of f is a point x0 in the domain of f satisfying 0∈ ∂f (x0).
Deﬁnition II.2. An element z∗ := (x∗, y∗, p∗) is called a critical point or stationary point of the Lagrangian function Lα if
it satisﬁes:



−A⊤p∗ =∇f(x∗)
B⊤p∗∈ ∂g(y∗)
Ax∗ = By∗.
(8)
Let us now collect some basic properties of the subdifferential (see [31]).
Proposition II.1. Let f : Rn→ R and g : Rn→ R be proper lower semi-continuous functions.
• ˆ∂f (x)⊂ ∂f (x) for each x∈ Rn. Moreover, the ﬁrst set is closed and convex, while the second is closed, and not
necessarily convex.
• Let (uk, xk) be sequences such that xk→ x, uk→ u, f(xk)→ f(x) and uk∈ ∂f (xk). Then by the deﬁnition of the
subdifferential, we have u∈ ∂f (x).
• The Fermat’s rule remains true: if x0∈ Rn is a local minimizer of f, then x0 is a critical point or stationary point of
f, that is, 0∈ ∂f (x0).
• If f is continuously differentiable function, then ∂(f + g)(x) =∇f(x) + ∂g(x).
A function f is said to be ℓf -Lipschitz continuous (ℓf≥ 0) if
∥f(x)− f(y)∥≤ ℓf∥x− y∥
for any x, y∈ domf; µ-strongly convex (µ > 0) if
f(y)≥ f(x) +⟨ξ(x), y− x⟩ + µ
2∥y− x∥2, (9)
for any x, y∈ domf and ξ(x)∈ ∂f (x).
B. Kurdyka-Łojasiewicz inequality
The Kurdyka-Łojasiewicz (K-L) inequality plays an important role in our subsequent analysis. This inequality was ﬁrst
introduced by Łojasiewicz [32] for real real analytic functions, and then was extended by Kurdyka [24] to smooth functions
whose graph belongs to an o-minimal structure, and recently was further extended to nonsmooth sub-analytic functions [3].
Deﬁnition II.3 (K-L inequality). A function f : Rn→ R is said to satisfy the K-L inequality at x0 if there exists η > 0, δ >
0, ϕ∈ Aη, such that for all x∈O (x0, δ)∩{ x : f(x0) < f (x) < f (x0) + η}
ϕ′(f(x)− f(x0))dist(0, ∂f(x))≥ 1,
where dist(x0, ∂f(x)) := inf{∥x0− y∥ : y∈ ∂f (x)}, and Aη stand for the class of functions ϕ : [0, η)→ R+ such that (a)
ϕ is continuous on [0, η); (b) ϕ is smooth concave on (0, η); (c) ϕ(0) = 0, ϕ′(x) > 0,∀x∈ (0, η).
The following is an extension of the conventional K-L inequality [4].
Lemma II.2 (K-L inequality on compact subsets) . Let f : Rn→ R be a proper lower semi-continuous function and let
Ω⊆ Rn be a compact set. If f is a constant on Ω and f satisﬁes the K-L inequality at each point in Ω, then there exists
η > 0, δ > 0, ϕ∈ Aη, such that for all x0∈ Ω and for all x∈{ x∈ Rn : dist(x, Ω) < δ )}∩{ x∈ Rn : f(x0) < f (x) <
f(x0) + η},
ϕ′(f(x)− f(x0))dist(0, ∂f(x))≥ 1.
Typical functions satisfying the K-L inequality include strongly convex functions, real analytic functions, semi-algebraic
functions and sub-analytic functions.

<!-- page 4 -->
4
A subset C⊂ Rn is said to be semi-algebraic if it can be written as
C =
r⋃
j=1
s⋂
i=1
{x∈ Rn : gi,j(x) = 0, hi,j(x) < 0},
where gi,j, hi,j : Rn→ R are real polynomial functions. Then a function f : Rn→ R is called semi-algebraic if its graph
G(f) :={(x, y)∈ Rn+1 : f(x) = y}
is a semi-algebraic subset in Rn+1. For example, the ℓq quasi norm ∥x∥q := ( ∑
i|xi|q)1/q with 0 < q ≤ 1, the sup-norm
∥x∥∞ := maxi|xi|, the Euclidean norm∥x∥,∥Ax− b∥q
q,∥Ax− b∥ and∥Ax− b∥∞ are all semi-algebraic functions [4], [39].
A real function on R is said to be analytic if it possesses derivatives of all orders and agrees with its Taylor series in a
neighborhood of every point. For a real function f on Rn, it is said to be analytic if the function of one variable g(t) := f(x+ty)
is analytic for any x, y∈ Rn. It is readily seen that real polynomial functions such as quadratic functions ∥Ax− b∥2 are
analytic. Moreover the ε-smoothed ℓq norm∥x∥ε,q := ∑
i(x2
i +ε)q/2 with 0 < q≤ 1 and the logistic loss function log(1+ e−t)
are also examples for real analytic functions [39].
A subset C⊂ Rn is said to be sub-analytic if it can be written as
C =
r⋃
j=1
s⋂
i=1
{x∈ Rn : gi,j(x) = 0, hi,j(x) < 0},
where gi,j, hi,j : Rn→ R are real analytic functions. Then a function f : Rn→ R is called sub-analytic if its graph G(f)
is a sub-analytic subset in Rn+1. It is clear that both real analytic and semi-algebraic functions are sub-analytic. Generally
speaking, the sum of of two sub-analytic functions is not necessarily sub-analytic. As shown in [3], [39], for two sub-analytic
functions, if at least one function maps bounded sets to bounded sets, then their sum is also sub-analytic. In particular, the
sum of a sub-analytic function and a analytic function is sub-analytic. Some sub-analytic functions that are widely used are
as follows:
• ∥Ax− b∥2 + λ∥y∥q
q;
• ∥Ax− b∥2 + λ ∑
i(y2
i + ε)q/2;
• 1
n
∑n
i=1 log(1 + exp(−ci(a⊤
i x + b)) + λ∥y∥q
q;
• 1
n
∑n
i=1 log(1 + exp(−ci(a⊤
i x + b)) + λ ∑
i(y2
i + ε)q/2.
C. Bregman distance
The Bregman distance, ﬁrst introduced in 1967 [6], plays an important role in various iterative algorithms. As a generalization
of squared Euclidean distance, the Bregman distance share many similar nice properties of the Euclidean distance. However,
the Bregman distance is not a metric, since it does not satisfy the triangle inequality nor symmetry. For a convex differential
function φ, the associated Bregman distance is deﬁned as
△φ(x, y) = φ(x)− φ(y)−⟨∇ φ(y), x− y⟩.
In particular, if we let φ(x) :=∥x∥2 in the above, then it is reduced to ∥x− y∥2, namely the classical Euclidean distance.
Some nontrivial examples of Bregman distance include [2]:
• Itakura-Saito distance: ∑
i xi(log xi/yi)− ∑
i(xi− yi);
• Kullback-Leibler divergence: ∑
i xi(log xi/yi);
• Mahalanobis distance:∥x− y∥2
Q =⟨Qx, x⟩ with Q a symmetric positive deﬁnite matrix.
Let us now collect some useful properties about Bregman distance.
Proposition II.3. Let φ be a convex differential function and △φ(x, y) the associated Bregman distance.
• Non-negativity:△φ(x, y)≥ 0,△φ(x, x) = 0 for all x, y.
• Convexity:△φ(x, y) is convex in x, but not necessarily in y.
• Strong Convexity: If φ is δ-strongly convex, then△φ(x, y)≥ δ
2∥x− y∥2 for all x, y.
As shown in the below, an appropriate choice of Bregman distance will simplify the x and y-subproblems, which in turn
improve the performance of the algorithm. For example, in y-subproblem (5), when taking g(y) =∥y∥1/2
1/2, ψ≡ 0, then the
problem is minimizing function
∥y∥1/2
1/2−⟨ pk, y⟩ + α
2∥By− Axk∥2.
In general ﬁnding a minimizer of this function is not a easy task. However, if we take ψ = µ
2∥y∥2− α
2∥By− Axk− pk/α∥2
with µ > α∥B∥2, then it is transformed into minimizing a problem of
∥y∥1/2
1/2 + α
2µ∥y− (yk− µ−1B⊤(By k− Axk− pk/α))∥2.
Such a problem has a closed form solution (see [40]), and thus it can be very easily solved.

<!-- page 5 -->
5
D. Basic assumption
We need the following basic assumptions on problem (1). A basic assumption to guarantee the convergence of the BADMM
is that the matrix A has full-row rank. The only difference between Assumptions 1 and 2 is: one needs B having full column
rank in Assumption 1, while in Assumption 2 one needs ψ being strongly convex. It worth noting that one can choose ψ≡ 0
under Assumption 1, so that the BADMM includes the standard ADMM as a special case. It is also worth noting that the
choice of ψ≡ 0 is not available under Assumption 2.
Assumption 1. Let min(µ0, µ1) > 0, f : Rn1 → R a continuous differential function and g : Rn2 → R a proper lower
semi-continuous functions. Assume that the following hold.
(a) AA⊤⪰ µ0I and B is injective;
(b) either Lα(x, y, p) with respect to x or φ is µ1 strongly convex;
(c) f + g is a sub-analytic function, and ∇f,∇φ and∇ψ are Lipshitz continuous.
In condition (b), the strong convexity of φ is easily attained, for example φ = µ1
2∥x∥2, while the strong convexity of
Lα(x, y, p) in x can be deduced from some standard assumptions, for example Neumann boundary condition in image processing
[15]. Condition (b) will be used to guarantee the sufﬁcient descent property of the augmented Lagrangian functions. More
speciﬁcally, it implies
Lα(xk+1, yk+1, pk)≤ Lα(xk, yk+1, pk)− µ1
2∥xk+1− xk∥2, (10)
where (xk, yk, pk) is generated by algorithm (5)-(7). As a matter of fact, if Lα(x, y, p) with respect to x is µ1-strongly convex,
then Lα(x, y, p) +△φ is also µ1-strongly convex because △φ is convex from Proposition II.3. Thus the desired inequality
will follow from the deﬁnition of strong convexity and Proposition II.3. If φ is strongly convex, then it follows again from
Proposition II.3 that
△φ(xk+1, xk)≥ µ1
2∥x− xk∥2,
which together with the deﬁnition of xk yields the desired inequality.
The condition that f + g is sub-analytic in (c) will be used to guarantee the auxiliary function constructed in the following
section satisfying the K-L inequality. We notice that all functions mentioned in subsection II-B satisfy assumption (c). The
Lipschitz continuity is a standard assumption for various algorithms, even in convex settings.
We also consider the BADMM under another set of conditions listed in Assumption 2 below. The only difference between
Assumptions 1 and 2 is that one needs B having full column rank in Assumption 1, where in Assumption 2 we assume that
ψ is strongly convex. It is worth noting that one can choose ψ≡ 0 under Assumption 1, so that the BADMM includes the
standard ADMM as a special case.
Assumption 2. Let min(µ0, µ1) > 0, f : Rn1 → R a continuous differential function and g : Rn2 → R a proper lower
semi-continuous functions. Assume that the following hold.
(a’) AA⊤⪰ µ0I and ψ is µ2-strongly convex.
(b) either Lα(x, y, p) with respect to x or φ is µ1 strongly convex.
(c) f + g is a sub-analytic function, and ∇f,∇φ and∇ψ are Lipshitz continuous.
III. C ONVERGENCE ANALYSIS
In this section we prove the convergence of BADMM under two different assumptions. In both assumptions, the parameter
α is chosen so that
α >
4((ℓf + ℓφ)2 + ℓ2
φ)
µ1µ0
,
where ℓf and ℓφ respectively stand for the Lipshitz constant of functions f and φ.
According to a recent work [1], the key point for convergence analysis of nonconvex algorithms is to show the descent
property of the augmented Lagrangian function. This is however not easily attained since the dual variable is updated by
maximizing the augmented Lagrangian function. As an alternative way, we construct an auxiliary function below, which helps
us to deduce the global convergence of BADMM.
A. The case B is injective
Lemma III.1. Let Assumption 1 be fulﬁlled. Then there exists σi > 0, i = 0, 1 such that
σ1∥xk+1− xk∥2≤ ˆL(xk, yk, pk, xk−1)− ˆL(xk+1, yk+1, pk+1, xk),
where ˆL(x, y, p, ˆx) := Lα(x, y, p) + σ0
2∥x− ˆx∥2.

<!-- page 6 -->
6
Proof: First we show that for each k∈ N
∥pk+1− pk∥2≤ 2(ℓf + ℓφ)2
µ0
∥xk+1− xk∥2
+
2ℓ2
φ
µ0
∥xk− xk−1∥2. (11)
Indeed applying Fermat’s rule to (6) yields
∇f(xk+1) + A⊤pk + αA⊤(Axk+1− By k+1)
+∇φ(xk+1)−∇ φ(xk) = 0, (12)
which together with (7) implies that
A⊤pk+1 = A⊤(pk + α(Axk+1− By k+1))
=−∇f(xk+1) +∇φ(xk)−∇ φ(xk+1). (13)
It then follows that
∥A⊤(pk+1− pk)∥2
=∥∇f(xk+1)−∇ f(xk) + (∇φ(xk+1)
−∇ φ(xk)) + (∇φ(xk−1)−∇ φ(xk))∥2
≤
(
∥∇f(xk+1)−∇ f(xk)∥ +∥∇φ(xk+1)
−∇ φ(xk)∥ +∥∇φ(xk−1)−∇ φ(xk)∥
)2
≤
(
ℓf∥xk+1− xk∥ + ℓφ∥xk− xk+1∥
+ ℓφ∥xk− xk−1∥
)2
≤ 2(ℓf + ℓφ)2∥xk+1− xk∥2
+ 2ℓ2
φ∥xk− xk−1∥2.
Since matrix A is surjective, we have
∥A⊤(pk+1− pk)∥2 =⟨A⊤(pk+1− pk), A⊤(pk+1− pk)⟩
=⟨AA⊤(pk+1− pk), pk+1− pk⟩
≥ µ0∥pk+1− pk∥2,
which at once implies (11), as desired.
Next we claim that
Lα(xk+1, yk+1, pk+1)− Lα(xk, yk, pk)
≤− µ1
2∥xk+1− xk∥2 + 1
α∥pk+1− pk∥2.
(14)
To see this, we deduce from (10) and (5)-(7) that
Lα(xk, yk+1, pk)≤ Lα(xk, yk, pk),
Lα(xk+1, yk+1, pk)≤ Lα(xk, yk+1, pk)
− µ1
2∥xk+1− xk∥2,
Lα(xk+1, yk+1, pk+1)− Lα(xk+1, yk+1, pk)
=⟨pk+1− pk, Axk+1− By k+1⟩
= 1
α∥pk+1− pk∥2.
Adding up the above formulas at once yields (14).
Finally it follows from (11) and (14) that
Lα(xk+1, yk+1, pk+1)− Lα(xk, yk, pk)
≤
(2(ℓf + ℓφ)2
αµ0
− µ1
2
)
∥xk+1− xk∥2
+
2ℓ2
φ
αµ0
∥xk− xk−1∥2,

<!-- page 7 -->
7
which is equivalent to
Lα(xk+1, yk+1, pk+1) +
2ℓ2
φ
αµ0
∥xk+1− xk∥2
≤ Lα(xk, yk, pk) +
2ℓ2
φ
αµ0
∥xk− xk−1∥2
−
(
µ1
2 − 2(ℓf + ℓφ)2
αµ0
−
2ℓ2
φ
αµ0
)
∥xk− xk+1∥2.
Let us now deﬁne
σ0 =
2ℓ2
φ
αµ0
, σ 1 =
(
µ1
2 − 2(ℓf + ℓφ)2
αµ0
−
2ℓ2
φ
αµ0
)
.
Clearly both σi are positive and thus the desired inequality follows.
Lemma III.2. If the sequence zk := (xk, yk, pk) is bounded, then
∞∑
k=0
∥zk− zk+1∥2 <∞.
In particular the sequence ∥zk− zk+1∥ is asymptotically regular, namely∥zk− zk+1∥→ 0 as k→∞ . Moreover any cluster
point of zk is a stationary point of Lα.
Proof: Let ˆzk := (xk, yk, pk, xk−1). Since ˆzk is clearly bounded, there exists a subsequence ˆzkj so that it is convergent
to some element ˆz∗. By our hypothesis the function ˆL is lower semicontinuous, which leads to
lim inf
j→∞
ˆL(ˆzkj)≥ ˆL(ˆz∗),
so that ˆL(ˆzkj) is bounded from below. By the previous lemma, ˆL(ˆzk) is nonincreasing, so that ˆL(ˆzkj) is convergent. Moreover
ˆL(ˆzk) is also convergent and ˆL(ˆzk)≥ ˆL(ˆz∗) for each k.
Now ﬁx k∈ N. It then follows from Lemma III.1 that
σ1
k∑
i=0
∥xi− xi+1∥2
≤
k∑
i=0
ˆL(ˆzi)− ˆL(ˆzi+1)
= ˆL(ˆz0)− ˆL(ˆzk+1)
≤ ˆL(ˆz0)− ˆL(ˆz∗) <∞.
Since k is chosen arbitrarily, we have ∑∞
k=0∥xk− xk+1∥2 <∞, which with (11) implies ∑∞
k=0∥pk− pk+1∥2 <∞. Since
B is injective, it is readily seen that there exists µB > 0 so that
α2µB∥yk− yk+1∥2
≤∥ αB(yk− yk+1)∥2
=∥(pk− pk+1) + (pk− pk−1)
+ α(Axk+1− Axk)∥2
≤ 2(∥pk− pk+1∥2 +∥pk− pk−1∥2
+ α2∥A∥2∥xk+1− xk∥2). (15)
Hence ∑∞
k=0∥yk− yk+1∥2 <∞, so that ∑∞
k=0∥zk− zk+1∥2 <∞; in particular∥zk− zk+1∥→ 0.
Let z∗ = (x∗, y∗, p∗) be any cluster point of zk and let zkj be a subsequence of zk converging to z∗. Since∥zk− zk+1∥
tends to zero as k→∞ , z kj and zkj +1 have the same limit point z∗. Since ˆL(ˆzk) is convergent, it is not hard to see that

<!-- page 8 -->
8
g(yk) is also convergent. It then follows from (5)-(7) that
pk+1 = pk + α(Axk+1− By k+1),
−∇f(xk+1) = A⊤pk+1 +∇φ(xk+1)−∇ φ(xk),
∂g(yk+1)∋ B⊤pk + αB⊤(Axk− By k+1)
+∇ψ(yk)−∇ ψ(yk+1)
= B⊤pk+1 + αB⊤(Axk− Axk+1)
+∇ψ(yk)−∇ ψ(yk+1).
Letting j→∞ in the above formulas yields
A⊤p∗ =−∇f(x∗), B⊤p∗∈ ∂g(y∗), Ax∗ = By∗,
which implies that z∗ is a stationary point.
Lemma III.3. Let ˆzk+1 := (xk+1, yk+1, pk+1, xk). Then there exists κ > 0 such that for each k
dist(0, ∂ ˆL(ˆzk+1))≤ κ(∥xk− xk+1∥ +∥xk− xk−1∥
+∥xk−1− xk−2∥).
Proof: By the deﬁnitions of ˆL and algorithm (5)-(7), we have
∂ ˆLx(ˆzk+1) =∇f(xk+1) + A⊤pk+1 + σ0(xk+1− xk)
+ αA⊤(Axk+1− By k+1)
=∇φ(xk)−∇ φ(xk+1) + σ0(xk+1− xk)
+ αA⊤(Axk+1− By k+1)
=∇φ(xk)−∇ φ(xk+1) + σ0(xk+1− xk)
+ A⊤(pk+1− pk),
where the last equality follows from (7). On the other hand, it follows from (5) that
0∈ ∂g(yk+1)− B⊤pk− αB⊤(Axk− By k+1)
+∇ψ(yk+1)−∇ ψ(yk)
= ∂g(yk+1)− B⊤pk+1− αB⊤(Axk− Axk+1)
+∇ψ(yk+1)−∇ ψ(yk),
which implies
∂ ˆLy(ˆzk+1)
= ∂g(yk+1)− B⊤pk+1 + αB⊤(By k+1− Axk+1)
∋∇ ψ(yk)−∇ ψ(yk+1) + αB⊤(Axk− Axk+1)
− αB⊤(Axk+1− By k+1)
=∇ψ(yk)−∇ ψ(yk+1) + αB⊤(Axk− Axk+1)
+ B⊤(pk− pk+1).
Also it is clear that ∂ ˆLˆx(ˆzk+1) =−σ0(xk+1− xk) and
∂ ˆLp(ˆzk+1) = Axk+1− By k+1 = 1
α(pk+1− pk).
Consequently, there exists κ0 > 0 so that
dist(0, ∂ ˆL(ˆzk+1))≤ κ0(∥xk− xk+1∥ +∥yk+1− yk∥ +∥pk+1− pk∥).

<!-- page 9 -->
9
On the other hand, it follows from (11) that
∥pk+1− pk∥≤
[2(ℓf + ℓφ)2
µ0
∥xk+1− xk∥2
+
2ℓ2
φ
µ0
∥xk− xk−1∥2
]1/2
≤
√
2(ℓf + ℓφ)√µ0
∥xk+1− xk∥
+
√
2ℓφ
√µ0
∥xk− xk−1∥
≤
√
2(ℓf + ℓφ)√µ0
(∥xk+1− xk∥ +∥xk− xk−1∥)
= κ1(∥xk+1− xk∥ +∥xk− xk−1∥), (16)
where we have deﬁned κ1 :=
√
2(ℓf + ℓφ)/√µ0. Furthermore, it follows from (15) that
∥yk− yk+1∥≤
√
2
α√µB
(∥pk− pk+1∥2 +∥pk− pk−1∥2
+ α2∥A∥2∥xk+1− xk∥2)1/2
≤
√
2
α√µB
(∥pk− pk+1∥ +∥pk− pk−1∥
+ α∥A∥∥xk+1− xk∥)
≤
√
2
α√µB
((κ1 + α∥A∥)∥xk− xk+1∥
+ 2κ1∥xk− xk−1∥ + κ1∥xk−1− xk−2∥)
= κ2(∥xk− xk+1∥ +∥xk− xk−1∥
+∥xk−1− xk−2∥), (17)
where we have deﬁned κ2 :=
√
2(2κ1 + α∥A∥)/α√µB. Hence, with κ := κ0(κ1 + κ2), we immediately obtain the inequality
as desired.
Theorem III.4. Let Assumption 1 be fulﬁlled. If zk := (xk, yk, pk) is bounded, then
∞∑
k=0
∥zk− zk+1∥ <∞.
Moreover the sequence (zk) converges to a stationary point of problem (1).
Proof: Let ˆzk+1 = (xk+1, yk+1, pk+1, xk) and let Ω denote the cluster point set of ˆzk. By Lemma III.2, the sequence
xk is asymptotically regular, then the sequence xk and xk+1 share the the same cluster points. Hence we can take ˆz∗ :=
(x∗, y∗, q∗, x∗)∈ Ω and let ˆzkj be a subsequence of ˆzk converging to ˆz∗. By our hypothesis on g, we have that ˆL(ˆzkj)→ ˆL(ˆz∗).
Since by Lemma III.2 the sequence ˆL(ˆzk) is convergent, this implies that ˆL(ˆzk)→ ˆL(ˆz∗); hence the function ˆL(·) is a constant
on Ω.
Let us now consider two possible cases on ˆL(ˆzk). First assume that there exists k0∈ N such that ˆLk0 = ˆL(ˆz∗). Then we
deduce from Lemma III.1 that for any k > k 0
σ1∥xk+1− xk∥2≤ ˆL(ˆzk)− ˆL(ˆzk+1)
≤ ˆL(ˆzk0)− ˆL(ˆz∗) = 0,
where we have used the fact that ˆL(ˆzk) is nonincreasing. This together with (16) and (17) implies that zk is a constant sequence
except for some ﬁnite terms, and thus it is a convergent sequence.
Let us now assume that ˆL(ˆzk) > ˆL(ˆz∗) for each k∈ N. By our hypothesis on f and g, it is clear that ˆL(·) is a sub-analytic
function and thus satisﬁes the K-L inequality. Thus by Lemma II.2 there exists η > 0, δ > 0, ϕ∈ Aη, such that for all ˆz
satisfying dist(ˆz, Ω) < δ and ˆL(ˆz∗) < ˆL(ˆz) < ˆL(ˆz∗) + η, there holds the inequality
ϕ′(ˆL(ˆz)− ˆL(ˆz∗))dist(0, ∂ ˆL(ˆz))≥ 1.

<!-- page 10 -->
10
By the deﬁnition of Ω, we have that limk dist(ˆzk, Ω) = 0. This together with the fact that ˆL(ˆzk)→ ˆL(ˆz∗) implies that there
exists k1∈ N such that dist(ˆzk, Ω) < δ and ˆL(ˆzk) < ˆL(ˆz∗) + η for all k≥ k1.
In what follows let us ﬁx k > k 1. It then follows that
ˆzk∈{ ˆz : dist(ˆz, Ω) < δ)}∩{ ˆz : ˆL(ˆz∗) < ˆL(ˆz) < ˆL(ˆz∗) + η}.
Hence dist(0, ∂ ˆL(ˆzk))ϕ′(ˆL(ˆzk)− ˆL(ˆz∗))≥ 1, which with Lemma III.3 yields
1
ϕ′(ˆL(ˆzk)− ˆL(ˆz∗))
≤ κ(∥xk− xk−1∥ +∥xk−1− xk−2∥ +∥xk−2− xk−3∥).
By the concavity of ϕ, this further implies
ˆL(ˆzk)− ˆL(ˆzk+1)
= (ˆL(ˆzk)− ˆL(ˆz∗))− (ˆL(ˆzk+1)− ˆL(ˆz∗))
≤ ϕ(ˆL(ˆzk)− ˆL(ˆz∗))− ϕ(ˆL(ˆzk+1)− ˆL(ˆz∗))
ϕ′(ˆL(ˆzk)− ˆL(ˆz∗))
≤ κ(∥xk− xk−1∥ +∥xk−1− xk−2∥ +∥xk−2− xk−3∥)
× [ϕ(ˆL(ˆzk)− ˆL(ˆz∗))− ϕ(ˆL(ˆzk+1)− ˆL(ˆz∗))].
Hence we deduce from Lemma III.1 that
∥xk+1− xk∥2
≤ κ
σ1
(∥xk− xk−1∥ +∥xk−1− xk−2∥ +∥xk−2− xk−3∥)
× [ϕ(ˆL(ˆzk)− ˆL(ˆz∗))− ϕ(ˆL(ˆzk+1)− ˆL(ˆz∗))],
which is equivalent to
4∥xk− xk+1∥
≤ 2(∥xk− xk−1∥ +∥xk−1− xk−2∥ +∥xk−2− xk−3∥)1/2
× 2
√ κ
σ1
[ϕ(ˆL(ˆzk)− ˆL(ˆz∗))− ϕ(ˆL(ˆzk+1)− ˆL(ˆz∗))]1/2.
On the other hand, using the inequality 2ab≤ a2 + b2, we get
2(∥xk− xk−1∥ +∥xk−1− xk−2∥ +∥xk−2− xk−3∥)1/2
× 2
√ κ
σ1
[ϕ(ˆL(ˆzk)− ˆL(ˆz∗))− ϕ(ˆL(ˆzk+1)− ˆL(ˆz∗))]1/2
≤∥ xk− xk−1∥ +∥xk−1− xk−2∥ +∥xk−2− xk−3∥
+ 4 κ
σ1
[ϕ(ˆL(ˆzk)− ˆL(ˆz∗))− ϕ(ˆL(ˆzk+1)− ˆL(ˆz∗))],
so that
4∥xk− xk+1∥
≤∥ xk− xk−1∥ +∥xk−1− xk−2∥ +∥xk−2− xk−3∥
+ 4 κ
σ1
[ϕ(ˆL(ˆzk)− ˆL(ˆz∗))− ϕ(ˆL(ˆzk+1)− ˆL(ˆz∗))].
Consequently we have
k∑
i=k1
4∥xi− xi+1∥
≤
k∑
i=k1
(∥xi− xi−1∥ +∥xi−1− xi−2∥ +∥xi−2− xi−3∥)
+ 4 κ
σ1
k∑
i=k1
[ϕ(ˆL(ˆzi)− ˆL(ˆz∗))− ϕ(ˆL(ˆzi+1)− ˆL(ˆz∗))],

<!-- page 11 -->
11
which is equivalent to
k∑
i=k1
∥xi− xi+1∥
≤
k∑
i=k1
(∥xi− xi−1∥−∥ xi− xi+1∥)
+
k∑
i=k1
(∥xi−1− xi−2∥−∥ xi− xi+1∥)
+
k∑
i=k1
(∥xi−2− xi−3∥−∥ xi− xi+1∥)
+ 4 κ
σ1
k∑
i=k1
[ϕ(ˆL(ˆzi)− ˆL(ˆz∗))− ϕ(ˆL(ˆzi+1)− ˆL(ˆz∗))]
≤ 3∥xk1− xk1−1∥ + 2∥xk1−1− xk1−2∥ +∥xk1−2− xk1−3∥
+ 4 κ
σ1
[ϕ(ˆL(ˆzk1)− ˆL(ˆz∗))− ϕ(ˆL(ˆzk+1)− ˆL(ˆz∗))]
≤ 3∥xk1− xk1−1∥ + 2∥xk1−1− xk1−2∥ +∥xk1−2− xk1−3∥
+ 4 κ
σ1
ϕ(ˆL(ˆzk1)− ˆL(ˆz∗)),
where the last inequality follows from the fact that ϕ(ˆL(ˆzk+1)− ˆL(ˆz∗))≥ 0. Since k is chosen arbitrarily, we deduce that∑∞
k=0∥xk− xk+1∥ <∞. It follows from the previous lemma that
∥qk+1− qk∥≤ κ1(∥xk+1− xk∥ +∥xk− xk−1∥
+∥yk+1− yk∥),
∥yk− yk+1∥≤ κ2(∥xk− xk+1∥ +∥xk− xk−1∥
+∥xk−1− xk−2∥).
Hence ∑∞
k=0(∥yk− yk+1∥ +∥qk− qk+1∥) <∞. Moreover we note that
∥zk− zk+1∥ = (∥xk− xk+1∥2 +∥yk− yk+1∥2
+∥qk+1− qk∥2)1/2
≤∥ xk− xk+1∥ +∥yk− yk+1∥
+∥qk+1− qk∥,
so that we can conclude ∑∞
k=0∥zk− zk+1∥ <∞. Consequently (zk) is a Cauchy sequence and thus is convergent, which
together with Lemma III.2 completes the proof.
Remark 1. We can deduce from (13) that pk is bounded if xk is. So in the above theorem, it sufﬁces to assume that the primal
variables xk and yk are bounded, which can be automatically fulﬁlled in many particular cases. For example, the boundedness
of xk or yk can be obtained by assuming the coerciveness of f or g.
B. The case that B is not injective
Lemma III.5. Let Assumption 2 be fulﬁlled. For each k∈ N there exists σi > 0, i = 0, 1 such that
σ1(∥xk+1− xk∥2 +∥yk+1− yk∥2)
≤ ˜L(xk, yk, pk, xk−1)− ˜L(xk+1, yk+1, pk+1, xk),
where ˜L(x, y, p, ˜x) := Lα(x, y, p) + σ0
2∥x− ˜x∥2.
Proof: Since ψ is strongly convex, we have
Lα(xk, yk+1, pk)≤ Lα(xk, yk, pk)− ∆ψ(yk+1, yk)
≤ Lα(xk, yk, pk)− µ2
2∥yk+1− yk∥2,

<!-- page 12 -->
12
which implies
Lα(xk+1, yk+1, pk)− Lα(xk, yk, pk)
≤− µ1
2∥xk+1− xk∥2− µ2
2∥yk+1− yk∥2.
Moreover we deduce form (11) and (7) that
Lα(xk+1, yk+1, pk+1)− Lα(xk, yk, pk)
= Lα(xk+1, yk+1, pk+1)− Lα(xk+1, yk+1, pk)
+ Lα(xk+1, yk+1, pk)− Lα(xk, yk, pk)
≤− µ1
2∥xk+1− xk∥2− µ2
2∥yk+1− yk∥2
+ 1
α∥pk+1− pk∥2
≤− µ1
2∥xk+1− xk∥2− µ2
2∥yk+1− yk∥2
+ 2(ℓf + ℓφ)2
αµ0
∥xk+1− xk∥2
+
2ℓ2
φ
αµ0
∥xk− xk−1∥2,
which is equivalent to
Lα(xk+1, yk+1, pk+1) +
2ℓ2
φ
αµ0
∥xk+1− xk∥2
≤ Lα(xk, yk, pk) +
2ℓ2
φ
αµ0
∥xk− xk−1∥2
− µ2
2∥yk+1− yk∥2
−
(
µ1
2 − 2(ℓf + ℓφ)2
αµ0
−
2ℓ2
φ
αµ0
)
∥xk− xk+1∥2.
Let us now deﬁne
σ0 =
4ℓ2
φ
αµ0
, σ 1 = min
(
µ2
2 , µ1
2 − 2(ℓf + ℓφ)2
αµ0
−
2ℓ2
φ
αµ0
)
.
Clearly both σi are positive and thus the desired inequality follows.
Lemma III.6. If the sequence zk := (xk, yk, pk) is bounded, then
∞∑
k=0
∥zk− zk+1∥2 <∞.
In particular the sequence ∥zk− zk+1∥ is asymptotically regular, namely∥zk− zk+1∥→ 0 as k→∞ . Moreover any cluster
point of zk is a stationary point of Lα.
Proof: Analogously, we can deduce as in Lemma III.2 that the sequence ˜L(˜zk) is convergent and ˜L(˜zk)≥ ˜L(˜z∗) for each
k, where ˜zk := (xk, yk, pk, xk−1) and ˜L is deﬁned as in Lemma III.5. Now ﬁx any k∈ N. It then follows from Lemma III.5
that
σ1
k∑
i=0
(∥xi− xi+1∥2 +∥yi− yi+1∥2)
≤
k∑
i=0
(˜L(˜zi)− ˜L(˜zi+1) = ˜L(˜z0)− ˜L(˜zk+1)
≤ ˜L(˜z0)− ˜L(˜z∗) <∞.
Since k is chosen arbitrarily, we can deduce that ∑∞
k=0(∥xk− xk+1∥2 +∥yk− yk+1∥2) <∞, which with (11) implies∑
k∥pk− pk+1∥2 <∞, so that ∑∞
k=0∥zk− zk+1∥2 <∞; in particular∥zk− zk+1∥→ 0. It is clear that any cluster point
of zk is a stationary point of function Lα.

<!-- page 13 -->
13
The proof of the following lemma is similar to that of Lemma III.3, so we omit the details.
Lemma III.7. Let ˜zk+1 = (xk+1, yk+1, pk+1, xk). Then for each k there exists κ > 0 such that
dist(0, ∂ ˜L(˜zk+1))≤ κ(∥xk− xk+1∥ +∥yk− yk+1∥ +∥xk− xk−1∥).
Theorem III.8. Assume that Assumption 2 is fulﬁlled. If the sequence zk := (xk, yk, qk) is bounded, then
∞∑
k=0
∥zk− zk+1∥ <∞.
In particular the sequence (zk) converges to a stationary point of Lα.
Proof: Let ˜zk+1 = (xk+1, yk+1, qk+1, xk) and let Ω be the cluster point set of ˜zk. Similar to the proof of Theorem III.4,
we can ﬁnd a sufﬁcient large k1 such that for all k > k 1
˜zk∈{ ˜z : dist(˜z, Ω) < δ}∩{ ˜z : ˜L(˜z∗) < ˜L(˜z) < ˜L(˜z∗) + η}.
In what follows, let us ﬁx k > k 1. Then the K-L inequality
dist(0, ∂ ˜L(˜zk))ϕ′(˜L(˜zk)− ˜L(˜z∗))≥ 1
together with Lemma III.7 implies
1
ϕ′(˜L(˜zk)− ˜L(˜z∗))
≤ κ(∥xk− xk−1∥ +∥yk− yk−1∥ +∥xk−2− xk−1∥),
so that the concavity of ϕ yields
˜L(˜zk)− ˜L(˜zk+1)
= (˜L(˜zk)− ˜L(˜z∗))− (˜L(˜zk+1)− ˜L(˜z∗))
≤ ϕ(˜L(˜zk)− ˜L(˜z∗))− ϕ(˜L(˜zk+1)− ˜L(˜z∗))
ϕ′(˜L(˜zk)− ˜L(˜z∗))
≤ κ(∥xk− xk−1∥ +∥yk− yk−1∥ +∥xk−2− xk−1∥)
× [ϕ(˜L(˜zk)− ˜L(˜z∗))− ϕ(˜L(˜zk+1)− ˜L(˜z∗))].
From Lemma III.5, this implies
∥xk+1− xk∥2 +∥yk+1− yk∥2
≤ κ
σ1
(∥xk− xk−1∥ +∥yk− yk−1∥ +∥xk−2− xk−1∥)
× [ϕ(˜L(˜zk)− ˜L(˜z∗))− ϕ(˜L(˜zk+1)− ˜L(˜z∗))],
which is equivalent to
3(∥xk− xk+1∥ +∥yk− yk+1∥)
≤ 3
√
2(∥xk+1− xk∥2 +∥yk+1− yk∥2)1/2
≤ 2(∥xk− xk−1∥ +∥yk− yk−1∥ +∥xk−2− xk−1∥)1/2
×
√
9κ
2σ1
[ϕ(˜L(˜zk)− ˜L(˜z∗))− ϕ(˜L(˜zk+1)− ˜L(˜z∗))]1/2.
It is readily seen that
2(∥xk− xk−1∥ +∥yk− yk−1∥ +∥xk−2− xk−1∥)1/2
×
√
9κ
2σ1
[ϕ(˜L(˜zk)− ˜L(˜z∗))− ϕ(˜L(˜zk+1)− ˜L(˜z∗))]1/2
≤∥ xk− xk−1∥ +∥yk− yk−1∥ +∥xk−2− xk−1∥
+ 9κ
2σ1
[ϕ(˜L(˜zk)− ˜L(˜z∗))− ϕ(˜L(˜zk+1)− ˜L(˜z∗))],

<!-- page 14 -->
14
so that
3(∥xk− xk+1∥ +∥yk− yk+1∥)
≤∥ xk− xk−1∥ +∥yk− yk−1∥ +∥xk−2− xk−1∥
+ 9κ
2σ1
[ϕ(˜L(˜zk)− ˜L(˜z∗))− ϕ(˜L(˜zk+1)− ˜L(˜z∗))].
Hence we have
k∑
i=k1
3(∥xi− xi+1∥ +∥yi− yi+1∥)
≤
k∑
i=k1
(∥xi− xi−1∥ +∥yi− yi−1∥ +∥xi−1− xi−2∥)
+ 9κ
2σ1
k∑
i=k1
[ϕ(˜L(˜zi)− ˜L(˜z∗))− ϕ(˜L(˜zi+1)− ˜L(˜z∗))],
from which it follows that
k∑
i=k1
∥xi− xi+1∥ + 2
k∑
i=k1
∥yi− yi+1∥
≤
k∑
i=k1
(∥xi− xi−1∥−∥ xi− xi+1∥)
+
k∑
i=k1
(∥xi−1− xi−2∥−∥ xi− xi+1∥)
+
k∑
i=k1
(∥yi− yi−1∥−∥ yi− yi+1∥)
+ 9κ
2σ1
k∑
i=k1
[ϕ(˜L(˜zi)− ˜L(˜z∗))− ϕ(˜L(˜zi+1)− ˜L(˜z∗))]
=∥xk1−1− xk1−2∥ + 2∥xk1− xk1−1∥
−∥ xk−1− xk1−2∥− 2∥xk− xk+1∥
+∥yk1−1− yk1∥−∥ yk− yk+1∥
+ 9κ
2σ1
[ϕ(˜L(˜zk1)− ˜L(˜z∗))− ϕ(˜L(˜zk+1)− ˜L(˜z∗))]
≤∥ xk1−1− xk1−2∥ + 2∥xk1− xk1−1∥ +∥yk1− yk1−1∥
+ 9κ
2σ1
ϕ(˜L(˜zk1)− ˜L(˜z∗)),
where the last inequality follows from the fact that ϕ(˜L(˜zk1)− ˜L(˜z∗))≥ 0. Since k is chosen arbitrarily, we can deduce that∑∞
k=0(∥xk− xk+1∥ +∥yk− yk+1∥) <∞, which together with (16) enables us to deduce that ∑∞
k=0∥qk− qk+1∥ <∞, and
moreover ∑∞
k=0∥zk− zk+1∥ <∞. Consequently (zk) is convergent, which together with Lemma III.5 completes the proof.
IV. A DEMONSTRATION EXAMPLE
In compressed sensing, a fundamental problem is recovering an n-dimensional sparse signal x from a set of m incomplete
measurements with m << n . It is possible as long as the number of nonzero elements of x is small enough. In such case one
needs to ﬁnd the sparsest solution of a linear system, which can be modeled as
min
x∈Rn
∥x∥0
s.t. Dx = b,
where D∈ Rm×n is the measurement matrix, b∈ Rm is the observed data, and ∥x∥0 denotes the number of nonzero elements
of x. In most cases, the sparsity is usually demonstrated under a linear transformation, for example in total variation denoising

<!-- page 15 -->
15
[34]. This then requires to solve:
min
x∈Rn
∥Ax∥0
s.t. Dx = b,
or its regularization version:
min
x∈Rn
∥Dx− b∥2 + λ∥Ax∥0, (18)
where λ > 0 is a regularization parameter and A∈ R(n−1)×n is the difference matrix, say, deﬁned by
Aij =



1, j = i + 1
−1, j = i
0, otherwise.
(19)
It is clear that the difference matrix has full-row rank.
In general, the above-mentioned problems are intractable because it is in fact a NP-hard problem. To overcome this difﬁculty,
one may relax the ℓ0 norm to the ℓ1 norm as in (18), which then leads to a convex composite problem:
min∥Dx− b∥2 + λ∥y∥1
s.t. Ax = y. (20)
where∥x∥1 = ∑
i|xi| stands for the ℓ1 norm. Applying BADMM to problem (20) with φ(x) = ψ(x) = µ∥x∥2/2 yields
yk+1 = H(Axk + pk/α; λ/α)
xk+1 = (2D⊤D + αA⊤A + µI)−1wk+1
pk+1 = pk + α(Axk+1− By k+1),
(21)
where wk+1 = µxk + αA⊤yk+1 + 2D⊤b− A⊤pk and S(·; µ) is the soft shrinkage operator.
Nevertheless, the ℓ1 regularization has been shown to be suboptimal in many cases; in particular it cannot enforce further
sparsity, since the ℓ1 norm is a loose approximation of the ℓ0 norm and often leads to an overpenalized problem. To overcome
the drawback caused by the ℓ1 regularization, an alternative way is to replace the ℓ1 norm by the ℓ1/2 quasi norm in problem
(18) (see e.g. [40]–[43]). This then leads to the following nonconvex composite problem:
min∥Dx− b∥2 + λ∥y∥1/2
1/2
s.t. Ax = y. (22)
Applying BADMM to problem (22) also with φ(x) = ψ(x) = µ∥x∥2/2 yields
yk+1 = H(Axk + pk/α; 2λ/α)
xk+1 = (2D⊤D + αA⊤A + µI)−1wk+1
pk+1 = pk + α(Axk+1− By k+1).
(23)
Here wk+1 = µxk + αA⊤yk+1 + 2D⊤b− A⊤pk and H(·; µ) is the half shrinkage operator [40] deﬁned as H(x; µ) =
{hµ(x1), hµ(x2)··· hµ(xn)}⊤ with
hµ(xi) =
{ 2xi
3
(
1 + cos 2
3(π− ϕ(|xi|))
)
,|xi| >
3√
54
4 µ2/3;
0, h µ(xi) = 0,
with ϕ(x) = arccos( µ
8 (|xi|
3 )−3/2).
For simplicity, we denote algorithms (23) and (21) by HADMM and SADMM, respectively. We now conduct an experiment to
verify convergence of the nonconvex BADMM, and reveal its advantages in sparsity-inducing and efﬁciency through comparing
the performance of HADMM and SADMM. In the experiment, the difference matrix A∈ R511×512 was generated according
to (19), and D∈ R256×512 was randomly generated with Gaussian N (0, 1/256) i.i.d. entries. We applied the HADMM and
SADMM with the same parameters λ = 0.015, α = 10 and µ1 = µ2 = 10.
The experimental results are shown in Figure 1, where the restoration accuracy is measured by means of the mean squared
error
MSE(∥x∗− xk∥) = 1
n∥x∗− xk∥,
MSE(∥y∗− yk∥) = 1
n∥y∗− yk∥.

<!-- page 16 -->
16
0 200 400 600 800 1000
0
0.005
0.01
0.015
0.02
0.025
 
 
HADMM
SADMM
(a) MSE(∥xk− x∗∥)
0 200 400 600 800 1000
0
0.002
0.004
0.006
0.008
0.01
0.012
 
 
HADMM
SADMM (b) MSE(∥yk− y∗∥)
Fig. 1. Comparison the performance of HADMM and SADMM
Here (x∗, y∗) is the true solution of the problem. As shown in Figure 1, both sequences xk and yk were fairly near the true
solution. i.e., the convergence is justiﬁed. It is readily seen that HADMM converges faster than SADMM does. Moreover, this
difference is particularly notable for yk. This supports in partial the advantage of the nonconvex model (22) over the convex
model (20) for the considered problem.
V. C ONCLUSION
In this paper, we conducted a convergence analysis on BADMM in the absence of convexity. We have shown that under
certain conditions, the BADMM algorithm can converge to a stationary point for sub-analytic functions. More importantly, our
analysis is based on the sufﬁcient descent property of the auxiliary function, instead of the augmented Lagrangian function.
It is worth noting that the order for updating the primal variables xk and yk plays a key role in our convergence analysis.
If we change the order, namely ﬁrst update xk and then yk, this may lead to a difﬁculty to derive an relation between xk and
pk. Thus how to establish the convergence results under this case is our next subject to study.
ACKNOWLEDGEMENT
The ﬁrst author wishes to thank Dr. Jinshan Zeng for his valuable suggestions on this paper. This work was partially supported
by the National 973 Programs (Grant No. 2013CB329404), the Key Program of National Natural Science Foundation of China
(Grant No. 11131006) and the National Natural Science Foundation of China (Grant No. 111301253).
REFERENCES
[1] H. Attouch, J. Bolte, B.F. Svaiter, Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward-backward splitting,
and regularized Gauss-Seidel methods, Mathematical Programming, 137(1-2): 91-129, 2013.
[2] A. Banerjee, S. Merugu, I. Dhillon, et al., Clustering with Bregman divergences, Journal of Machine Learning Research, 6: 1705–1749, 2005.
[3] J. Bolte, A. Daniilidis, A. Lewis, The Łojasiewicz inequality for nonsmooth sub-analytic functions with applications to subgradient dynamical systems,
SIAM J. Optim., 17:1205–1223, 2007.
[4] J. Bolte, S. Sabach, M. Teboulle, Proximal alternating linearized minimization for nonconvex and nonsmooth problems, Mathematical Programming,
146(1-2):459–494, 2014.
[5] S. Boyd, N. Parikh, E. Chu, et al., Distributed optimization and statistical learning via the alternating direction method of multipliers, Foundations and
Trends in Machine Learning, 3(1): 1–122, 2011.
[6] L. M. Bregman, The relaxation method of ﬁnding the common points of convex sets and its application to the solution of problems in convex programming,
USSR Computational Mathematics and Mathematical Physics, 7(3): 200–217, 1967.
[7] E. J. Cand `es, M. B. Wakinm, S.P. Boyd, Enhancing sparsity by reweighted ℓ1 minimization. Journal of Fourier Analysis and Applications, 14(5): 877–905,
2008.
[8] R. Chartrand, Exact reconstruction of sparse signals via nonconvex minimization, IEEE Signal Processing Letters, 14(10): 707-710, 2007.
[9] R. Chartrand, V . Staneva, Restricted isometry properties and nonconvex compressive sensing, Inverse Problems, 24: 1-14, 2008.
[10] G. Chen, M. Teboulle, A proximal-based decomposition method for convex minimization problems, Mathematical Programming, 64(1-3): 81–101, 1994.
[11] X. Chen, M. Zhou, Convergence of reweighted l1 minimization algorithms and unique solution of truncated lp minimization, Department of Applied
Mathematics, The Hong Kong Polytechnic University, 2010.
[12] W. Deng, W. Yin, On the global linear convergence of alternating direction methods, 2012, preprint.
[13] J. Eckstein, Splitting methods for monotone operators with applications to parallel optimization, 1989, Ph.D Thesis, Operations Research Center, MIT.
[14] J. Eckstein, D. P. Bertsekas, On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators, Mathematical
Programming, 55(1):293–318, 1992.
[15] E. Esser, Applications of Lagrangian-based alternating direction methods and connections to split Bregman, CAM report 9:31, 2009.
[16] Q. Fu, H. Wang, A. Banerjee, Bethe-ADMM for tree decomposition based parallel MAP inference, arXiv preprint arXiv:1309.6829, 2013.

<!-- page 17 -->
17
[17] D. Gabay and B. Mercier, A dual algorithm for the solution of nonlinear variational problems via ﬁnite element approximation, Computers & Mathematics
with Applications, 2:17–40, 1976.
[18] R. Glowinski, A. Marroco, Sur lapproximation par elements ﬁnis dordre un, et la resolu- tion par penalisation-dualite dune classe de problemes de
Dirichlet nonlineaires, Rev. Francaise dAut. Inf. Rech. Oper., R-2:41–76, 1975.
[19] T. Goldstein, B. O. Donoghue, S. Setzer, Fast alternating direction optimization methods, UCLA CAM technical report, 2012.
[20] B. He, X. Yuan, On the O(1/n) convergence rate of the Douglas-Rachford alternating direction method, SIAM Journal on Numerical Analysis, 50(2):700–
709, 2012.
[21] M. Hong, T. Chang, X. Wang et al., A block successive upper bound minimization method of multipliers for linearly constrained convex optimization,
2013, Preprint, arXiv:1401.7079.
[22] M. Hong, Z. Luo, M. Razaviyayn, Convergence Analysis of Alternating Direction Method of Multipliers for a Family of Nonconvex Problems, 2014,
Preprint, arXiv:1410.1390
[23] A. Kaban, Fractional norm regularization: learning with very few relevant features, IEEE Trans. Neural Networks and Learning Systems, 24(6): 953-963,
2013.
[24] K. Kurdyka, On gradients of functions deﬁnable in o-minimal structures, Annales de linstitut Fourier (Grenoble) 48(3):769–783, 1998.
[25] M. Lai, J. Wang, An unconstrained lq minimization with 0 < q≤ 1 for sparse solution of underdetermined linear systems, SIAM J. Optim., 21, 82–101,
2010.
[26] G. Li, T.K. Pong, Splitting methods for nonconvex composite optimization, arXiv:1407.0753v2
[27] A. Liavas, N. Sidiropoulos, Parallel Algorithms for Constrained Tensor Factorization via the Alternating Direction Method of Multipliers, arXiv preprint
arXiv:1409.2383, 2014.
[28] J. Liu, L. Yuan, J. Ye, Dictionary LASSO: Guaranteed Sparse Recovery under Linear Transformation, arXiv:1305.0047, 2013.
[29] Z. Lu, Iterative reweighted minimization methods for lp regularized unconstrained nonlinear programming, Mathematical Programming, 147:277-307,
2014.
[30] R. Monteiro, B. Svaiter, Iteration-complexity of block-decomposition algorithms and the alternating direction method of multipliers, SIAM Journal on
Optimization, 23(1):475–507, 2013.
[31] B. Mordukhovich, Variational Analysis and Generalized Differentiation, I: Basic Theory, Springer-Verlag, Berlin, 2006.
[32] S. Łojasiewicz, Une propri ´et´e topologique des sous-ensembles analytiques reels, Les ´Equations aux D ´eriv´ees Partielles, ´Editions du Centre National de
la Recherche Scientiﬁque Paris, 87–89, 1963
[33] Y . Qian, S. Jia, J. Zhou, et al., Hyperspectral unmixing via L1/2 sparsity-constrained nonnegative matrix factorization, IEEE Transaction on Geoscience
and Remote Sensing, 49(11): 4282-4297, 2011.
[34] L. Rudin, S. Osher, E. Fatemi, Nonlinear total variation based noise removal algorithms, Physica D, 60:259–268, 1992.
[35] H. Wang, A. Banerjee, Online alternating direction method. In International Conference on Machine Learning (ICML), 2012.
[36] H. Wang, A. Banerjee, Bregman Alternating Direction Method of Multipliers, Neural Information Processing System (NIPS), 2014.
[37] J. Wright, A. Yang, A. Ganesh, et al., Robust face recognition via sparse representation, IEEE Transactions on Pattern Analysis and Machine Intelligence,
31(2): 210-227, 2008.
[38] Y . Xu, W. Yin, Z. Wen, et al, An alternating direction algorithm for matrix completion with nonnegative factors, Frontiers of Mathematics in China,
2012, 7(2): 365-384.
[39] Y . Xu, W. Yin, A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and
completion, SIAM J. Imaging Sciences, 6(3): 1758–1789, 2013.
[40] Z. Xu, X. Chang, et al., L1/2 regularization: a thresholding representation theory and a fast solver, IEEE Transactions on Neural Networks and Learning
Systems, 23: 1013-1027, 2012.
[41] J. Zeng, S. Lin, Y . Wang, Z. Xu, L1/2 Regularization: Convergence of Iterative Half Thresholding Algorithm, IEEE Transactions on Signal Processing,
62(9):2317–2329, 2014.
[42] J. Zeng, J. Fang, Z. Xu, Sparse SAR imaging based on L1/2 regularization, Sciences China F, 55: 1755-1775, 2012.
[43] J. Zeng, Z. Xu, B. Zhang, et al., Accelerated L1/2 regularization based SAR imaging via BCR and reduced Newton skills, Signal Processing, 93:
1831-1844, 2013.
[44] R. Zhang, J. Kwok, Asynchronous distributed ADMM for consensus optimization, Proceedings of the 31st International Conference on Machine Learning
(ICML-14). 2014: 1701–1709.
[45] T. Zhang, Analysis of multi-stage convex relaxation for sparse regularization, Journal of Machine Learning Research, 11: 1081-1107, 2010.
[46] Y . Zhang, An alternating direction algorithm for nonnegative matrix factorization, preprint, 2010.
[47] X. Zhang, M. Burger, S. Osher, A uniﬁed primal-dual algorithm framework based on Bregman iteration, Journal of Scientiﬁc Computing, 46(1): 20–46,
2011.