# On the convergence rate improvement of a primal-dual splitting algorithm for solving monotone inclusion problems

**arXiv ID:** 1303.2875v1

**Authors:** Radu Ioan Bot, Ernö Robert Csetnek, Andre Heinrich

**Abstract:** We present two modified versions of the primal-dual splitting algorithm relying on forward-backward splitting proposed in \cite{vu} for solving monotone inclusion problems. Under strong monotonicity assumptions for some of the operators involved we obtain for the sequences of iterates that approach the solution orders of convergence of O(1/n) and O(ω^n), for $ω\in (0,1)$, respectively. The investigated primal-dual algorithms are fully decomposable, in the sense that the operators are processed individually at each iteration. We also discuss the modified algorithms in the context of convex optimization problems and present numerical experiments in image processing and support vector machines classification.

---

> **Note:** This text was extracted with pypdf (plain-text fallback). LaTeX formulas may be garbled. Install `marker-pdf` for better results.

<!-- page 1 -->
On the convergence rate improvement of a primal-dual
splitting algorithm for solving monotone inclusion problems
Radu Ioan Bot ¸∗ Ern¨ o Robert Csetnek† Andr´ e Heinrich‡
October 30, 2018
Abstract. We present two modiﬁed versions of the primal-dual splitting algorithm relying
on forward-backward splitting proposed in [21] for solving monotone inclusion problems.
Under strong monotonicity assumptions for some of the operators involved we obtain for
the sequences of iterates that approach the solution orders of convergence of O( 1
n) and
O(ωn), for ω∈ (0, 1), respectively. The investigated primal-dual algorithms are fully de-
composable, in the sense that the operators are processed individually at each iteration.
We also discuss the modiﬁed algorithms in the context of convex optimization problems
and present numerical experiments in image processing and support vector machines clas-
siﬁcation.
Key Words. maximally monotone operator, strongly monotone operator, resolvent, op-
erator splitting, subdiﬀerential, strongly convex function, convex optimization algorithm,
duality
AMS subject classiﬁcation. 47H05, 65K05, 90C25
1 Introduction and preliminaries
The problem of ﬁnding the zeros of the sum of two (or more) maximally monotone op-
erators in Hilbert spaces continues to be a very active research ﬁeld, with applications
in convex optimization, partial diﬀerential equations, signal and image processing, etc.
(see [1,5–7,9,12,13,21]). To the most prominent methods in this area belong the proximal
point algorithm for ﬁnding the zeros of a maximally monotone operator (see [17]) and the
Douglas-Rachford splitting algorithm for ﬁnding the zeros of the sum of two maximally
monotone operators (see [14]). However, also motivated by diﬀerent applications, the re-
search community was interested in considering more general problems, in which the sum
of ﬁnitely many operators appear, some of them being composed with linear continuous
∗Department of Mathematics, Chemnitz University of Technology, D-09107 Chemnitz, Germany, e-
mail: radu.bot@mathematik.tu-chemnitz.de. Research partially supported by DFG (German Research
Foundation), project BO 2516/4-1.
†Department of Mathematics, Chemnitz University of Technology, D-09107 Chemnitz, Germany, e-mail:
robert.csetnek@mathematik.tu-chemnitz.de. Research supported by DFG (German Research Foundation),
project BO 2516/4-1.
‡Department of Mathematics, Chemnitz University of Technology, D-09107 Chemnitz, Germany, e-mail:
andre.heinrich@mathematik.tu-chemnitz.de. Research supported by the European Union, the European
Social Fund (ESF) and prudsys AG in Chemnitz.
1
arXiv:1303.2875v1  [math.OC]  12 Mar 2013

<!-- page 2 -->
operators [1, 9, 12]. In the last years, even more complex structures were considered, in
which also parallel sums are involved, see [6,7,13,21].
The algorithms introduced in the literature for these issues have the remarkable prop-
erty that the operators involved are evaluated separately in each iteration, either by for-
ward steps in the case of the single-valued ones (including here the linear continuous oper-
ators and their adjoints) or by backward steps for the set-valued ones, by using the corre-
sponding resolvents. More than that they share the common feature to be of primal-dual
type, meaning that they solve not only the primal inclusion problem, but also its Attouch-
Th´ era-type dual. In this context we mention the primal-dual algorithms relying on Tseng’s
forward-backward-forward splitting method (see [9, 13]), on the forward-backward split-
ting method (see [21]) and on the Douglas-Rachford splitting method (see [7]). A relevant
task is to adapt these iterative methods in order be able to investigate their convergence,
namely, to eventually determine convergence rates for the sequences generated by the
schemes in discussion. This could be important when one is interested in obtaining an
optimal solution more rapidly than in their initial formulation, which furnish “only” the
convergence statement. Accelerated versions of the primal-dual algorithm from [13] were
already provided in [6], whereby the reported numerical experiments emphasize the ad-
vantages of the ﬁrst over the original iterative scheme.
The aim of this paper is to provide modiﬁed versions of the algorithm proposed by V˜ u
in [21] for which an evaluation of their convergence behaviour is possible. By assuming
that some of the operators involved are strongly monotone, we are able to obtain for the
sequences of iterates orders of convergence ofO( 1
n) andO(ωn), forω∈ (0, 1), respectively.
For the readers convenience we present ﬁrst some notations which are used throughout
the paper (see [1–3, 15, 19, 22]). Let H be a real Hilbert space with inner product ⟨·,·⟩
and associated norm ∥·∥ =
√
⟨·,·⟩. The symbols ⇀ and→ denote weak and strong
convergence, respectively. When G is another Hilbert space and K :H →G a linear
continuous operator, then thenorm ofK is deﬁned as∥K∥ = sup{∥Kx∥ :x∈H,∥x∥≤ 1},
whileK∗ :G→H , deﬁned by⟨K∗y,x⟩ =⟨y,Kx⟩ for all (x,y )∈H×G , denotes the adjoint
operator of K.
For an arbitrary set-valued operatorA :H ⇒H we denote by GrA ={(x,u )∈H×H :
u∈ Ax} its graph, by dom A ={x∈H : Ax̸=∅} its domain and by A−1 :H ⇒H its
inverse operator, deﬁned by ( u,x )∈ GrA−1 if and only if ( x,u )∈ GrA. We say that
A is monotone if⟨x−y,u−v⟩≥ 0 for all ( x,u ), (y,v )∈ GrA. A monotone operator
A is said to be maximally monotone , if there exists no proper monotone extension of the
graph of A onH×H . The resolvent of A, JA :H ⇒H, is deﬁned by JA = (IdH +A)−1,
where IdH :H→H , IdH(x) = x for all x∈H , is the identity operator onH. Moreover,
if A is maximally monotone, then JA :H→H is single-valued and maximally monotone
(cf. [1, Proposition 23.7 and Corollary 23.10]). For an arbitrary γ > 0 we have (see [1,
Proposition 23.2])
p∈JγAx if and only if (p,γ−1(x−p))∈ GrA
and (see [1, Proposition 23.18])
JγA +γJγ−1A−1◦γ−1 IdH = IdH. (1)
Letγ >0 be arbitrary. We say thatA isγ-strongly monotone if⟨x−y,u−v⟩≥ γ∥x−y∥2
for all (x,u ), (y,v )∈ GrA. A single-valued operator A :H→H is said to be γ-cocoercive
2

<!-- page 3 -->
if⟨x−y,Ax−Ay⟩≥ γ∥Ax−Ay∥2 for all (x,y )∈H×H . Moreover, A isγ-Lipschitzian if
∥Ax−Ay∥≤ γ∥x−y∥ for all (x,y )∈H×H . A single-valued linear operator A :H→H
is said to be skew, if⟨x,Ax⟩ = 0 for all x∈H . Finally, the parallel sum of two operators
A,B :H ⇒H is deﬁned by A□B :H ⇒H,A □B = (A−1 +B−1)−1.
The following problem represents the starting point of our investigations (see [21]).
Problem 1 LetH be a real Hilbert space, z∈H , A :H ⇒H a maximally monotone
operator and C :H→H an η-cocoercive operator for η >0. Let m be a strictly positive
integer and for any i∈{ 1,...,m} letGi be a real Hilbert space, ri∈G i, Bi :Gi ⇒Gi
a maximally monotone operator, Di :Gi ⇒Gi a maximally monotone and νi-strongly
monotone operator for νi> 0 and Li :H→G i a nonzero linear continuous operator. The
problem is to solve the primal inclusion
ﬁnd x∈H such that z∈Ax +
m∑
i=1
L∗
i
(
(Bi□Di)(Lix−ri)
)
+Cx, (2)
together with the dual inclusion
ﬁnd v1∈G 1,..., vm∈G m such that∃x∈H :
{ z− ∑m
i=1L∗
ivi∈Ax +Cx
vi∈ (Bi□Di)(Lix−ri),i = 1,...,m.
(3)
We say that (x,v1,..., vm)∈H×G 1×...×Gm is a primal-dual solution to Problem 1,
if
z−
m∑
i=1
L∗
ivi∈Ax +Cx and vi∈ (Bi□Di)(Lix−ri),i = 1,...,m. (4)
If x∈H is a solution to (2), then there exists ( v1,..., vm)∈G 1×...×G m such that
(x,v1,..., vm) is a primal-dual solution to Problem 1 and, if (v1,..., vm)∈G 1×...×Gm is a
solution to (3), than there exists x∈H such that (x,v1,..., vm) is a primal-dual solution
to Problem 1. Moreover, if ( x,v1,..., vm)∈H×G 1×...×Gm is a primal-dual solution to
Problem 1, then x is a solution to (2) and ( v1,..., vm)∈G 1×...×Gm is a solution to (3).
By employing the classical forward-backward algorithm (see [12, 20]) in a renormed
product space, V˜ u proposed in [21] an iterative scheme for solving a slightly modiﬁed
version of Problem 1 formulated in the presence of some given weights wi∈ (0, 1],i =
1,...,m , with ∑m
i=1wi = 1 for the terms occurring in the second summand of the primal
inclusion problem. The following result is an adaption of [21, Theorem 3.1] to Problem 1
in the error-free case and when λn = 1 for any n≥ 0.
Theorem 2 In Problem 1 suppose that
z∈ ran
(
A +
m∑
i=1
L∗
i
(
(Bi□Di)(Li·−ri)
)
+C
)
.
Letτ and σi, i = 1,...,m , be strictly positive numbers such that
2· min{τ−1,σ−1
1 ,...,σ−1
m}· min{η,ν 1,...,ν m}

1−
√τ
m∑
i=1
σi∥Li∥2

> 1.
3

<!-- page 4 -->
Let (x0,v 1,0,...,v m,0)∈H×G 1×...×G m and for all n≥ 0 set:
xn+1 =JτA
[
xn−τ
(∑m
i=1L∗
ivi,n +Cxn−z
)]
yn = 2xn+1−xn
vi,n+1 =JσiB−1
i
[vi,n +σi(Liyn−D−1
i vi,n−ri)],i = 1,...,m.
Then there exists a primal-dual solution (x,v1,..., vm) to Problem 1 such that xn⇀x and
(v1,n,...,v m,n)⇀ (v1,..., vm) as n→ +∞.
The structure of the paper is as follows. In the next section we propose under appropriate
strong monotonicity assumptions two modiﬁed versions of the above algorithm which en-
sure for the sequences of iterates orders of convergence of O( 1
n) andO(ωn), for ω∈ (0, 1),
respectively. In Section 3 we show how to particularize the general results in the context
of nondiﬀerentiable convex optimization problems, where some of the functions occurring
in the objective are strongly convex. In the last section we present some numerical exper-
iments in image denoising and support vector machines classiﬁcation and emphasize also
the practical advantages of the modiﬁed iterative schemes over the initial one provided in
Theorem 2.
2 Two modiﬁed primal-dual algorithms
In this section we propose in two diﬀerent settings modiﬁed versions of the algorithm in
Theorem 2 and discuss the orders of convergence of the sequences of iterates generated by
the new schemes.
2.1 The case A + C is strongly monotone
For the beginning, we show that in case A +C is strongly monotone one can guarantee
an order of convergence of O( 1
n) for the sequence ( xn)n≥0. To this end, we update in
each iteration the parameters τ and σi, i = 1,...,m , and use a modiﬁed formula for the
sequence (yn)n≥0. Due to technical reasons, we apply this method in case D−1
i is equal to
zero for i = 1,...,m , that is Di(0) =Gi and Di(x) =∅ for x̸= 0. Let us notice that, by
using the approach proposed in [6, Remark 3.2], one can extend the statement of Theorem
7 below, which is the main result of this subsection, to the primal-dual pair of monotone
inclusions as stated in Problem 1.
More precisely, the problem we consider throughout this subsection is as follows.
Problem 3 LetH be a real Hilbert space, z∈H , A :H ⇒H a maximally monotone
operator and C :H→H a monotone and η-Lipschitzian operator for η >0. Let m be a
strictly positive integer and for any i∈{ 1,...,m} letGi be a real Hilbert space, ri∈G i,
Bi :Gi ⇒Gi a maximally monotone operator and Li :H→G i a nonzero linear continuous
operator. The problem is to solve the primal inclusion
ﬁnd x∈H such that z∈Ax +
m∑
i=1
L∗
i (Bi(Lix−ri)) +Cx, (5)
together with the dual inclusion
ﬁnd v1∈G 1,..., vm∈G m such that∃x∈H :
{ z− ∑m
i=1L∗
ivi∈Ax +Cx
vi∈Bi(Lix−ri),i = 1,...,m. (6)
4

<!-- page 5 -->
As for Problem 1, we say that (x,v1,..., vm)∈H×G 1×...×Gm is a primal-dual solution
to Problem 3, if
z−
m∑
i=1
L∗
ivi∈Ax +Cx and vi∈Bi(Lix−ri),i = 1,...,m. (7)
Remark 4 One can notice that, in comparison to Problem 1, we relax in Problem 3 the
assumptions made on the operator C. It is obvious that, if C is a η-cocoercive operator
for η >0, then C is monotone and 1/η-Lipschitzian. Although in case C is the gradient
of a convex and diﬀerentiable function, due to the celebrated Baillon-Haddad Theorem
(see, for instance, [1, Corollary 8.16]), the two classes of operators coincide, in general
the second one is larger. Indeed, nonzero linear, skew and Lipschitzian operators are not
cocoercive. For example, whenH andG are real Hilbert spaces and L :H→G is nonzero
linear continuous, then ( x,v )↦→ (L∗v,−Lx) is an operator having all these properties.
This operator appears in a natural way when considering primal-dual monotone inclusion
problems as done in [9].
Under the assumption that A +C is γ-strongly monotone for γ > 0 we propose the
following modiﬁcation of the iterative scheme in Theorem 2.
Algorithm 5
Initialization: Choose τ0> 0,σi,0> 0, i = 1,..,m , such that
τ0< 2γ/η, λ≥η + 1, τ0
∑m
i=1σi,0∥Li∥2≤
√
1 +τ0(2γ−ητ0)/λ
and (x0,v 1,0,...,v m,0)∈H×G 1×...×G m.
For n≥ 0 set: xn+1 =J(τn/λ)A
[
xn− (τn/λ)
(∑m
i=1L∗
ivi,n +Cxn−z
)]
θn = 1/
√
1 +τn(2γ−ητn)/λ
yn =xn+1 +θn(xn+1−xn)
vi,n+1 =Jσi,nB−1
i
[vi,n +σi,n(Liyn−ri)], i = 1,...,m
τn+1 =θnτn, σi,n+1 =σi,n/θn+1, i = 1,...,m.
Remark 6 Notice that the assumption τ0
∑m
i=1σi,0∥Li∥2≤
√
1 +τ0(2γ−ητ0)/λ in Al-
gorithm 5 is equivalent to τ1
∑m
i=1σi,0∥Li∥2≤ 1, being fulﬁlled if τ0 > 0 is chosen such
that
τ0≤ γ/λ +
√
γ2/λ2 + (∑m
i=1σi,0∥Li∥2)2 +η/λ
(∑m
i=1σi,0∥Li∥2)2 +η/λ .
Theorem 7 Suppose thatA+C isγ-strongly monotone for γ >0 and let (x,v1,..., vm) be
a primal-dual solution to Problem 3. Then the sequences generated by Algorithm 5 fulﬁll
for any n≥ 0
λ∥xn+1−x∥2
τ 2
n+1
+
(
1−τ1
m∑
i=1
σi,0∥Li∥2
) m∑
i=1
∥vi,n−vi∥2
τ1σi,0
≤
λ∥x1−x∥2
τ 2
1
+
m∑
i=1
∥vi,0−vi∥2
τ1σi,0
+∥x1−x0∥2
τ 2
0
+ 2
τ0
m∑
i=1
⟨Li(x1−x0),vi,0−vi⟩.
Moreover, lim
n→+∞
nτn = λ
γ , hence one obtains for (xn)n≥0 an order of convergence ofO( 1
n).
5

<!-- page 6 -->
Proof. The idea of the proof relies on showing that the following Fej´ er-type inequality
is true for any n≥ 0
λ
τ 2
n+2
∥xn+2−x∥2 +
m∑
i=1
∥vi,n+1−vi∥2
τ1σi,0
+∥xn+2−xn+1∥2
τ 2
n+1
−
2
τn+1
m∑
i=1
⟨Li(xn+2−xn+1),−vi,n+1 +vi⟩≤ (8)
λ
τ 2
n+1
∥xn+1−x∥2 +
m∑
i=1
∥vi,n−vi∥2
τ1σi,0
+∥xn+1−xn∥2
τ 2n
−
2
τn
m∑
i=1
⟨Li(xn+1−xn),−vi,n +vi⟩.
To this end we use ﬁrst that in the light of the deﬁnition of the resolvents it holds for
anyn≥ 0
λ
τn+1
(xn+1−xn+2)−
( m∑
i=1
L∗
ivi,n+1 +Cxn+1−z
)
+Cxn+2∈ (A +C)xn+2. (9)
Since A +C is γ-strongly monotone, (7) and (9) yield for any n≥ 0
γ∥xn+2−x∥2≤
⟨
xn+2−x, λ
τn+1
(xn+1−xn+2)
⟩
+
⟨
xn+2−x,−
( m∑
i=1
L∗
ivi,n+1 +Cxn+1−z
)
+Cxn+2−
(
z−
m∑
i=1
L∗
ivi
)⟩
=
λ
τn+1
⟨xn+2−x,xn+1−xn+2⟩ +⟨xn+2−x,Cxn+2−Cxn+1⟩ +
m∑
i=1
⟨Li(xn+2−x),vi−vi,n+1⟩.
Further, we have
⟨xn+2−x,xn+1−xn+2⟩ =∥xn+1−x∥2
2 −∥xn+2−x∥2
2 −∥xn+1−xn+2∥2
2 (10)
and, since C is η-Lipschitzian,
⟨xn+2−x,Cxn+2−Cxn+1⟩≤ ητn+1
2 ∥xn+2−x∥2 + η
2τn+1
∥xn+2−xn+1∥2,
hence for any n≥ 0 it yields
( λ
τn+1
+ 2γ−ητn+1
)
∥xn+2−x∥2≤
λ
τn+1
∥xn+1−x∥2− λ−η
τn+1
∥xn+2−xn+1∥2 + 2
m∑
i=1
⟨Li(xn+2−x),vi−vi,n+1⟩.
6

<!-- page 7 -->
Taking into account that λ≥η + 1 we obtain for any n≥ 0 that
( λ
τn+1
+ 2γ−ητn+1
)
∥xn+2−x∥2≤
λ
τn+1
∥xn+1−x∥2− 1
τn+1
∥xn+2−xn+1∥2 + 2
m∑
i=1
⟨Li(xn+2−x),vi−vi,n+1⟩. (11)
On the other hand, for every i = 1,...,m and any n≥ 0, from
1
σi,n
(vi,n−vi,n+1) +Liyn−ri∈B−1
i vi,n+1, (12)
the monotonicity of B−1
i and (7) we obtain
0 ≤
⟨ 1
σi,n
(vi,n−vi,n+1) +Liyn−ri− (Lix−ri),vi,n+1−vi
⟩
= 1
σi,n
⟨vi,n−vi,n+1,vi,n+1−vi⟩ +⟨Li(yn−x),vi,n+1−vi⟩
= 1
2σi,n
∥vi,n−vi∥2− 1
2σi,n
∥vi,n−vi,n+1∥2− 1
2σi,n
∥vi,n+1−vi∥2
+ ⟨Li(yn−x),vi,n+1−vi⟩,
hence
∥vi,n+1−vi∥2
σi,n
≤∥vi,n−vi∥2
σi,n
−∥vi,n−vi,n+1∥2
σi,n
+ 2⟨Li(yn−x),vi,n+1−vi⟩. (13)
Summing up the inequalities in (11) and (13) we obtain for any n≥ 0
( λ
τn+1
+ 2γ−ητn+1
)
∥xn+2−x∥2 +
m∑
i=1
∥vi,n+1−vi∥2
σi,n
≤
λ
τn+1
∥xn+1−x∥2 +
m∑
i=1
∥vi,n−vi∥2
σi,n
−∥xn+2−xn+1∥2
τn+1
−
m∑
i=1
∥vi,n−vi,n+1∥2
σi,n
(14)
+2
m∑
i=1
⟨Li(xn+2−yn),−vi,n+1 +vi⟩.
Further, sinceyn =xn+1 +θn(xn+1−xn), for every i = 1,...,m and anyn≥ 0 it holds
⟨Li(xn+2−yn),−vi,n+1 +vi⟩ =⟨Li
(
xn+2−xn+1−θn(xn+1−xn)
)
,−vi,n+1 +vi⟩ =
⟨Li(xn+2−xn+1),−vi,n+1 +vi⟩− θn⟨Li(xn+1−xn),−vi,n +vi⟩ +
θn⟨Li(xn+1−xn),−vi,n +vi,n+1⟩≤
⟨Li(xn+2−xn+1),−vi,n+1 +vi⟩− θn⟨Li(xn+1−xn),−vi,n +vi⟩ +
θ2
n∥Li∥2σi,n
2 ∥xn+1−xn∥2 +∥vi,n−vi,n+1∥2
2σi,n
.
7

<!-- page 8 -->
By combining the last inequality with (14) we obtain for any n≥ 0
( λ
τn+1
+ 2γ−ητn+1
)
∥xn+2−x∥2 +
m∑
i=1
∥vi,n+1−vi∥2
σi,n
+∥xn+2−xn+1∥2
τn+1
−
2
m∑
i=1
⟨Li(xn+2−xn+1),−vi,n+1 +vi⟩≤ (15)
λ
τn+1
∥xn+1−x∥2 +
m∑
i=1
∥vi,n−vi∥2
σi,n
+
( m∑
i=1
∥Li∥2σi,n
)
θ2
n∥xn+1−xn∥2−
2
m∑
i=1
θn⟨Li(xn+1−xn),−vi,n +vi⟩.
After dividing (15) by τn+1 and noticing that for any n≥ 0
λ
τ 2
n+1
+ 2γ
τn+1
−η = λ
τ 2
n+2
,
τn+1σi,n =τnσi,n−1 =... =τ1σi,0
and (∑m
i=1∥Li∥2σi,n
)
θ2
n
τn+1
= τn+1
∑m
i=1∥Li∥2σi,n
τ 2n
= τ1
∑m
i=1∥Li∥2σi,0
τ 2n
≤ 1
τ 2n
,
it follows that the Fej´ er-type inequality (8) is true.
Let N∈ N,N ≥ 2. Summing up the inequality in (8) from n = 0 to N− 1, it yields
λ
τ 2
N +1
∥xN +1−x∥2 +
m∑
i=1
∥vi,N−vi∥2
τ1σi,0
+∥xN +1−xN∥2
τ 2
N
≤
λ
τ 2
1
∥x1−x∥2 +
m∑
i=1
∥vi,0−vi∥2
τ1σi,0
+∥x1−x0∥2
τ 2
0
+ (16)
2
m∑
i=1
( 1
τN
⟨Li(xN +1−xN),−vi,N +vi⟩− 1
τ0
⟨Li(x1−x0),−vi,0 +vi⟩
)
.
Further, for every i = 1,...,m we use the inequality
2
τN
⟨Li(xN +1−xN),−vi,N +vi⟩≤
σi,0∥Li∥2
τ 2
N(∑m
i=1σi,0∥Li∥2)∥xN +1−xN∥2 +
∑m
i=1σi,0∥Li∥2
σi,0
∥vi,N−vi∥2
and obtain ﬁnally
λ∥xN +1−x∥2
τ 2
N +1
+
m∑
i=1
∥vi,N−vi∥2
τ1σi,0
≤ λ∥x1−x∥2
τ 2
1
+
m∑
i=1
∥vi,0−vi∥2
τ1σi,0
+∥x1−x0∥2
τ 2
0
+ 2
τ0
m∑
i=1
⟨Li(x1−x0),vi,0−vi⟩ +
m∑
i=1
∑m
j=1σj,0∥Lj∥2
σi,0
∥vi,N−vi∥2,
8

<!-- page 9 -->
which rapidly yields the inequality in the statement of the theorem.
We close the proof by showing that lim
n→+∞
nτn =λ/γ. Notice that for any n≥ 0,
τn+1 = τn√
1 + τn
λ (2γ−ητn). (17)
Since 0 < τ0 < 2γ/η, it follows by induction that 0 < τn+1 < τn < τ0 < 2γ/η for any
n≥ 1, hence the sequence (τn)n≥0 converges. In the light of (17) one easily obtains that
lim
n→+∞
τn = 0 and, further, that lim
n→+∞
τn
τn+1
= 1. As ( 1
τn )n≥0 is a strictly increasing and
unbounded sequence, by applying the Stolz-Ces` aro Theorem it yields
lim
n→+∞
nτn = lim
n→+∞
n
1
τn
= lim
n→+∞
n + 1−n
1
τn+1
− 1
τn
= lim
n→+∞
τnτn+1
τn−τn+1
= lim
n→+∞
τnτn+1(τn +τn+1)
τ 2n−τ 2
n+1
= lim
n→+∞
τnτn+1(τn +τn+1)
τ 2
n+1
τn
λ (2γ−ητn)
= lim
n→+∞
τn +τn+1
τn+1( 2γ
λ − η
λτn)
= lim
n→+∞
τn
τn+1
+ 1
2γ
λ − η
λτn
= λ
γ.
■
Remark 8 Let us mention that, if A +C is γ-strongly monotone with γ >0, then the
operator A + ∑m
i=1L∗
i (Bi(Li·−ri)) +C is strongly monotone, as well, thus the monotone
inclusion problem (5) has at most one solution. Hence, if ( x,v1,..., vm) is a primal-dual
solution to Problem 3, then x is the unique solution to (5). Notice that the problem (6)
may not have an unique solution.
2.2 The case A + C and B−1
i + D−1
i , i = 1, ..., m, are strongly monotone
In this subsection we propose a modiﬁed version of the algorithm in Theorem 2 which
guarantees when A +C and B−1
i +D−1
i ,i = 1,...,m, are strongly monotone orders of
convergence ofO(ωn), for ω∈ (0, 1), for the sequences (xn)n≥0 and (vi,n)n≥0,i = 1,...,m .
The algorithm aims to solve the primal-dual pair of monotone inclusions stated in Problem
1 under relaxed assumptions for the operators C and D−1
i ,i = 1,...,m .
Problem 9 LetH be a real Hilbert space, z∈H , A :H ⇒H a maximally monotone
operator and C :H →H a monotone and η-Lipschitzian operator for η > 0. Let m
be a strictly positive integer and for any i∈{ 1,...,m} letGi be a real Hilbert space,
ri∈G i, Bi :Gi ⇒Gi a maximally monotone operator, Di :Gi ⇒Gi a monotone operators
such that D−1
i is νi-Lipschitzian for νi > 0 and Li :H→G i a nonzero linear continuous
operator. The problem is to solve the primal inclusion
ﬁnd x∈H such that z∈Ax +
m∑
i=1
L∗
i
(
(Bi□Di)(Lix−ri)
)
+Cx, (18)
together with the dual inclusion
ﬁnd v1∈G 1,..., vm∈G m such that∃x∈H :
{ z− ∑m
i=1L∗
ivi∈Ax +Cx
vi∈ (Bi□Di)(Lix−ri),i = 1,...,m.
(19)
9

<!-- page 10 -->
Under the assumption that A +C is γ-strongly monotone for γ >0 and B−1
i +D−1
i
is δi-strongly monotone with δi > 0,i = 1,....m , we propose the following modiﬁcation of
the iterative scheme in Theorem 2.
Algorithm 10
Initialization: Choose µ> 0 such that
µ≤ min
{
γ2/η2,δ 2
1/ν2
1,...,δ 2
m/ν2
m,
√
γ/ (∑m
i=1∥Li∥2/δi)
}
,
τ =µ/(2γ), σi =µ/(2δi), i = 1,..,m ,
θ∈ [2/(2 +µ), 1] and (x0,v 1,0,...,v m,0)∈H×G 1×...×G m.
For n≥ 0 set: xn+1 =JτA
[
xn−τ
(∑m
i=1L∗
ivi,n +Cxn−z
)]
yn =xn+1 +θ(xn+1−xn)
vi,n+1 =JσiB−1
i
[vi,n +σi(Liyn−D−1
i vi,n−ri)], i = 1,...,m.
Theorem 11 Suppose that A +C is γ-strongly monotone for γ > 0, B−1
i +D−1
i is δi-
strongly monotone for δi> 0, i = 1,...,m , and let (x,v1,..., vm) be a primal-dual solution
to Problem 9. Then the sequences generated by Algorithm 10 fulﬁll for any n≥ 0
γ∥xn+1−x∥2 + (1−ω)
m∑
i=1
δi∥vi,n−vi∥2≤
ωn
(
γ∥x1−x∥2 +
m∑
i=1
δi∥vi,0−vi∥2 + γ
2ω∥x1−x0∥2 +µω
m∑
i=1
⟨Li(x1−x0),vi,0−vi⟩
)
,
where 0<ω = 2(1+θ)
4+µ < 1.
Proof. For anyn≥ 0 we have
1
τ (xn+1−xn+2)−
( m∑
i=1
L∗
ivi,n+1 +Cxn+1−z
)
+Cxn+2∈ (A +C)xn+2, (20)
thus, since A +C is γ-strongly monotone, (19) yields
γ∥xn+2−x∥2≤
⟨
xn+2−x, 1
τ (xn+1−xn+2)
⟩
+
⟨
xn+2−x,−
( m∑
i=1
L∗
ivi,n+1 +Cxn+1−z
)
+Cxn+2−
(
z−
m∑
i=1
L∗
ivi
)⟩
=
1
τ⟨xn+2−x,xn+1−xn+2⟩ +⟨xn+2−x,Cxn+2−Cxn+1⟩ +
m∑
i=1
⟨Li(xn+2−x),vi−vi,n+1⟩.
Further, by using (10) and
⟨xn+2−x,Cxn+2−Cxn+1⟩≤ γ
2∥xn+2−x∥2 + η2
2γ∥xn+2−xn+1∥2,
10

<!-- page 11 -->
we get for any n≥ 0
( 1
2τ + γ
2
)
∥xn+2−x∥2≤
1
2τ∥xn+1−x∥2−
( 1
2τ− η2
2γ
)
∥xn+2−xn+1∥2 +
m∑
i=1
⟨Li(xn+2−x),vi−vi,n+1⟩.
After multiplying this inequality with µ and taking into account that
µ
2τ =γ,µ
( 1
2τ + γ
2
)
=γ
(
1 +µ
2
)
and µ
( 1
2τ− η2
2γ
)
=γ− η2
2γµ≥ γ
2,
we obtain for any n≥ 0
γ
(
1 +µ
2
)
∥xn+2−x∥2≤ (21)
γ∥xn+1−x∥2− γ
2∥xn+2−xn+1∥2 +µ
m∑
i=1
⟨Li(xn+2−x),vi−vi,n+1⟩.
On the other hand, for every i = 1,...,m and any n≥ 0, from
1
σi
(vi,n−vi,n+1) +Liyn−D−1
i vi,n−ri +D−1
i vi,n+1∈ (B−1
i +D−1
i )vi,n+1, (22)
the δi-strong monotonicity of B−1
i +D−1
i and (19) we obtain
δi∥vi,n+1−vi∥2 ≤
⟨ 1
σi
(vi,n−vi,n+1),vi,n+1−vi
⟩
+
⟨
Liyn−ri−D−1
i vi,n +D−1
i vi,n+1− (Lix−ri),vi,n+1−vi
⟩
.
Further, for every i = 1,...,m and any n≥ 0 we have
1
σi
⟨vi,n−vi,n+1,vi,n+1−vi⟩ = 1
2σi
∥vi,n−vi∥2− 1
2σi
∥vi,n−vi,n+1∥2− 1
2σi
∥vi,n+1−vi∥2
and, since D−1
i is a νi-Lipschitzian operator,
⟨D−1
i vi,n+1−D−1
i vi,n,vi,n+1−vi⟩≤ δi
2∥vi,n+1−vi∥2 + ν2
i
2δi
∥vi,n+1−vi,n∥2.
Consequently, for everyi = 1,...,m and any n≥ 0 it holds
( 1
2σi
+ δi
2
)
∥vi,n+1−vi∥2≤
1
2σi
∥vi,n−vi∥2−
( 1
2σi
− ν2
i
2δi
)
∥vi,n+1−vi,n∥2 +⟨Li(x−yn),vi−vi,n+1⟩,
which, after multiplying it by µ (here is the initial choice of µ determinant), yields
δi
(
1 +µ
2
)
∥vi,n+1−vi∥2≤δi∥vi,n−vi∥2−δi
2∥vi,n+1−vi,n∥2+µ⟨Li(x−yn),vi−vi,n+1⟩. (23)
11

<!-- page 12 -->
We denote
an :=γ∥xn+1−x∥2 +
m∑
i=1
δi∥vi,n−vi∥2∀n≥ 0.
Summing up the inequalities in (21) and (23) we obtain for any n≥ 0
(
1 +µ
2
)
an+1≤an (24)
−γ
2∥xn+2−xn+1∥2−
m∑
i=1
δi
2∥vi,n−vi,n+1∥2 +µ
m∑
i=1
⟨Li(xn+2−yn),vi−vi,n+1⟩.
Further, since yn =xn+1 +θ(xn+1−xn) and ω≤θ, for every i = 1,...,m and any n≥ 0
it holds
⟨Li(xn+2−yn),vi−vi,n+1⟩ =⟨Li (xn+2−xn+1−θ(xn+1−xn)),vi−vi,n+1⟩ =
⟨Li(xn+2−xn+1),vi−vi,n+1⟩− ω⟨Li(xn+1−xn),vi−vi,n⟩ +
ω⟨Li(xn+1−xn),vi,n+1−vi,n⟩ + (θ−ω)⟨Li(xn+1−xn),vi,n+1−vi⟩≤
⟨Li(xn+2−xn+1),vi−vi,n+1⟩− ω⟨Li(xn+1−xn),vi−vi,n⟩ +
ω∥Li∥
(
µω∥Li∥∥xn+1−xn∥2
2δi
+δi
∥vi,n+1−vi,n∥2
2µω∥Li∥
)
+
(θ−ω)∥Li∥
(
µω∥Li∥∥xn+1−xn∥2
2δi
+δi
∥vi,n+1−vi∥2
2µω∥Li∥
)
=
⟨Li(xn+2−xn+1),vi−vi,n+1⟩− ω⟨Li(xn+1−xn),vi−vi,n⟩ +
θµω∥Li∥2∥xn+1−xn∥2
2δi
+δi
∥vi,n+1−vi,n∥2
2µ + (θ−ω)δi
∥vi,n+1−vi∥2
2µω .
Taking into consideration that
µ2θω
2
m∑
i=1
∥Li∥2
δi
≤ γθ
2 ω≤ γ
2ω and 1 + µ
2 = 1
ω + θ−ω
ω ,
from (24) we obtain for any n≥ 0
1
ωan+1 + γ
2∥xn+2−xn+1∥2≤
an + γ
2ω∥xn+1−xn∥2− θ−ω
ω
(
an+1−
m∑
i=1
δi
2∥vi,n+1−vi∥2
)
+
µ
m∑
i=1
(
⟨Li(xn+2−xn+1),vi−vi,n+1⟩− ω⟨Li(xn+1−xn),vi−vi,n⟩
)
.
As ω≤θ and an+1− ∑m
i=1
δi
2∥vi,n+1−vi∥2≥ 0, we further get after multiplying the last
inequality with ω−n the following Fej´ er-type inequality that holds for anyn≥ 0
ω−(n+1)an+1 + γ
2ω−n∥xn+2−xn+1∥2 +µω−n
m∑
i=1
⟨Li(xn+2−xn+1),vi,n+1−vi⟩≤
ω−nan + γ
2ω−(n−1)∥xn+1−xn∥2 +µω−(n−1)
m∑
i=1
⟨Li(xn+1−xn),vi,n−vi⟩. (25)
12

<!-- page 13 -->
Let N∈ N,N ≥ 2. Summing up the inequality in (25) from n = 0 to N− 1, it yields
ω−NaN + γ
2ω−N +1∥xN−xN +1∥2 +µω−N +1
m∑
i=1
⟨Li(xN +1−xN),vi,N−vi⟩≤
a0 + γ
2ω∥x1−x0∥2 +µω
m∑
i=1
⟨Li(x1−x0),vi,0−vi⟩.
Using that
⟨Li(xN +1−xN),vi,N−vi⟩≥− µ∥Li∥2
4δi
∥xN +1−xN∥2− δi
µ∥vi,N−vi∥2, i= 1,...,m,
this further yields
ω−NaN +ω−N +1
(
γ
2− µ2
4
m∑
i=1
∥Li∥2
δi
)
∥xN−xN +1∥2−ω−N +1
m∑
i=1
δi∥vi,N−vi∥2≤
a0 + γ
2ω∥x1−x0∥2 +µω
m∑
i=1
⟨Li(x1−x0),vi,0−vi⟩. (26)
Taking into account the way µ has been chosen, we have
γ
2− µ2
4
m∑
i=1
∥Li∥2
δi
≥ γ
2− γ
4 > 0,
hence, after multiplying (26) with ω−N it yields
aN−ω
m∑
i=1
δi∥vi,N−vi∥2≤ωN
(
a0 + γ
2ω∥x1−x0∥2 +µω
m∑
i=1
⟨Li(x1−x0),vi,0−vi⟩
)
.
The conclusion follows by taking into account the deﬁnition of the sequence ( an)n≥0. ■
Remark 12 If A +C is γ-strongly monotone for γ > 0 and B−1
i +D−1
i is δi-strongly
monotone for δi > 0, i = 1,...,m , then there exists at most one primal-dual solution to
Problem 9. Hence, if ( x,v1,..., vm) is a primal-dual solution to Problem 9, then x is the
unique solution to the primal inclusion (18) and ( v1,..., vm) is the unique solution to the
dual inclusion (19).
3 Convex optimization problems
The aim of this section is to show that the two algorithms proposed in this paper and
investigated from the point of view of their convergence properties can be employed when
solving a primal-dual pair of convex optimization problems.
For a functionf :H→ R, where R := R∪{±∞} is the extended real line, we denote by
domf ={x∈H :f(x)< +∞} its eﬀective domain and say that f is proper if domf̸=∅
andf(x)̸=−∞ for all x∈H . We denote by Γ(H) the family of proper convex and lower
semi-continuous extended real-valued functions deﬁned on H. Let f∗ :H→ R, f∗(u) =
supx∈H{⟨u,x⟩−f(x)} for allu∈H , be the conjugate function off. The subdiﬀerential of
13

<!-- page 14 -->
f atx∈H , with f(x)∈ R, is the set ∂f (x) :={v∈H :f(y)≥f(x) +⟨v,y−x⟩∀y∈H} .
We take by convention∂f (x) :=∅, if f(x)∈{±∞} . Notice that if f∈ Γ(H), then ∂f is a
maximally monotone operator (cf. [16]) and it holds ( ∂f )−1 =∂f∗. For f,g :H→ R two
proper functions, we consider theirinﬁmal convolution, which is the functionf □g :H→ R,
deﬁned by (f □g)(x) = infy∈H{f(y) +g(x−y)}, for all x∈H .
LetS⊆H be a nonempty set. The indicator function ofS,δS :H→ R, is the function
which takes the value 0 on S and +∞ otherwise. The subdiﬀerential of the indicator
function is the normal cone of S, that is NS(x) = {u∈H :⟨u,y−x⟩≤ 0∀y∈ S}, if
x∈S and NS(x) =∅ for x /∈S.
When f ∈ Γ(H) and γ > 0, for every x∈H we denote by prox γf (x) the proximal
point of parameter γ of f at x, which is the unique optimal solution of the optimization
problem
inf
y∈H
{
f(y) + 1
2γ∥y−x∥2
}
. (27)
Notice that Jγ∂f = (IdH +γ∂f )−1 = proxγf , thus prox γf :H → His a single-valued
operator fulﬁlling the extended Moreau’s decomposition formula
proxγf +γ prox(1/γ)f∗◦γ−1 IdH = IdH. (28)
Let us also recall that the function f :H→ R is said to be γ-strongly convex for γ >0,
if f− γ
2∥·∥ 2 is a convex function. Let us mention that this property implies γ-strong
monotonicity of ∂f (see [1, Example 22.3]).
Finally, we notice that for f =δS, where S⊆H is a nonempty convex and closed set,
it holds
JγNS =JNS =J∂δS = (IdH +NS)−1 = proxδS =PS, (29)
wherePS :H→ C denotes the projection operator onS (see [1, Example 23.3 and Example
23.4]).
In order to investigate the applicability of the algorithm introduced in Subsection 2.1
we consider the following primal-dual pair of convex optimization problems.
Problem 13 LetH be a real Hilbert space, z∈H , f∈ Γ(H) and h :H→ R a convex
and diﬀerentiable function with a η-Lipschitzian gradient for η >0. Let m be a strictly
positive integer and for any i∈{ 1,...,m} letGi be a real Hilbert space, ri∈G i,gi∈ Γ(Gi)
and Li :H→G i a nonzero linear continuous operator. Consider the convex optimization
problem
inf
x∈H
{
f(x) +
m∑
i=1
gi(Lix−ri) +h(x)−⟨x,z⟩
}
(30)
and its Fenchel-type dual problem
sup
vi∈Gi,i=1,...,m
{
−
(
f∗□h∗)
(
z−
m∑
i=1
L∗
ivi
)
−
m∑
i=1
(
g∗
i (vi) +⟨vi,ri⟩
)
}
. (31)
Considering maximal monotone operators
A =∂f,C =∇h and Bi =∂gi,i = 1,...,m,
14

<!-- page 15 -->
the monotone inclusion problem (5) reads
ﬁnd x∈H such that z∈∂f (x) +
m∑
i=1
L∗
i (∂gi(Lix−ri)) +∇h(x), (32)
while the dual inclusion problem (6) reads
ﬁnd v1∈G 1,..., vm∈G m such that∃x∈H :
{ z− ∑m
i=1L∗
ivi∈∂f (x) +∇h(x)
vi∈∂gi(Lix−ri),i = 1,...,m. (33)
If (x,v1,..., vm)∈H×G 1×...×Gm is a primal-dual solution to (32)-(33), namely,
z−
m∑
i=1
L∗
ivi∈∂f (x) +∇h(x) and vi∈∂gi(Lix−ri),i = 1,...,m, (34)
then x is an optimal solution of the problem (30), ( v1,..., vm) is an optimal solution of
(31) and the optimal objective values of the two problems coincide. Notice that (34) is
nothing else than the system of optimality conditions for the primal-dual pair of convex
optimization problems (30)-(31).
In case a qualiﬁcation condition is fulﬁlled, these optimality conditions are also neces-
sary. For the readers convenience, let us present some qualiﬁcation conditions which are
suitable in this context. One of the weakest qualiﬁcation conditions of interiority-type
reads (see, for instance, [13, Proposition 4.3, Remark 4.4])
(r1,...,r m)∈ sqri
( m∏
i=1
domgi−{ (L1x,...,L mx) :x∈ domf}
)
. (35)
Here, forH a real Hilbert space and S⊆H a convex set, we denote by
sqriS :={x∈S :∪λ>0λ(S−x) is a closed linear subspace of H}
its strong quasi-relative interior . Notice that we always have int S⊆ sqriS (in general
this inclusion may be strict). If H is ﬁnite-dimensional, then sqri S coincides with ri S,
the relative interior of S, which is the interior of S with respect to its aﬃne hull. The
condition (35) is fulﬁlled if (i) dom gi = Gi, i = 1,...,m or (ii) H andGi are ﬁnite-
dimensional and there exists x∈ ri domf such that Lix−ri ∈ ri domgi, i = 1,...,m
(see [13, Proposition 4.3]). Another useful and easily veriﬁable qualiﬁcation condition
guaranteing the optimality conditions (34) has the following formulation: there exists
x′∈ domf∩ ⋂m
i=1L−1
i (ri + domgi) such that gi is continuous at Lix′−ri, i = 1,...,m
(see [3, Remark 2.5] and [5]).
The following two statements are particular instances of Algorithm 5 and Theorem 7,
respectively.
Algorithm 14
Initialization: Choose τ0> 0,σi,0> 0, i = 1,..,m , such that
τ0< 2γ/η, λ≥η + 1, τ0
∑m
i=1σi,0∥Li∥2≤
√
1 +τ0(2γ−ητ0)/λ
and (x0,v 1,0,...,v m,0)∈H×G 1×...×G m.
For n≥ 0 set: xn+1 = prox(τn/λ)f
[
xn− (τn/λ)
(∑m
i=1L∗
ivi,n +∇h(xn)−z
)]
θn = 1/
√
1 +τn(2γ−ητn)/λ
yn =xn+1 +θn(xn+1−xn)
vi,n+1 = proxσi,ng∗
i
[vi,n +σi,n(Liyn−ri)], i = 1,...,m
τn+1 =θnτn, σi,n+1 =σi,n/θn+1, i = 1,...,m.
15

<!-- page 16 -->
Theorem 15 Suppose that f +h is γ-strongly convex for γ > 0 and the qualiﬁcation
condition (35) holds. Then there exists a unique optimal solution x to (30), an optimal
solution (v1,..., vm) to (31) fulﬁlling the optimality conditions (34) and such that the op-
timal objective values of the problems (30) and (31) coincide. The sequences generated by
Algorithm 14 fulﬁll for any n≥ 0
λ∥xn+1−x∥2
τ 2
n+1
+
(
1−τ1
m∑
i=1
σi,0∥Li∥2
) m∑
i=1
∥vi,n−vi∥2
τ1σi,0
≤
λ∥x1−x∥2
τ 2
1
+
m∑
i=1
∥vi,0−vi∥2
τ1σi,0
+∥x1−x0∥2
τ 2
0
+ 2
τ0
m∑
i=1
⟨Li(x1−x0),vi,0−vi⟩.
Moreover, lim
n→+∞
nτn = λ
γ , hence one obtains for (xn)n≥0 an order of convergence ofO( 1
n).
Remark 16 The uniqueness of the solution of (30) in the above theorem follows from [1,
Corollary 11.16].
Remark 17 In case h(x) = 0 for all x∈H , one has to choose in Algorithm 14 as initial
points τ0 > 0,σi,0 > 0, i = 1,..,m , with τ0
∑m
i=1σi,0∥Li∥2≤
√
1 + 2τ0γ/λ and λ≥ 1
and to update the sequence ( θn)n≥0 via θn = 1/
√
1 + 2τnγ/λ for any n≥ 0, in order to
obtain a suitable iterative scheme for solving the pair of primal-dual optimization problems
(30)-(31) with the same convergence behavior as of Algorithm 14.
We turn now our attention to the algorithm introduced in Subsection 2.2 and consider
to this end the following primal-dual pair of convex optimization problems.
Problem 18 LetH be a real Hilbert space,z∈H ,f∈ Γ(H) andh :H→ R a convex and
diﬀerentiable function with aη-Lipschitzian gradient forη >0. Let m be a strictly positive
integer and for any i∈{ 1,...,m} letGi be a real Hilbert space, ri∈G i, gi,li∈ Γ(Gi) such
that li is ν−1
i -strongly convex for νi > 0 and Li :H→G i a nonzero linear continuous
operator. Consider the convex optimization problem
inf
x∈H
{
f(x) +
m∑
i=1
(gi□li)(Lix−ri) +h(x)−⟨x,z⟩
}
(36)
and its Fenchel-type dual problem
sup
vi∈Gi,i=1,...,m
{
−
(
f∗□h∗)
(
z−
m∑
i=1
L∗
ivi
)
−
m∑
i=1
(
g∗
i (vi) +l∗
i (vi) +⟨vi,ri⟩
)
}
. (37)
Considering the maximal monotone operators
A =∂f,C =∇h,Bi =∂gi and Di =∂li,i = 1,...,m,
according to [1, Proposition 17.10, Theorem 18.15], D−1
i =∇l∗
i is a monotone and νi-
Lipschitzian operator for i = 1,...,m . The monotone inclusion problem (18) reads
ﬁnd x∈H such that z∈∂f (x) +
m∑
i=1
L∗
i ((∂gi□∂li)(Lix−ri)) +∇h(x), (38)
16

<!-- page 17 -->
while the dual inclusion problem (19) reads
ﬁnd v1∈G 1,..., vm∈G m such that∃x∈H :
{ z− ∑m
i=1L∗
ivi∈∂f (x) +∇h(x)
vi∈ (∂gi□∂li)(Lix−ri),i = 1,...,m.
(39)
If (x,v1,..., vm)∈H×G 1×...×Gm is a primal-dual solution to (38)-(39), namely,
z−
m∑
i=1
L∗
ivi∈∂f (x) +∇h(x) and vi∈ (∂gi□∂li)(Lix−ri),i = 1,...,m, (40)
then x is an optimal solution of the problem (36), ( v1,..., vm) is an optimal solution of
(37) and the optimal objective values of the two problems coincide. Notice that (40) is
nothing else than the system of optimality condition for the primal-dual pair of convex
optimization problems (36)-(37).
The assumptions made on li guarantees that gi□li∈ Γ(Gi) (see [1, Corollary 11.16,
Proposition 12.14]) and, since dom( gi□li) = dom gi + domli,i = 1,...,m , one can can
consider the following qualiﬁcation condition of interiority-type in order to guarantee (40)
(r1,...,r m)∈ sqri
( m∏
i=1
(domgi + domli)−{ (L1x,...,L mx) :x∈ domf}
)
. (41)
The following two statements are particular instances of Algorithm 10 and Theorem
11, respectively.
Algorithm 19
Initialization: Choose µ> 0 such that
µ≤ min
{
γ2/η2,δ 2
1/ν2
1,...,δ 2
m/ν2
m,
√
γ/ (∑m
i=1∥Li∥2/δi)
}
,
τ =µ/(2γ), σi =µ/(2δi), i = 1,..,m ,
θ∈ [2/(2 +µ), 1] and (x0,v 1,0,...,v m,0)∈H×G 1×...×G m.
For n≥ 0 set: xn+1 = proxτf
[
xn−τ
(∑m
i=1L∗
ivi,n +∇h(xn)−z
)]
yn =xn+1 +θ(xn+1−xn)
vi,n+1 = proxσig∗
i
[vi,n +σi(Liyn−∇l∗
i (vi,n)−ri)], i = 1,...,m.
Theorem 20 Suppose that f +h is γ-strongly convex for γ > 0, g∗
i +l∗
i is δi-strongly
convex for δi > 0, i = 1,...,m , and the qualiﬁcation condition (41) holds. Then there
exists a unique optimal solution x to (36), a unique optimal solution (v1,..., vm) to (37)
fulﬁlling the optimality conditions (40) and such that the optimal objective values of the
problems (36) and (37) coincide. The sequences generated by Algorithm 19 fulﬁll for any
n≥ 0
γ∥xn+1−x∥2 + (1−ω)
m∑
i=1
δi∥vi,n−vi∥2≤
ωn
(
γ∥x1−x∥2 +
m∑
i=1
δi∥vi,0−vi∥2 + γ
2ω∥x1−x0∥2 +µω
m∑
i=1
⟨Li(x1−x0),vi,0−vi⟩
)
,
where 0<ω = 2(1+θ)
4+µ < 1.
17

<!-- page 18 -->
4 Numerical experiments
In this section we illustrate the applicability of the theoretical results in the context of two
numerical experiments in image processing and support vector machines classiﬁcation.
4.1 Image processing
In this subsection we compare the numerical performances of Algorithm 14 with the ones
in the iterative scheme in Theorem 2 for an image denoising problem. To this end we treat
the nonsmooth regularized convex optimization problem
inf
x∈[0,1]k
{ 1
2∥x−b∥2 +λ1TV (x) +λ2∥Wx∥1
}
, (42)
where TV : Rk → R denotes a discrete isotropic total variation, W : Rk → Rk a the
discrete Haar wavelet transform with four levels, λ1,λ 2> 0 the regularization parameters
and b∈ Rk the observed noisy image. Notice that we consider images of size k =M×N
as vectors x∈ Rk, where each pixel denoted by xi,j, 1≤i≤M, 1≤j≤N, ranges in the
closed interval from 0 (pure black) to 1 (pure white).
Two popular choices for the discrete total variation functional are the isotropic total
variationTV iso : Rk→ R,
TV iso(x) =
M−1∑
i=1
N−1∑
j=1
√
(xi+1,j−xi,j)2 + (xi,j+1−xi,j)2
+
M−1∑
i=1
|xi+1,N−xi,N| +
N−1∑
j=1
|xM,j +1−xM,j|,
and the anisotropic total variation TV aniso : Rk→ R,
TV aniso(x) =
M−1∑
i=1
N−1∑
j=1
|xi+1,j−xi,j| +|xi,j+1−xi,j|
+
M−1∑
i=1
|xi+1,N−xi,N| +
N−1∑
j=1
|xM,j +1−xM,j|,
where in both cases reﬂexive (Neumann) boundary conditions are assumed. Obviously, in
both situations the qualiﬁcation condition in Theorem 15 is fulﬁlled.
DenoteY = Rk× Rk and deﬁne the linear operatorL : Rk→Y ,xi,j↦→ (L1xi,j,L 2xi,j),
where
L1xi,j =
{ xi+1,j−xi,j, if i<M
0, if i =M and L2xi,j =
{ xi,j+1−xi,j, if j <N
0, if j =N .
The operator L represents a discretization of the gradient in horizontal and vertical direc-
tion. One can easily check that ∥L∥2≤ 8 and that its adjoint L∗ :Y→ Rk is as easy to
implement as the operator itself (cf. [10]). Moreover, since W is an orthogonal wavelet, it
holds∥W∥ = 1.
18

<!-- page 19 -->
(a) Noisy image, σ = 0.06
 (b) Noisy image, σ = 0.12
(c) Denoised image, λ1 = 0.035,
λ2 = 0.01, isotropic TV
(d) Denoised image, λ1 = 0 .07,
λ2 = 0.01, isotropic TV
Figure 1: The noisy images in (a) and (b) were obtained after adding white Gaussian noise
with standard deviation σ = 0.06 and σ = 0.12, respectively, to the original 256 × 256
lichtenstein test image. The outputs of Algorithm 14 after 100 iterations when solving
(42) with isotropic total variation are shown in (c) and (d), respectively.
When considering the isotropic total variation, the problem (42) can be formulated as
inf
x∈Rk
{f(x) +g1(Lx) +g2(Wx )}, (43)
wheref : Rk→ R,f (x) = 1
2∥x−b∥2 +δ[0,1]k(x) is 1-strongly convex,g1 :Y→ R is deﬁned
as g1(u,v ) = λ1∥(u,v )∥×, where ∥(·,·)∥× :Y→ R,∥(u,v )∥× = ∑M
i=1
∑N
j=1
√
u2
i,j +v2
i,j,
is a norm on the Hilbert space Y and g2 : Rk→ R,g 2(x) = λ2∥x∥1. Take (p,q )∈Y and
σ >0. We have
proxσf (p) =P[0,1]k
(
(1 +σ)−1(p +σb)
)
.
Moreover,g∗
1(p,q ) =δS(p,q ) and
proxσg∗
1
(p,q ) =PS (p,q ),
where (cf. [6])
S =


(p,q )∈Y : max
1≤i≤M
1≤j≤N
√
p2
i,j +q2
i,j≤λ1



19

<!-- page 20 -->
ε = 10−4 isotropic TV anisotropic TV
σ = 0.06 σ = 0.12 σ = 0.06 σ = 0.12
Algorithm in [21] 373 329 383 388
Algorithm 14 95 180 126 255
Table 1: Comparison of the algorithm in [21] and its modiﬁcation, Algorithm 14, when
denoising the images in Figure 1 (a) and (b) for isotropic TV and anisotropic TV. The
entries represent the number of iterations needed for attaining a root mean squared error
for the primal iterates below the tolerance level of ε = 10−4.
1 5 10 15 20
0.5
1.0
anisotropicisotropicanisotropic modiﬁedisotropic modiﬁed
Iterations
RMSE
Figure 2: RMSE curves for image denoising: the case of white Gaussian noise with stan-
dard deviation σ = 0.06
and the projection operator PS :Y→ S is deﬁned via
(pi,j,qi,j)↦→λ1
(pi,j,qi,j)
max
{
λ1,
√
p2
i,j +q2
i,j
}, 1≤i≤M, 1≤j≤N.
Finally,g∗
2(p) =δ[−λ2,λ2]k(p), hence
proxσg∗
2
(p) =P[−λ2,λ2]k(p)∀p∈ Rk.
On the other hand, when considering the anisotropic total variation, the problem (42)
can be formulated as
inf
x∈Rk
{f(x) + ˜g1(Lx) +g2(Wx )}, (44)
where the functions f,g 2 are taken as above and ˜g1 :Y →R is deﬁned as g1(u,v ) =
λ1∥(u,v )∥1. For every (p,q )∈Y we have ˜g∗
1(p) =δ[−λ1,λ1]k×[−λ1,λ1]k(p,q ) and
proxσ˜g∗
1
(p,q ) =P[−λ1,λ1]k×[−λ1,λ1]k(p,q ).
We experimented with the 256× 256 lichtenstein test image to which we added white
Gaussian noise with standard deviation σ = 0.06 and σ = 0.12, respectively. We solved
(42) by Algorithm 14 (with the modiﬁcations mentioned in Remark 17) for both instances
of the discrete total variation functional. For the ﬁrst noisy image (added noise with
standard deviation σ = 0.06), we took as regularization parameters λ1 = 0.035 and
λ2 = 0.01 and for the second one (added noise with standard deviationσ = 0.12),λ1 = 0.07
20

<!-- page 21 -->
1 5 10 15 20
0.5
1.0
anisotropicisotropicanisotropic modiﬁedisotropic modiﬁed
Iterations
RMSE
Figure 3: RMSE curves for image denoising: the case of white Gaussian noise with stan-
dard deviation σ = 0.12
andλ2 = 0.01. As initial choices in Algorithm 14 we opted forλ = 1,τ0 = 50,σ 1,0 = 0.0241
andσ2,0 = 0.008. The reconstructed images after 100 iterations for isotropic total variation
are shown in Figure 1.
We compared Algorithm 14 from the point of view of the number of iterations needed
for a good recovery with the iterative method from Theorem 2 (see [21]) for both isotropic
and anisotropic total variation. For the iterative scheme from [21] we have chosen for
both noisy images and both discrete total variation functionals the optimal initializations
τ = 0.35,σ 1 = 0.2 and σ2 = 0.01.
The comparisons concerning the number of iterations needed for a good recovery were
made via the root mean squared error (RMSE) . We refer the reader to Table 1 for the
achieved results, which show that Algorithm 14 outperforms the iterative scheme in [21].
Figures 2 and 3 show the evolution of the RMSE curves when solving (43) and (44) with
the algorithm from [21] and with its modiﬁed version, Algorithm 14, respectively.
4.2 Support vector machines classiﬁcations
The numerical experiments we present in this subsection refer to the class of kernel based
learning methods and address the problem of classifying images via support vector ma-
chines.
We make use of a data set of 11800 training images and 1983 test images of size
28×28 from the website http://www.cs.nyu.edu/∼roweis/data.html. The problem consists
in determining a decision function based on a pool of handwritten digits showing either
the number eight or the number nine, labeled by −1 and +1, respectively (see Figure
4). We evaluate the quality of the decision function on a test data set by computing the
percentage of misclassiﬁed images. Notice that we use only a half of the available images
from the training data set, in order to reduce the computational eﬀort.
Figure 4: A sample of images belonging to the classes −1 and +1, respectively.
The classiﬁer functional f is assumed to be an element of the Reproducing Kernel
21

<!-- page 22 -->
Hilbert Space (RKHS) Hκ, which in our case is induced by the symmetric and ﬁnitely
positive deﬁnite Gaussian kernel function
κ : Rd× Rd→ R, κ(x,y ) = exp
(
−∥x−y∥2
2σ2
)
.
Let⟨·,·⟩κ be the inner product on Hκ,∥·∥ κ the corresponding norm and K∈ Rn×n the
Gram matrix with respect to the training data set Z ={(X1,Y 1),..., (Xn,Yn)}⊆ Rd×
{+1,−1}, namely the symmetric and positive deﬁnite matrix with entriesKij =κ(Xi,Xj)
for i,j = 1,...,n . We make use of the hinge loss function v : R× R→ R,v (x,y ) =
max{1−xy, 0}, which penalizes the deviation between the predicted value f(x) and the
true value y∈{ +1,−1}. The smoothness of the decision function f∈H κ is employed
by means of the smoothness functional Ω : Hκ→ R, Ω(f) = ∥f∥2
κ, taking high values
for non-smooth functions and low values for smooth ones. The decision function is the
optimal solution of the Tikhonov regularization problem
inf
f∈Hκ
{
1
2Ω(f) +C
n∑
i=1
v(f(Xi),Yi)
}
, (45)
where C > 0 denotes the regularization parameter controlling the tradeoﬀ between the
loss function and the smoothness functional.
By taking into account the representer theorem (see [18]), there exists a vector c =
(c1,...,c n)T∈ Rn such that the minimizerf of (45) can be expressed as a kernel expansion
in terms of the training data, namely, it holds f(·) = ∑n
i=1ciκ(·,Xi). In this case the
smoothness functional becomes Ω( f) = ∥f∥2
κ = ⟨f,f⟩κ = ∑n
i=1
∑n
j=1cicjκ(Xi,Xj) =
cTKc and for i = 1,...,n it holds f(Xi) = ∑n
j=1cjκ(Xi,Xj) = (Kc)i. This means that in
order to ﬁnd the decision function it is enough to solve the convex optimization problem
inf
c∈Rn
{
h(c) +
n∑
i=1
gi(Kc)
}
, (46)
where h : Rn→ R, h(c) = 1
2cTKc is convex, diﬀerentiable with ∥K∥-Lipschitz gradient
and gi : Rn→ R, gi(c) = Cv(ci,Yi), i = 1,...,n, are convex functions. The optimization
problem (46) has the structure of (30), where f is taken to be identically 0. Notice that
in this case the function f +h =h is γ-strongly convex with γ =λmin, where λmin is the
smallest eigenvalue of the matrix K. Due to the continuity of the functions gi,i = 1,...,n ,
the qualiﬁcation condition required in Theorem 15 is guaranteed. We solved (46) by
Algorithm 14 and used for µ> 0 the following formula (see [8])
proxµg∗
i
(c) =
(
0,...,P Yi[−C,0](ci−µYi),..., 0
)T.
With respect to the considered data set, we denote by D={(Xi,Yi),i = 1,..., 5899}⊆
R784×{ +1,−1} the set of available training data consisting of 2974 images in the class
−1 and 2925 images in the class +1. A sample from each class of images is shown in
Figure 4. The images have been vectorized and normalized by dividing each of them by
the quantity
(
1
5899
∑5899
i=1 ∥Xi∥2
)1
2
.
As initial choices in Algorithm 14 we took τ0 = 0.99 2γ
∥K∥,λ =∥K∥ + 1 and σi,0 =√
1 +τ0(2γ−∥K∥τ0)/λ/(nτ0∥K∥2), i = 1,...,n , and tested diﬀerent combinations of the
22

<!-- page 23 -->
σ 0.15 0 .175 0 .2 0 .25 0 .5
training error 0 0 0 .14 4 .09 49 .55
test error 1 .36 2 .12 3 .33 5 .60 49 .12
Table 2: Misclassiﬁcation rate in percentage for diﬀerent choices of the kernel parameter
σ and for both the training and the test data set.
kernel parameter σ over a ﬁxed number of 1500 iterations. In Table 2 we present the
misclassiﬁcation rate in percentage for the training and for the test data (the error for the
training data is less than the one for the test data). One can notice that for certain choices
ofσ the misclassiﬁcation rate outperforms the one reported in the literature dealing with
numerical methods for support vector classiﬁcation. Let us mention that the numerical
results are given for the case C = 1. We tested also other choices for C, however we did
not observe great impact on the results.
References
[1] H.H. Bauschke, P.L. Combettes, Convex Analysis and Monotone Operator Theory in
Hilbert Spaces, CMS Books in Mathematics, Springer, New York, 2011
[2] J.M. Borwein and J.D. Vanderwerﬀ, Convex Functions: Constructions, Characteri-
zations and Counterexamples , Cambridge University Press, Cambridge, 2010
[3] R.I. Bot ¸, Conjugate Duality in Convex Optimization , Lecture Notes in Economics
and Mathematical Systems, Vol. 637, Springer, Berlin Heidelberg, 2010
[4] R.I. Bot ¸, E.R. Csetnek, Regularity conditions via generalized interiority notions in
convex optimization: new achievements and their relation to some classical state-
ments, Optimization 61(1), 35–65, 2012
[5] R.I. Bot ¸, E.R. Csetnek, A. Heinrich,A primal-dual splitting algorithm for fnding zeros
of sums of maximally monotone operators , arXiv:1206.5953, 2012
[6] R.I. Bot ¸, C. Hendrich, Convergence analysis for a primal-dual monotone + skew
splitting algorithm with applications to total variation minimization , arXiv:1211.1706,
2012
[7] R.I. Bot ¸, C. Hendrich, A Douglas-Rachford type primal-dual method for solving
inclusions with mixtures of composite and parallel-sum type monotone operators ,
arXiv:1212.0326, 2012
[8] R.I. Bot ¸, C. Hendrich,A variable smoothing algorithm for solving convex optimization
problems, arXiv:1207.3254, 2012
[9] L.M. Brice˜ no-Arias, P.L. Combettes,A monotone + skew splitting model for compos-
ite monotone inclusions in duality , SIAM Journal on Optimization 21(4), 1230-1250,
2011
23

<!-- page 24 -->
[10] A. Chambolle, An algorithm for total variation minimization and applications , Jour-
nal of Mathematical Imaging and Vision, 20(1–2), 89–97, 2004
[11] A. Chambolle, T. Pock, A ﬁrst-order primal-dual algorithm for convex problems with
applications to imaging, Journal of Mathematical Imaging and Vision 40(1), 120-145,
2011
[12] P.L. Combettes, Solving monotone inclusions via compositions of nonexpansive aver-
aged operators, Optimization 53(5-6), 475–504, 2004
[13] P.L. Combettes, J.-C. Pesquet, Primal-dual splitting algorithm for solving inclusions
with mixtures of composite, Lipschitzian, and parallel-sum type monotone operators ,
Set-Valued and Variational Analysis 20(2), 307–330, 2012
[14] J. Douglas, H.H. Rachford, On the numerical solution of the heat conduction problem
in 2 and 3 space variables, Transactions of the American Mathematical Society 82,
421–439, 1956
[15] I. Ekeland, R. Temam, Convex Analysis and Variational Problems , North-Holland
Publishing Company, Amsterdam, 1976
[16] R.T. Rockafellar, On the maximal monotonicity of subdiﬀerential mappings , Paciﬁc
Journal of Mathematics 33(1), 209–216, 1970
[17] R.T. Rockafellar, Monotone operators and the proximal point algorithm , SIAM Jour-
nal on Control and Optimization 14(5), 877-898, 1976
[18] B. Sch¨ olkopf, A.J. Smola,Learning with Kernels. Support Vector Machines, Regular-
ization, Optimization, and Beyond , MIT Press, Cambridge, 2002
[19] S. Simons, From Hahn-Banach to Monotonicity , Springer, Berlin, 2008
[20] P. Tseng, A modiﬁed forward-backward splitting method for maximal monotone map-
pings, SIAM Journal on Control and Optimization 38(2), 431-446, 2000
[21] B.C. V˜ u,A splitting algorithm for dual monotone inclusions involving cocoercive op-
erators, Advances in Computational Mathematics, DOI 10.1007/s10444-011-9254-8
[22] C. Z˘ alinescu,Convex Analysis in General Vector Spaces, World Scientiﬁc, Singapore,
2002
24