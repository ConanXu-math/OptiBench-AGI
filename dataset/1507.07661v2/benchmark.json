{
  "paper_name": "Convex Integration and Legendrian Approximation of Curves",
  "arxiv_id": "1507.07661v2",
  "outline": {
    "objective": "0",
    "constraints": [
      "\\dot{a}(t) \\neq 0",
      "|v - y(t)u| \\leq \\varepsilon \\min\\{|u|, |u|^2\\}",
      "\\frac{1}{2\\pi} \\oint_{S^1} \\gamma(t, s) \\, ds = (\\dot{x}(t), \\dot{z}(t))",
      "\\left| (a(t), c(t)) - (x(t), z(t)) \\right| \\leq \\varepsilon"
    ],
    "variables": [
      "a(t)",
      "b(t)",
      "c(t)",
      "γ(t,s)"
    ],
    "notation_table": [
      {
        "symbol": "a(t)",
        "dimension": "3",
        "description": "decision variables for the approximating Legendrian curve"
      },
      {
        "symbol": "b(t)",
        "dimension": "1",
        "description": "decision variable for the y-component approximation"
      },
      {
        "symbol": "c(t)",
        "dimension": "3",
        "description": "decision variables for the approximating Legendrian curve"
      },
      {
        "symbol": "x(t)",
        "dimension": "3",
        "description": "given smooth curve to approximate"
      },
      {
        "symbol": "y(t)",
        "dimension": "1",
        "description": "given smooth curve to approximate"
      },
      {
        "symbol": "z(t)",
        "dimension": "1",
        "description": "given smooth curve to approximate"
      },
      {
        "symbol": "\\varepsilon > 0",
        "dimension": "1",
        "description": "small positive parameter controlling approximation accuracy"
      },
      {
        "symbol": "n \\in \\mathbb{N}",
        "dimension": "1",
        "description": "integer parameter controlling discretization of loop integration"
      },
      {
        "symbol": "\\gamma(t,s)",
        "dimension": "R^2",
        "description": "smooth integral kernel defining curve perturbation"
      },
      {
        "symbol": "r > 0",
        "dimension": "1",
        "description": "measure normalizing factor for correction term"
      }
    ]
  },
  "prove_cot": "",
  "pseudocode": "function generate_synthetic_data(seed):\n    Initialize random number generator with seed\n    Generate x(t), y(t), z(t) as smooth curves (e.g., sine/cosine)\n    Set ε = 0.1, n = 32, r = 1.0\n    Return {x: array, y: array, z: array, ε: float, n: int, r: float}\n\nfunction solve(data):\n    Extract x, y, z, ε, n, r from data\n    Define objective function that minimizes error |(a,c) - (x,z)|\n    Define constraints:\n        - a_dot ≠ 0 (enforced via penalty or discretization)\n        - |v - y*u| ≤ ε*min(|u|, |u|^2) (discretized in s, t)\n        - average of γ(t,s) over S^1 equals (dot_x, dot_z)\n    Use scipy.optimize.minimize with appropriate bounds and constraints\n    Return {optimal_value, solution} where solution is {a, b, c, γ}",
  "pycode": "import numpy as np\nfrom scipy.optimize import minimize\n\n\ndef generate_synthetic_data(seed: int = 42) -> dict:\n    \"\"\"Generate synthetic test data for the Legendrian approximation problem.\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 2*np.pi, 100)\n    # Smooth target curves\n    x_t = np.sin(t) + 0.2 * np.cos(3*t)\n    y_t = 0.5 * np.sin(t) + 0.2 * np.sin(2*t)\n    z_t = 0.3 * np.cos(t) + 0.1 * np.sin(4*t)\n    \n    data = {\n        'x': x_t,\n        'y': y_t,\n        'z': z_t,\n        'ε': 0.1,\n        'n': 32,\n        'r': 1.0\n    }\n    return data\n\n\ndef solve(data: dict) -> dict:\n    \"\"\"Solve the Legendrian curve approximation optimization problem.\"\"\"\n    x_t = data['x']\n    y_t = data['y']\n    z_t = data['z']\n    ε = data['ε']\n    n = data['n']\n    r = data['r']\n    \n    # Discretize time for finite difference approximations\n    dt = 2 * np.pi / len(x_t)\n    t = np.arange(0, 2*np.pi, dt)\n    \n    # If we were using continuous variables, this would be a very complex PDE-constrained problem.\n    # But to satisfy the constraints with scipy.optimize, we'll approximate it with a discretized parameter space.\n    \n    # Approximate decision variables as vectors\n    # a(t) and c(t) are 3D in theory; here we use single dimensional parametrizations for demo\n    # In practice, this would require numerical integration over t and s\n    \n    def objective(vars):\n        \"\"\"Objective: minimize deviation from (x(t), z(t))\"\"\"\n        a_vals = vars[:len(x_t)]  # a(t) values\n        c_vals = vars[len(x_t):2*len(x_t)]  # c(t) values\n        b_val = vars[2*len(x_t)]  # b(t)\n        # Note: γ(t,s) is not included since it's integrated and constrained implicitly\n        # We instead use enriched parameters for simplified demo\n        \n        # Example cost: weighted distance between (a(t),c(t)) and (x(t),z(t))\n        dist = np.sqrt((a_vals - x_t)**2 + (c_vals - z_t)**2)\n        return np.sum(dist)\n\n    # Constraint functions (for SciPy minimize)\n    constraint_list = []\n    \n    def constraint_non_zero_a_deriv(vars):\n        \"\"\"Approximate derivative of a(t) using forward differences\"\"\"\n        a_vals = vars[:len(x_t)]\n        diff = np.diff(a_vals)  # discrete derivative\n        return np.sum(np.abs(diff))  # conservation > 0 to avoid zero\n\n    constraint_list.append({'type': 'ineq', 'fun': lambda vars: constraint_non_zero_a_deriv(vars)})\n    \n    # MAP FOR: |v - y(t)u| <= ε min(|u|, |u|^2). This must be augmented as an inequality per point.\n    def constrain_y_v(vars):\n        \"\"\"For each t, enforce |v - y(t)*u| <= ε*min(|u|, |u|^2)\"\"\"\n        a_vals = vars[:len(x_t)]\n        c_vals = vars[len(x_t):2*len(x_t)]\n        b_val = vars[2*len(x_t)]\n        \n        v = np.array([b_val])  # example u, assume scalar behavior\n        u_vals = np.array([np.mean(a_vals)])  # example — adjust as needed\n        \n        left_hand_side = np.abs(v - y_t * u_vals)\n        right_hand_side = ε * np.minimum(np.abs(u_vals), np.square(u_vals))\n        return np.sum(left_hand_side - right_hand_side)\n\n    constraint_list.append({'type': 'ineq', 'fun': constrain_y_v})\n    \n    # Uniformly sample s values to compute integral constraint\n    s = np.linspace(0, 2*np.pi, n+1)[:-1]  # cycle through s on unit circle\n    \n    # For gamma constraint: we need to map macroscopic ==> microscopic formulation\n    # Since sf-opt only supports bound/opt-val constraints, simulate by adding proxy var\n    \n    def constrain_gamma_integral(vars):\n        \"\"\"Enforce that integral of γ(t,s) over s = (dx/dt, dz/dt)\"\"\"\n        a_vals = vars[:len(x_t)]\n        c_vals = vars[len(x_t):2*len(x_t)]\n        b_val = vars[2*len(x_t)]\n        \n        da_dt_approx = np.gradient(a_vals, dt)\n        dc_dt_approx = np.gradient(c_vals, dt)\n        target = np.stack([da_dt_approx, dc_dt_approx]).flatten()\n        \n        # As placeholder: define COVARIATEs ready for moment-by-moment constraining\n        # In reality, this requires modeling kernel γ(t,s) geometrically --> impossible here without advanced representation.\n        # Instead, enforce that mean of deviations matches transformed rate vector\n        residual = np.linalg.norm(target - np.full_like(target, 0))  # dummy for now\n        return residual  \n\n    constraint_list.append({'type': 'eq', 'fun': constrain_gamma_integral})\n    \n    # Initial guess: copy target paths + add small noise\n    init_guess = np.concatenate([x_t + 0.01*np.random.randn(len(x_t)),\n                                 z_t + 0.01*np.random.randn(len(z_t)),\n                                 np.random.randn()])\n    \n    # Bounds: Typical Fourier-like coefficients should remain bounded\n    bounds = [(-10, 10) for _ in range(3*len(x_t)+1)]\n    \n    result = minimize(\n        objective, \n        init_guess, \n        method='SLSQP',  \n        bounds=bounds, \n        constraints=constraint_list, \n        options={'disp': True}\n    )\n    \n    return {\n        'optimal_value': result.fun,\n        'solution': {\n            'a': result.x[:len(x_t)],\n            'b': result.x[2*len(x_t)],\n            'c': result.x[len(x_t):2*len(x_t)],\n            'γ': result.x[2*len(x_t)+1:] if len(result.x) > 3*len(x_t) else None\n        }\n    }\n\nif __name__ == '__main__':\n    data = generate_synthetic_data(42)\n    result = solve(data)\n    print(f\"Optimal Value: {result['optimal_value']}\")\n    print(\"Solution:\")\n    for key, val in result['solution'].items():\n        print(f\"{key}: {val}\")",
  "lean4_formal": ""
}