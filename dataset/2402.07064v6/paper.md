# Piecewise SOS-Convex Moment Optimization and Applications via Exact Semi-Definite Programs

**arXiv ID:** 2402.07064v6

**Authors:** Queenie Yingkun Huang, Vaithilingam Jeyakumar, Guoyin Li

**Abstract:** This paper presents exact Semi-Definite Program (SDP) reformulations for infinite-dimensional moment optimization problems involving a new class of piecewise Sum-of-Squares (SOS)-convex functions and projected spectrahedral support sets. These reformulations show that solving a single SDP finds the optimal value and an optimal probability measure of the original moment problem. This is done by establishing an SOS representation for the non-negativity of a piecewise SOS-convex function over a projected spectrahedron. Finally, as an application and a proof-of-concept illustration, the paper presents numerical results for the Newsvendor and revenue maximization problems with higher-order moments by solving their equivalent SDP reformulations. These reformulations promise a flexible and efficient approach to solving these models. The main novelty of the present work in relation to the recent research lies in finding the solution to moment problems, for the first time, with piecewise SOS-convex functions from their numerically tractable exact SDP reformulations.

---

> **Note:** This text was extracted with pypdf (plain-text fallback). LaTeX formulas may be garbled. Install `marker-pdf` for better results.

<!-- page 1 -->
Piecewise SOS-Convex Moment Optimization and
Applications via Exact Semi-Definite Programs
Q.Y. Huangâˆ—, V. Jeyakumar â€  and G. Li â€¡
July 3, 2024
Abstract
This paper presents exact Semi-Definite Program (SDP) reformulations for infinite-dimensional
moment optimization problems involving a new class of piecewise Sum-of-Squares (SOS)-
convex functions and projected spectrahedral support sets. These reformulations show that
solving a single SDP finds the optimal value and an optimal probability measure of the
original moment problem. This is done by establishing an SOS representation for the non-
negativity of a piecewise SOS-convex function over a projected spectrahedron. Finally, as an
application and a proof-of-concept illustration, the paper also presents numerical results for
the Newsvendor and revenue maximization problems with higher-order moments by solving
their equivalent SDP reformulations. These reformulations promise a flexible and efficient
approach to solving these models. The main novelty of the present work in relation to the
recent research lies in finding the solution to moment problems, for the first time, with
piecewise SOS-convex functions from their numerically tractable exact SDP reformulations.
Keywords. Moment Optimization; Sum-of-Squares Convex Polynomials; Piecewise Functions;
Generalized moment problems; Semi-Definite Programming
1 Introduction
Consider the generalized moment problem
min
ÂµâˆˆPâ„¦
n
Eâ„¦
Âµ

min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (Ï‰)

: Eâ„¦
Âµ[hj(Ï‰)] â‰¤ Î³j, j = 1, . . . , J
o
(P)
where gk
â„“ , â„“ = 1 , . . . , L, k = 1 , . . . , r, and hj, j = 1 , . . . , J, are Sum-of-Squares (SOS)-convex
polynomials. The SOS-convexity for polynomials is a numerically tractable relaxation of convexity
âˆ—Corresponding author. Department of Applied Mathematics, University of New South Wales, Sydney 2052,
Australia. Email: yingkun.huang@unsw.edu.au. The research of Ms Queenie Yingkun Huang was partially
supported by a grant from the Australian Research Council.
â€ Department of Applied Mathematics, University of New South Wales, Sydney 2052, Australia. Email: v.
jeyakumar@unsw.edu.au. The research of Prof Vaithilingam Jeyakumar was supported by a grant from the
Australian Research Council.
â€¡Department of Applied Mathematics, University of New South Wales, Sydney 2052, Australia. Email: g.li@
unsw.edu.au. Research of Prof Guoyin Li was supported by a grant from the Australian Research Council
1
arXiv:2402.07064v6  [math.OC]  2 Jul 2024

<!-- page 2 -->
since checking whether a polynomial is SOS-convex or not can be achieved by solving an SDP
[1, 15]. Recent applications of SOS-convexity in optimization may be found in [25, 18]. The set
â„¦ âŠ‚ Rm is a convex compact projected spectrahedron and Pâ„¦ is the set of probability measures
supported on â„¦. The expectation of a random variable Ï‰ with respect to the probability measure
Âµ âˆˆ Pâ„¦ is denoted by Eâ„¦
Âµ[Ï‰].
Our model (P) is closely related to generalized moment problems [24] where gk
â„“ , â„“ = 1 , . . . , L,
k = 1, . . . , r, and hj, j = 1, . . . , J, are arbitrary real-valued continuous functions and the support
set â„¦ is an arbitrary compact subset of Rm. The dominant technique to reformulate a general-
ized moment problem as a semi-infinite optimization problem is via the duality theory [32, 36].
However, in general, a semi-infinite optimization problem can be computationally intractable.
The classical moment problem can be recovered from (P) if L = 1, r = 1, the random variable is
univariate, and the constraints are the first J moments [24]. The multivariate classical moment
problem extends to countably many moments, but numerically tractable forms are limited. When
L = 1, r = 1, g1
1 and hj, j = 1, . . . , J, are linear functions and the support â„¦ is a spectrahedron,
(P) admits an exact SDP reformulation [17]. When L = 1, gk
1, k = 1, . . . , r, are linear functions
with mean and variance constraints, the support â„¦ is an ellipsoid, it has been shown that (P)
admits an exact SDP reformulation [8].
When L = 1, r = 1, g1
1 and hj, j = 1 , . . . , J, are any polynomials and the support â„¦ is a
compact semi-algebraic set, the optimal solution for (P) can be found by solving a convergent
hierarchy of SDPs [22]. Further, polynomial optimization techniques were employed in [6] to
obtain exact SOS reformulations when P contains probability measures whose density functions
are SOS polynomials. A review of generalized moment problems and their applications is available
in [7].
The model problem (P) is of great interest in distributionally robust optimization [36], where
(P) appears as an uncertainty quantification problem to mitigate risks and uncertainties from the
input data [36, 13]. For example, in the Newsvendor model of [33], the cost of ordering represented
as a piecewise linear loss function is minimized to meet the uncertain demand. In the portfolio
management problem of [8], the profit of the portfolio represented as a piecewise linear utility
function is maximized amid random investment returns.
Motivated by the importance of such piecewise functions in optimization and the usefulness of
the minimax functions of the form mink=1,...,r maxâ„“=1,...,L gk
â„“ in applications across diverse domains,
we introduce a new class of functions, called the piecewise SOS-convex functions. To the best of
our knowledge, this notion of piecewise SOS-convex function has so far not been studied in the
literature.
A function f on Rn is piecewise SOS-convex if there exist SOS-convex polynomialsgk
â„“ , â„“ = 1, . . . , L,
k = 1, . . . , r, on Rn such that f(v) = min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (v) for all v âˆˆ Rn.
Note that a piecewise SOS-convex function is not necessarily a convex or a differentiable function
and it covers a broad range of functions that appear in applications across several domains, as
outlined below. Details of their piecewise representations are provided in Appendix A.
â€¢ The truncated â„“1-norm, f(v) = min{1, Îµ|v|}, Îµ > 0, which is non-convex and non-smooth, is
used extensively in machine-learning regularization [26].
â€¢ The piecewise linear function f(v) = min
k=1,...,r
pâŠ¤
k v + qk, pk âˆˆ Rn, qk âˆˆ R, k = 1, . . . , r, appears
in Newsvendor-type [3, 10], conditional value-at-risk [30], and portfolio selection [8] models,
2

<!-- page 3 -->
and it is a useful approximation for various important utility functions [8].
â€¢ The class of max-SOS-convex functions f(v) = max
â„“=1,...,L
gâ„“(v), where gâ„“â€™s are SOS-convex
polynomials, is studied in [21]. Further, convex quadratics and convex separable polynomials
are SOS-convex [1, 19].
â€¢ The class of difference-of-convex functions of the form f(v) = max
â„“=1,...,L
fâ„“(v)âˆ’ max
k=1,...,r
(pâŠ¤
k v+qk),
where fâ„“â€™s are SOS-convex polynomials, can be expressed as f(v) = min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (v) for
gk
â„“ (v) = fâ„“(v) âˆ’ (pâŠ¤
k v + qk), â„“ = 1 , . . . , L, k = 1 , . . . , r. These functions have attracted
applications in feature selection models of machine-learning [18].
â€¢ The piecewise convex quadratic function
f(v) =
(
a(v âˆ’ b)2 + c, if 0 â‰¤ v â‰¤ b,
c, if v > b.
with a, b â‰¥ 0, c âˆˆ R, is used to model offer prices from customers [13].
â€¢ The Huber loss function with parameter Îµ > 0,
f(v) =
(
1
2 v2, if |v| â‰¤ Îµ,
Îµ|v| âˆ’ 1
2 Îµ2, otherwise,
is studied in robust statistics [33].
The present study of moment optimization is motivated by two aspects. Firstly, moment prob-
lems are numerically challenging due to the presence of infinite-dimensional distributions and
multi-dimensional integrals. In response, we examine equivalent Semi-Definite Program (SDP) re-
formulations of these problems which can be efficiently solved numerically by commonly available
software via interior point methods.
Secondly, the study of piecewise SOS-convexity in (P) was stimulated by the computationally
attractive features of SOS-convex polynomial optimization and its applications in many areas. It
is known that SOS-convex polynomial optimization problems admit numerically favourable SDP
reformulations [19, 20, 21, 25].
The main goal of this paper is to provide numerically tractable reformulations for the moment
problem involving piecewise SOS-convex functions and show how the solution can be recovered
from its SDP reformulation.
Our contributions. Our main contributions are as follows.
(i) Firstly, we introduce the notion of piecewise SOS-convex functions and establish a new SOS
representation of the non-negativity of a piecewise SOS-convex function over a compact pro-
jected spectrahedron. This result not only generalizes the known representation result of
a non-negative SOS-convex polynomial over a spectrahedron [19] but also provides numeri-
cally tractable representations of non-negativity for a broad class of functions that are not
necessarily convex.
(ii) Secondly, exploiting the representation result, we derive an equivalent numerically tractable
SOS optimization reformulation for the generalized moment problem (P). Moreover, under
3

<!-- page 4 -->
suitable conditions, we show how to recover an optimal probability measure for (P) by
solving a single SDP. This extends the corresponding result of Lasserre [23] on SOS-convex
polynomial optimization.
(iii) Finally, as an application and a proof-of-concept illustration, we present numerical results
for two important practical models, i.e., the Newsvendor problem [12] with higher-order
moments and the revenue maximization problem [13] with a higher-order utility function,
by solving their equivalent SDP reformulations.
The novelty of the present work in relation to recent research in SOS-convexity and moment
optimization is the derivation of numerically tractable exact SDP reformulations for moment
problems involving, for the first time, piecewise SOS-convex functions . The main innovation is
the combined utilization of powerful tools from real algebraic geometry, convex analysis, and SOS
polynomials to produce the exact reformulations. We employ the key computationally favourable
features of SOS-convexity within an infinite-dimensional setting and exploit the geometry of the
projected spectrahedra to facilitate the SDP reformulations.
Our approach makes use of the piecewise structure of the functions for transforming infinite-
dimensional problems into finite-dimensional numerically tractable problems. Moreover, the pro-
jected spectrahedron covers a broad class of semi-algebraic sets [25], such as the spectrahedra,
ellipsoids, and boxes, used in robust optimization [2].
The paper is organized as follows. Section 2 provides SOS representation results for piecewise
SOS-convex functions. Section 3 presents the main theorems for the SOS reformulation of (P)
and the optimal solution recovery of (P). Section 4 describes applications to the Newsvendor
and revenue maximization models. Section 5 concludes with discussions on potential future work.
The appendices provide technical details related to explicit piecewise representations for some
piecewise SOS-convex functions (Appendix A), infinite-dimensional conic duality (Appendix B),
and SDP representations of SOS optimization problems (Appendix C).
2 Non-Negativity of Piecewise SOS-Convex Functions
In this section, we outline SOS representations for a class of piecewise functions involving SOS-
convex polynomials. This will play a vital role in reformulating the generalized moment problem
(P) as a numerically tractable SOS optimization problem.
We begin with some preliminaries on polynomials. Denote Rm the Euclidean space of dimension
m and Rm
+ the non-negative orthant of Rm. The standard inner product is aâŠ¤b for a, b âˆˆ Rm. Let
R[v] be the space of polynomials with real coefficients over v âˆˆ Rm. A polynomial f âˆˆ R[v] is
called a Sum-of-Squares (SOS) polynomial if there exist polynomials fj âˆˆ R[v], j = 1, . . . , J, such
that f =PJ
j=1 f 2
j . For a polynomial f âˆˆ R[v], we use deg f to denote its degree. We also use
Î£2
d(v) to denote the set of all SOS polynomials f of degree at most d with respect to the variable
v âˆˆ Rm. Next, we recall the definition of SOS-convex polynomials.
Definition 2.1 (SOS-convex polynomial [1, 16]). A polynomial f âˆˆ R[v] is SOS-convex if its
Hessian H(v) is an SOS matrix polynomial, that is, if there exists an (s Ã— m) polynomial matrix
P (v) for some s âˆˆ N such that H(v) = P (v)âŠ¤P (v).
Several equivalent conditions for SOS-convexity are available in [1]. For instance, f âˆˆ R[v] is
4

<!-- page 5 -->
SOS-convex whenever the polynomial g(v1, v2) = f(v1) âˆ’ f(v2) âˆ’ âˆ‡f(v2)âŠ¤(v1 âˆ’ v2) on Rm Ã— Rm
is an SOS polynomial with respect to the variable ( v1, v2).
Any SOS-convex polynomials are convex polynomials, but the converse is not true [1]. In other
words, the class of SOS-convex polynomials is a proper subclass of convex polynomials. More-
over, the class of SOS-convex polynomials covers affine functions, convex quadratics, and convex
separable polynomials, while SOS-convex polynomials can be non-quadratic or non-separable in
general [19].
Now, we formally define the notion of a piecewise SOS-convex function.
Definition 2.2 (Piecewise SOS-convex function). We call a function f on Rm piecewise
SOS-convex if there exist SOS-convex polynomials gk
â„“ , â„“ = 1, . . . , L, k = 1, . . . , r, on Rm such that
f(v) = min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (v) for all v âˆˆ Rm.
The following proposition links non-negative SOS-convex polynomials to SOS polynomials which
will be deployed in the tractable representation of piecewise SOS-convex functions (Theorem 2.4).
Proposition 2.3. (see [16] and [21, Corollary 2.1]). Let f âˆˆ R[v] be a non-negative SOS-convex
polynomial. Then, f is an SOS polynomial.
Denote ei âˆˆ Rm the i-th standard basis vector of Rm, i = 1, . . . , m. The simplex in RL is defined as
âˆ† = {Î´ âˆˆ RL
+ :PL
â„“=1 Î´â„“ = 1}. Let SÎ½ be the set of ( Î½ Ã— Î½) symmetric matrices. The trace product
on SÎ½ is tr(M N) for any M, N âˆˆ SÎ½. A matrix M âˆˆ SÎ½ is positive semi-definite, denoted as M âª° 0
(resp. positive definite, denoted as M â‰» 0), if xâŠ¤M x â‰¥ 0 for all x âˆˆ RÎ½ (resp. xâŠ¤M x > 0 for all
x âˆˆ RÎ½, x Ì¸= 0). Let SÎ½
+ be the cone of symmetric ( Î½ Ã— Î½) positive semi-definite matrices.
A projected spectrahedron [15], or a spectrahedral shadow [29], takes the form
â„¦ =
(
v âˆˆ Rm : âˆƒÎ¾ âˆˆ RN , F0 +
mX
i=1
viFi +
NX
t=1
Î¾tMt âª° 0
)
, (LMI)
for some Fi âˆˆ SÎ½, i = 0 , 1, . . . , m, and Mt âˆˆ SÎ½, t = 1 , . . . , N. This set will later be used as a
support set for probability measures. This set is versatile as any semi-algebraic set {v âˆˆ Rm :
fi(v) â‰¤ 0, i = 1, . . . , n} described by SOS-convex polynomials fi, i = 1, . . . , n, can be represented
as a projected spectrahedron [15]. Moreover, spectrahedra, ellipsoids, and boxes are special forms
of projected spectrahedra.
The following representation of a piecewise SOS-convex function extends the result in [19] for an
SOS-convex polynomial over a spectrahedron and in [21] for a max-SOS-convex polynomial over
a subclass of projected spectrahedron.
Theorem 2.4 (SOS representation for non-negative piecewise SOS-convex functions ).
Let â„¦ âŠ‚ Rm be given as in (LMI), gk
â„“ , â„“ = 1, . . . , L, k= 1, . . . , r, be SOS-convex polynomials on Rm.
Assume that â„¦ is compact and there exist Â¯v âˆˆ Rm and Â¯Î¾ âˆˆ RN such that F0+
mX
i=1
Â¯viFi+
NX
t=1
Â¯Î¾tMt â‰» 0.
Then, the following statements are equivalent.
(i) v âˆˆ â„¦ = â‡’ min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (v) â‰¥ 0.
5

<!-- page 6 -->
(ii) For each k = 1, . . . , r, there exist Zk âˆˆ SÎ½
+ and Î´k = (Î´k
1 , . . . , Î´k
L) âˆˆ âˆ† such that
ï£±
ï£´ï£²
ï£´ï£³
LX
â„“=1
Î´k
â„“ gk
â„“ (v) âˆ’ tr(ZkF0) âˆ’
mX
i=1
vitr(ZkFi) âˆˆ Î£2
d(v), k = 1, . . . , r,
tr(ZkMt) = 0, t = 1, . . . , N, k = 1, . . . , r.
where d is the smallest even integer larger than max
k=1,...,r
max
â„“=1,...,L
deg gk
â„“ .
Proof. [(ii) =â‡’ (i)]. Assume that there exist Zk âˆˆ SÎ½
+ and Î´k âˆˆ âˆ†, k = 1, . . . , r, such that
ï£±
ï£´ï£²
ï£´ï£³
LX
â„“=1
Î´k
â„“ gk
â„“ (v) âˆ’ tr(ZkF0) âˆ’
mX
i=1
vitr(ZkFi) âˆˆ Î£2
d(v), k = 1, . . . , r,
tr(ZkMt) = 0, t = 1, . . . , N, k = 1, . . . , r.
Fix k âˆˆ {1, . . . , r}. Since an SOS polynomial always takes non-negative values, it follows that
LX
â„“=1
Î´k
â„“ gk
â„“ (v) âˆ’ tr(ZkF0) âˆ’
mX
i=1
vitr(ZkFi) â‰¥ 0, for all v âˆˆ Rm.
Further, tr(ZkF0) +Pm
i=1 vitr(ZkFi) = tr(Zk(F0 +Pm
i=1 viFi +PN
t=1 Î¾tMt)) for any Î¾ âˆˆ RN. This
implies that, for all ( v, Î¾) âˆˆ Rm Ã— RN,
LX
â„“=1
Î´k
â„“ gk
â„“ (v) â‰¥ tr

Zk

F0 +
mX
i=1
viFi +
NX
t=1
Î¾tMt

. (1)
Since ( Î´k
1 , . . . , Î´k
L) âˆˆ âˆ†, we have max
â„“=1,...,L
gk
â„“ (v) â‰¥
LX
â„“=1
Î´k
â„“ gk
â„“ (v). Further, for an arbitrary v âˆˆ â„¦,
there exists Î¾ âˆˆ RN such that F0 +
mX
i=1
viFi +
NX
t=1
Î¾tMt âˆˆ SÎ½
+. Also, since Zk âˆˆ SÎ½
+, we have
tr

Zk

F0 +
mX
i=1
viFi +
NX
t=1
Î¾tMt

â‰¥ 0. It follows from Eqn. (1) that max
â„“=1,...,L
gk
â„“ (v) â‰¥ 0 for any
v âˆˆ â„¦. This holds for any k âˆˆ {1, . . . , r}, so min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (v) â‰¥ 0 for any v âˆˆ â„¦.
[(i) =â‡’ (ii)]. Assume Statement (i) is true. Fix k âˆˆ {1, . . . , r}. We have
min
vâˆˆâ„¦
max
â„“=1,...,L
gk
â„“ (v) = min
vâˆˆâ„¦
max
Î´âˆˆâˆ†
LX
â„“=1
Î´â„“gk
â„“ (v) â‰¥ 0. (2)
Define a bifunction Pk : â„¦ Ã— âˆ† â†’ R by Pk(v, Î´) = PL
â„“=1 Î´â„“gk
â„“ (v). Direct verification shows that
Pk(v, Â·) is linear for each fixed v âˆˆ â„¦ and Pk(Â·, Î´) is convex continuous for each fixed Î´ âˆˆ âˆ†. Note
that both â„¦ âŠ† Rm and âˆ† âŠ† RL are convex compact sets. The Convex-Concave Minimax Theorem
6

<!-- page 7 -->
[cf. 35, Theorem 2.10.2] gives us that
min
vâˆˆâ„¦
max
Î´âˆˆâˆ†
Pk(v, Î´) = max
Î´âˆˆâˆ†
min
vâˆˆâ„¦
Pk(v, Î´).
Thus, there exists Î´k âˆˆ âˆ† such that max
Î´âˆˆâˆ†
min
vâˆˆâ„¦
Pk(v, Î´) = min
vâˆˆâ„¦
Pk(v, Î´k) = min
vâˆˆâ„¦
LX
â„“=1
Î´k
â„“ gk
â„“ (v).
Observe that
min
vâˆˆâ„¦
LX
â„“=1
Î´k
â„“ gk
â„“ (v) = min
vâˆˆRm
( LX
â„“=1
Î´k
â„“ gk
â„“ (v) : F0 +
mX
i=1
viFi +
NX
t=1
Î¾tMt âª° 0 for some Î¾ âˆˆ RN
)
= min
(v,Î¾)âˆˆRmÃ—RN
( LX
â„“=1
Î´k
â„“ gk
â„“ (v) : F0 +
mX
i=1
viFi +
NX
t=1
Î¾tMt âª° 0
)
. (3)
By the Lagrangian Duality under the strict feasibility assumption on â„¦ [35, Theorem 2.9.2], there
exists Zk âˆˆ SÎ½
+ such that
min (3) = min
(v,Î¾)âˆˆRmÃ—RN
( LX
â„“=1
Î´k
â„“ gk
â„“ (v) âˆ’ tr(ZkF0) âˆ’
mX
i=1
vitr(ZkFi) âˆ’
NX
t=1
Î¾ttr(ZkMt)
)
.
Hence, Eqn. (2) implies that there exist Zk âˆˆ SÎ½
+ and Î´k âˆˆ âˆ† such that
LX
â„“=1
Î´k
â„“ gk
â„“ (v) âˆ’ tr(ZkF0) âˆ’
mX
i=1
vitr(ZkFi) âˆ’
NX
t=1
Î¾ttr(ZkMt) â‰¥ 0, (4)
for all ( v, Î¾) âˆˆ Rm Ã— RN. Letting Î¾ = 0 âˆˆ RN in Eqn. (4) gives
Ïƒk(v) :=
LX
â„“=1
Î´k
â„“ gk
â„“ (v) âˆ’ tr(ZkF0) âˆ’
mX
i=1
vitr(ZkFi) â‰¥ 0, for all v âˆˆ Rm.
Since Ïƒk is a sum of finitely many SOS-convex polynomials, Ïƒk is a non-negative SOS-convex
polynomial. By Proposition 2.3, Ïƒk is thus an SOS polynomial. Moreover, by fixing v = bv for
somebv âˆˆ Rm, Eqn. (4) further implies that Ïƒk(bv) âˆ’PN
t=1 Î¾ttr(ZkMt) â‰¥ 0 for any Î¾ âˆˆ RN where
Ïƒk(bv) â‰¥ 0 is fixed. This forces tr( ZkMt) = 0 for all t = 1 , . . . , N. The conclusion now follows
because k is chosen arbitrarily from {1, . . . , r}.
3 Piecewise SOS-Convex Moment Problems
In this section, we focus on reformulating a generalized moment optimization problem with piece-
wise SOS-convex functions as an SDP.
We begin by fixing some terminologies. Let Y and Y â€² be real topological vector spaces. They are
paired if a bilinear form âŸ¨Â·, Â·âŸ© : Y â€² Ã— Y â†’ R is defined. The cone generated by any set S âŠ† Y and
the interior of S are denoted as cone( S) and int( S) respectively. The (positive) polar of a cone
7

<!-- page 8 -->
S âŠ† Y is defined as S+ = {yâ€² âˆˆ Y â€² : âŸ¨yâ€², yâŸ© â‰¥ 0, âˆ€y âˆˆ S}. See Appendix B for details on strong
conic duality.
For a non-empty compact set â„¦ âŠ‚ Rm, Câ„¦ denotes the vector space of all continuous real-valued
functions on â„¦ equipped with the supremum norm. Denote B the Borel Ïƒ-algebra of â„¦. From this
section onwards, we let X be the vector space of (finite signed regular) Borel measures on (â„¦, B).
It is known that X can be identified as the topological dual space of Câ„¦, i.e., X = Câˆ—
â„¦. We equip
X with the weak âˆ— topology. For any point v âˆˆ â„¦, the Dirac measure 1 v which takes a mass of 1
at the point v âˆˆ â„¦ and 0 otherwise belongs to X [32].
Consider the following generalized moment problem with piecewise SOS-convex functions:
min
ÂµâˆˆX
Eâ„¦
Âµ
h
min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (Ï‰)
i
(P)
s.t. Eâ„¦
Âµ [hj(Ï‰)] â‰¤ Î³j, j = 1, . . . , J,
Eâ„¦
Âµ[1] = 1, Âµ âª°B 0,
where Î³j âˆˆ R, j = 1 , . . . , J, and hj, j = 1 , . . . , J, and gk
â„“ , â„“ = 1 , . . . , L, k = 1 , . . . , r, are
SOS-convex polynomials on Rm. Define g : Rm â†’ R, g(v) = min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (v) the piecewise
SOS-convex function, and note that g âˆˆ C â„¦ and hj âˆˆ C â„¦, j = 1, . . . , J. We let X â€² be the vector
space generated by g, hj, j = 1, . . . , J, and the constant function of 1. Note that X â€² âŠ† C â„¦, and
X â€² is equipped with the supremum norm. Following [32], X and X â€² are paired spaces, and the
continuous bilinear form between them is given by
âŸ¨f, ÂµâŸ© = Eâ„¦
Âµ[f(Ï‰)] :=
Z
â„¦
f(Ï‰) dÂµ(Ï‰), for all f âˆˆ X â€² and Âµ âˆˆ X,
where Eâ„¦
Âµ[f(Ï‰)] refers to the expectation of the random variable f(Ï‰) with respect to the measure
Âµ supported on â„¦. Denote the set of probability measures by Pâ„¦ = {Âµ âˆˆ X : Eâ„¦
Âµ[1] = 1, Âµ âª°B 0},
where Âµ âª°B 0 means that Âµ(A) = Âµ(Ï‰ âˆˆ A) â‰¥ 0 for all B-measurable sets A.
The moment problem (P) is known as an uncertainty quantification problem in the distributionally
robust optimization literature [14], and the feasible set {Âµ âˆˆ Pâ„¦ : Eâ„¦
Âµ[hj(Ï‰)] â‰¤ Î³j, j = 1, . . . , J} is
called a moment ambiguity set.
We assume that the support set â„¦ is a (convex) projected spectrahedron:
â„¦ =
n
v âˆˆ Rm : âˆƒÎ¾ âˆˆ RN , F 0 +
mX
i=1
viFi +
NX
t=1
Î¾tMt âª° 0
o
, (LMI)
for some Fi âˆˆ SÎ½, i = 0, 1, . . . , m, and Mt âˆˆ SÎ½, t = 1, . . . , N. We associate (P) with the following
8

<!-- page 9 -->
SOS optimization problem
max
Î»âˆˆRJ
+Ã—R
ZkâˆˆSÎ½
+,Î´kâˆˆâˆ†
k=1,...,r
âˆ’
JX
j=1
Î»jÎ³j âˆ’ Î»J+1 (D)
s.t.
LX
â„“=1
Î´k
â„“ gk
â„“ (v) +
JX
j=1
Î»jhj(v) + Î»J+1 âˆ’ tr(ZkF0) âˆ’
mX
i=1
vitr(ZkFi) âˆˆ Î£2
d(v), k = 1, . . . , r,
tr(ZkMt) = 0, t = 1, . . . , N, k = 1, . . . , r,
where Î´k = (Î´k
1 , . . . , Î´k
L) âˆˆ âˆ† = {Î´ âˆˆ RL
+ :PL
â„“=1 Î´â„“ = 1}, Î£ 2
d(v) is the set of SOS polynomials of
degree at most d with respect to the variable v âˆˆ Rm, and d is the smallest even integer with
d â‰¥ max

max
k=1,...,r
max
â„“=1,...,L
deg gk
â„“ , max
j=1,...,J
deg hj

.
The SOS program (D) can be equivalently rewritten as a Semi-Definite Program (SDP), and thus
can be efficiently solved via existing SDP software. We refer the readers to Appendix C for the
procedure.
The following useful characterization states that the polar cone of the cone of non-negative mea-
sures is the convex cone of functions in X â€² that are non-negative on â„¦. This characterization is
known (e.g., [4, Examples 2.37, 2.122], [17, Lemma 3.1]), but we provide the proof here for the
sake of self-containment.
Note that, for the convex cone of non-negative measures C = {Âµ âˆˆ X : Âµ âª°B 0}, we denote
C+ = {f âˆˆ X â€² : âŸ¨f, ÂµâŸ© â‰¥ 0, âˆ€Âµ âˆˆ C}. Here, C+ is the intersection of the topological polar cones
of C with the subspace X â€² of the vector space of continuous real-valued functions Câ„¦.
Lemma 3.1 (Characterization of cone of non-negative measures). Let â„¦ Ì¸= âˆ… be any
convex compact subset of Rm, C = {Âµ âˆˆ X : Âµ âª°B 0}, and f âˆˆ X â€². Then, f âˆˆ C+ if and only if
minvâˆˆâ„¦ f(v) â‰¥ 0.
Proof. Suppose that f âˆˆ C+. Then, âŸ¨f, ÂµâŸ© â‰¥ 0 for all Âµ âˆˆ C. Note that the Dirac measure 1 v at
any point v âˆˆ â„¦ belongs to C. This gives f(v) = âŸ¨f, 1 vâŸ© â‰¥ 0 for any v âˆˆ â„¦. Hence, min
vâˆˆâ„¦
f(v) â‰¥ 0.
Conversely, suppose that min
vâˆˆâ„¦
f(v) â‰¥ 0. Now, for any Âµ âˆˆ Pâ„¦ = {Âµ âˆˆ X :
R
â„¦ 1dÂµ(Ï‰) = 1, Âµ âª°B 0},
we have
Z
â„¦
f(Ï‰) dÂµ(Ï‰) â‰¥
Z
â„¦

min
vâˆˆâ„¦
f(v)

dÂµ(Ï‰) =

min
vâˆˆâ„¦
f(v)
Z
â„¦
1dÂµ(Ï‰) = min
vâˆˆâ„¦
f(v).
This implies âŸ¨f, ÂµâŸ© â‰¥ 0 for any Âµ âˆˆ Pâ„¦. Since C = cone(Pâ„¦), we see f âˆˆ C+.
Now, we show that (D) is an exact SOS reformulation for (P) in the sense that min (P) = max (D).
Assumption 3.2 (Interior point condition for moment problem).
(Î³1, . . . , Î³J , 1) âˆˆ int

{(âŸ¨h1, ÂµâŸ©, . . . ,âŸ¨hJ , ÂµâŸ©, âŸ¨1, ÂµâŸ©) : Âµ âˆˆ X, Âµ âª°B 0} + RJ
+ Ã— {0}

.
9

<!-- page 10 -->
This interior point condition is commonly used in the moment problem literature and can be
satisfied in many situations. See [32, 34] for discussions.
Assumption 3.3. The projected spectrahedronâ„¦ as in (LMI) is compact and there exist Â¯v âˆˆ Rm
and Â¯Î¾ âˆˆ RN such that F0 +Pm
i=1 Â¯viFi +PN
t=1
Â¯Î¾tMt â‰» 0. For the problem (P), the polynomials gk
â„“ ,
â„“ = 1, . . . , L, k = 1, . . . , r, and hj, j = 1, . . . , J, are SOS-convex polynomials on Rm.
Theorem 3.4 (Exact SOS program for piecewise SOS-convex moment optimization ).
Assume that (P) admits a minimizer and Assumptions 3.2-3.3 are satisfied. Then, min (P) =
max (D).
Proof. [Conic duality] . Let g(v) = min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (v), b = ( Î³1, . . . , Î³J , 1), K = RJ
+ Ã— {0},
and C = {Âµ âˆˆ X : Âµ âª°B 0}. Note that g âˆˆ X â€², and C and K are convex cones that are
closed in the respective topologies. Define a continuous linear map A : X â†’ RJ+1 by AÂµ =
(âŸ¨h1, ÂµâŸ©, . . . ,âŸ¨hJ , ÂµâŸ©, âŸ¨1, ÂµâŸ©). Then,
min (P) = min
ÂµâˆˆC
{âŸ¨g, ÂµâŸ© : âˆ’AÂµ + b âˆˆ K}.
The adjoint mapping Aâˆ— : RJ+1 7â†’ X â€² is given by Aâˆ—Î» =PJ
j=1 Î»jhj + Î»J+1 Â· 1, and the (positive)
polar cone of K is K+ = RJ
+ Ã— R. Recall that C+ = {f âˆˆ X â€² : âŸ¨f, ÂµâŸ© â‰¥ 0, âˆ€Âµ âˆˆ C}. Then,
Assumption 3.2 can be equivalently rewritten as b âˆˆ int(A(C) + K). It follows from strong conic
linear duality (Corollary B.2) that
min (P) = max
(
âˆ’
JX
j=1
Î»jÎ³j âˆ’ Î»J+1 : g +
JX
j=1
Î»jhj + Î»J+1 Â· 1 âˆˆ C+, Î» âˆˆ K+
)
, (5)
and the maximum of the problem in Eqn. (5) is attained.
[Equivalent SOS representation of conic constraint]. This means, there exists Î» âˆˆ RJ
+ Ã— R
with g +
JX
j=1
Î»jhj + Î»J+1 Â· 1 âˆˆ C+ such that min (P) = âˆ’
JX
j=1
Î»jÎ³j âˆ’ Î»J+1. By Lemma 3.1,
g +PJ
j=1 Î»jhj + Î»J+1 Â· 1 âˆˆ C+ if and only if
min
vâˆˆâ„¦
(
min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (v) +
JX
j=1
Î»jhj(v) + Î»J+1
)
â‰¥ 0. (6)
The polynomial gk
â„“ +PJ
j=1 Î»jhj + Î»J+1 is SOS-convex for each â„“ = 1 , . . . , L, k = 1 , . . . , r. By
Theorem 2.4, Eqn. (6) is equivalent to, for each k = 1, . . . , r,
ï£±
ï£´ï£²
ï£´ï£³
LX
â„“=1
Î´k
â„“ gk
â„“ (v) +
JX
j=1
Î»jhj(v) + Î»J+1 âˆ’ tr(ZkF0) âˆ’
mX
i=1
vitr(ZkFi) âˆˆ Î£2
d(v),
tr(ZkMt) = 0, t = 1, . . . , N,
for some Zk âˆˆ SÎ½
+ and Î´k âˆˆ âˆ†. Therefore, min (P) â‰¤ max (D).
[Weak duality]. Take any feasible point Î» âˆˆ RJ
+ Ã— R, Zk âˆˆ SÎ½
+, Î´k âˆˆ âˆ†, k = 1, . . . , r, for (D).
10

<!-- page 11 -->
Theorem 2.4 gives min
k=1,...,r
max
â„“=1,...,L
n
gk
â„“ (v) +
JX
j=1
Î»jhj(v) + Î»J+1
o
â‰¥ 0 for all v âˆˆ â„¦, and thus,
min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (v) â‰¥ âˆ’
JX
j=1
Î»jhj(v) âˆ’ Î»J+1, for all v âˆˆ â„¦.
Take any feasible point Âµ for (P), then, Eâ„¦
Âµ
h
min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (Ï‰)
i
â‰¥ Eâ„¦
Âµ
h
âˆ’
JX
j=1
Î»jhj(Ï‰) âˆ’ Î»J+1
i
.
Further, âˆ’
JX
j=1
Î»jÎ³j âˆ’ Î»J+1 â‰¤ Eâ„¦
Âµ
h
âˆ’
JX
j=1
Î»jhj(Ï‰) âˆ’ Î»J+1
i
. Thus, Eâ„¦
Âµ
h
min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (Ï‰)
i
â‰¥
âˆ’
JX
j=1
Î»jÎ³j âˆ’ Î»J+1, implying min (P) â‰¥ max (D). Together, min (P) = max (D).
The SOS optimization problem (D) can be expressed equivalently as the following SDP (Proposi-
tion C.1),
max
Î»âˆˆRJ
+Ã—R,Î´kâˆˆâˆ†
ZkâˆˆSÎ½
+,QkâˆˆSs(m,d/2)
+
k=1,...,r
âˆ’
JX
j=1
Î»jÎ³j âˆ’ Î»J+1 (R)
s.t.
LX
â„“=1
Î´k
â„“ (gk
â„“ )Î± +
JX
j=1
Î»j(hj)Î± + Î»J+1(1)Î± âˆ’ tr(ZkFÎ±) = tr(QkBÎ±),
for all Î± âˆˆ N , k = 1, . . . , r,
tr(ZkMt) = 0, t = 1, . . . , N, k = 1, . . . , r,
where s(m, d) =
 m+d
d

, N = {Î± = (Î±1, . . . , Î±m) âˆˆ ({0} âˆª N)m :Pm
i=1 Î±i â‰¤ d}, and BÎ± âˆˆ Ss(m,d/2),
Î± âˆˆ N , are matrices given in Appendix C. The notation (f)Î± of any polynomial f âˆˆ R[v] of degree
at most d refers to the Î±-th coefficient of f. In particular, (1) Î± refers to the Î±-th coefficient of
the constant function 1, that is, (1) Î± = 1 when Î± = 0, and (1) Î± = 0 otherwise. The matrices are
F0 := F0 âˆˆ SÎ½, Fei := Fi âˆˆ SÎ½, i = 1, . . . , m, and FÎ±, Î± âˆˆ N \ { 0, e1, . . . , em}, is the zero matrix.
11

<!-- page 12 -->
The dual problem of (R), which is also an SDP, is given by
min
ykâˆˆRs(m,d),Î¾kâˆˆRN
zkâˆˆR,k=1,...,r
rX
k=1
zk (S)
s.t.
rX
k=1
X
Î±âˆˆN
yk
Î±(hj)Î± â‰¤ Î³j, j = 1, . . . , J, (7)
X
Î±âˆˆN
yk
Î±(gk
â„“ )Î± â‰¤ zk, â„“ = 1, . . . , L, k = 1, . . . , r, (8)
yk
0 F0 +
mX
i=1
yk
eiFi +
NX
t=1
Î¾k
t Mt âª° 0, k = 1, . . . , r, (9)
rX
k=1
yk
0 = 1,
X
Î±âˆˆN
yk
Î±BÎ± âª° 0, k = 1, . . . , r, (10)
Denote the minimum value of the problem in (S) as min (S). Sufficient conditions for the strong
duality between the conic programs (R) and (S) are standard [cf. 4, 35].
We will make use of (S) to recover an optimal probability measure for (P) under suitable sign
assumptions. The relationship between these problems is depicted in Figure 1.
Moment problem (P)
Exact SOS reformulation (D) Equivalent SDP representation (R)
Conic program (S)
Theorem 3.4
Proposition C.1
Conic duality
Theorem 3.6
Figure 1: Recovering an optimal probability measure for (P) from (S).
We make use of the following generalized Jensenâ€™s inequality [23, Theorem 2.6].
Proposition 3.5. Let f âˆˆ R[v] be an SOS-convex polynomial of an even degree d, y = (yÎ±)Î±âˆˆN
satisfy y0 = 1 and
X
Î±âˆˆN
yÎ±BÎ± âª° 0, and Ly : R[v] â†’ R be the linear functional Ly(f) =
X
Î±âˆˆN
(f)Î±yÎ±.
Then, Ly(f) â‰¥ f(Ly(v)), where Ly(v) = (Ly(v1), . . . , Ly(vm)), and vi denotes the polynomial that
maps a vector v âˆˆ Rm to its i-th coordinate vi, i = 1, . . . , m.
Theorem 3.6 (Recovering an optimal solution ). Suppose that (P) admits a minimizer, As-
sumptions 3.2-3.3 are satisfied, and strong duality between (R) and (S) holds, i.e., max (R) =
min (S). Let (Â¯yk, Â¯Î¾k, Â¯zk) âˆˆ Rs(m,d) Ã— RN Ã— R, Â¯yk = (Â¯yk
Î±)Î±âˆˆN , be a minimizer for (S) with Â¯yk
0 â‰¥ 0
for all k = 1, . . . , r. Denote K := {k âˆˆ {1, . . . , r} : yk
0 > 0} and buk := 1
Â¯yk
0
(Â¯yk
e1, . . . ,Â¯yk
em) âˆˆ Rm for
k âˆˆ K. Suppose that, for all k /âˆˆ K,
zk â‰¥ 0 and
X
Î±âˆˆN
yk
Î±(hj)Î± â‰¥ 0 for all j = 1, . . . , J.
12

<!-- page 13 -->
Then, bÂµ :=
X
kâˆˆK
Â¯yk
0 1 buk, the linear combination of Dirac measures at points buk âˆˆ Rm, k âˆˆ K, is a
minimizer for (P).
Proof. The equalities min (P) = max (D), max (D) = max (R), and max (R) = min (S) follow
from Theorem 3.4, Proposition C.1, and the strong duality assumption, respectively. Hence,
min (P) = min (S). Let (Â¯yk, Â¯Î¾k, Â¯zk) âˆˆ Rs(m,d) Ã— RN Ã— R, k = 1, . . . , r, be a solution for problem (S)
with Â¯yk
0 â‰¥ 0 for all k = 1, . . . , r, so it satisfies Eqns. (7)-(10).
For k âˆˆ K, definebyk := 1
Â¯yk
0
Â¯yk âˆˆ Rs(m,d), which satisfies byk
0 = 1 and by Eqn. (10),
X
Î±âˆˆN
byk
Î±BÎ± = 1
Â¯yk
0
X
Î±âˆˆN
Â¯yk
Î±BÎ± âª° 0.
Letbuk = (buk
1, . . . ,buk
m) = (byk
e1, . . . ,byk
em), k âˆˆ K. Clearly, buk âˆˆ â„¦ for all k âˆˆ K because we can take
bÎ¾k := 1
Â¯yk
0
Â¯Î¾k âˆˆ RN such that, by Eqn. (9),
F0 +
mX
i=1
buk
i Fi +
NX
t=1
bÎ¾k
t Mt = 1
Â¯yk
0

Â¯yk
0 F0 +
mX
i=1
Â¯yk
eiFi +
NX
t=1
Â¯Î¾k
t Mt

âª° 0, k âˆˆ K.
DefinebÂµ :=P
kâˆˆK Â¯yk
0 1 buk. We note that for any f âˆˆ X â€²,
Eâ„¦
bÂµ[f(Ï‰)] = âŸ¨f,bÂµâŸ© = âŸ¨f,
X
kâˆˆK
Â¯yk
0 1 bukâŸ© =
X
kâˆˆK
Â¯yk
0 âŸ¨f, 1 bukâŸ© =
X
kâˆˆK
Â¯yk
0 f(buk).
By Eqn. (10), bÂµ is a probability measure supported on â„¦ since Eâ„¦
bÂµ[1] =P
kâˆˆK Â¯yk
0 =Pr
k=1 Â¯yk
0 = 1.
Now,buk = Lbyk(v), giving hj(buk) = hj(Lbyk(v)) for j = 1, . . . , J, k âˆˆ K. Using Proposition 3.5 and
the fact that hjâ€™s are SOS-convex polynomials, we have
hj(buk) â‰¤ Lbyk(hj) =
X
Î±âˆˆN
byk
Î±(hj)Î± = 1
Â¯yk
0
X
Î±âˆˆN
Â¯yk
Î±(hj)Î±, j = 1, . . . , J, k âˆˆ K.
By the assumption that
X
k /âˆˆK
X
Î±âˆˆN
Â¯yk
Î±(hj)Î± â‰¥ 0 and Eqn. (7), it follows that
Eâ„¦
bÂµ[hj(Ï‰)] =
X
kâˆˆK
Â¯yk
0 hj(buk) â‰¤
X
kâˆˆK
X
Î±âˆˆN
Â¯yk
Î±(hj)Î± â‰¤
rX
k=1
X
Î±âˆˆN
Â¯yk
Î±(hj)Î± â‰¤ Î³j, j = 1, . . . , J,
sobÂµ is feasible for (P).
Similarly, one can derive that Â¯ yk
0 gk
â„“ (buk) â‰¤
X
Î±âˆˆN
Â¯yk
Î±(gk
â„“ )Î±, â„“ = 1 , . . . , L, k âˆˆ K. From Eqn. (8),
for each k âˆˆ K,
X
Î±âˆˆN
Â¯yk
Î±(gk
â„“ )Î± â‰¤ Â¯zk. This shows that Â¯ yk
0 gk
â„“ (buk) â‰¤ Â¯zk for all â„“ = 1 , . . . , L, and so,
max
â„“=1,...,L
Â¯yk
0 gk
â„“ (buk) â‰¤ Â¯zk. Let g(v) = min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (v). Recall that g âˆˆ X â€² and, for each k = 1, . . . , r,
13

<!-- page 14 -->
g(v) â‰¤ max
â„“=1,...,L
gk
â„“ (v) for any v âˆˆ â„¦. Then, since Â¯yk
0 > 0 for all k âˆˆ K, one has
Eâ„¦
bÂµ[g(Ï‰)] =
X
kâˆˆK
Â¯yk
0 g(buk) â‰¤
X
kâˆˆK
Â¯yk
0

max
â„“=1,...,L
gk
â„“ (buk)

=
X
kâˆˆK

max
â„“=1,...,L
Â¯yk
0 gk
â„“ (buk)

â‰¤
X
kâˆˆK
Â¯zk.
Using the assumption that
X
k /âˆˆK
Â¯zk â‰¥ 0, one has Eâ„¦
bÂµ[g(Ï‰)] â‰¤
X
kâˆˆK
Â¯zk â‰¤
rX
k=1
Â¯zk = min (S), giving
min (P) â‰¤ min (S).
Conversely, min (P) = min (S) ensures that min (P) = Eâ„¦
bÂµ
h
min
k=1,...,r
max
â„“=1,...,L
gk
â„“ (Ï‰)
i
. Therefore, bÂµ is
an optimal solution for the original moment problem (P).
The optimal probability measure bÂµ :=P
kâˆˆK Â¯yk
0 1 buk refers to the discrete distribution with prob-
abilities P(Ï‰ = buk) = Â¯yk
0, k âˆˆ K, i.e., the distribution with masses of Â¯ yk
0 at points buk âˆˆ â„¦,
k âˆˆ K.
We present a recovery result under a stronger condition that is easy to verify.
Corollary 3.7 (Recovery under a simple stronger condition ). Suppose that (P) admits a
minimizer, Assumptions 3.2-3.3 are satisfied, and max (R) = min (S). Let (Â¯yk, Â¯Î¾k, Â¯zk) âˆˆ Rs(m,d) Ã—
RN Ã— R, Â¯yk = (Â¯yk
Î±)Î±âˆˆN , be a minimizer for (S) with Â¯yk
0 > 0 for all k = 1 , . . . , r. Denote buk :=
1
Â¯yk
0
(Â¯yk
e1, . . . ,Â¯yk
em) âˆˆ Rm, k = 1, . . . , r. Then, bÂµ :=Pr
k=1 Â¯yk
0 1 buk is a minimizer for (P).
Proof. This follows from Theorem 3.6 by setting K = {1, . . . , r}.
As an application to polynomial optimization, our recovery result reduces to the known Lasserreâ€™s
result in [23]. More explicitly, consider
min
vâˆˆâ„¦
g(v), (11)
where â„¦ âŠ‚ Rm is given as in (LMI) and g is an SOS-convex polynomial. Notice that, in [23,
Theorem 3.3], the feasible set â„¦ is a compact semi-algebraic set formed by SOS-convex polynomial
inequalities, and thus can be represented as a projected spectrahedron [15].
We associate (11) with the following conic programs
max
Î»âˆˆR,ZâˆˆSÎ½
+
n
âˆ’ Î» : g(v) + Î» âˆ’ tr(ZF0) âˆ’
mX
i=1
vitr(ZF i) âˆˆ Î£2
d(v), tr(ZM t) = 0, t = 1, . . . , N
o
, (12)
and its dual problem
min
yâˆˆRs(m,d),Î¾âˆˆRN
nX
Î±âˆˆN
yÎ±(g)Î± : F0 +
mX
i=1
yeiFi +
NX
t=1
Î¾tMt âª° 0, y 0 = 1,
X
Î±âˆˆN
yÎ±BÎ± âª° 0
o
. (13)
The following Corollary shows how an optimal solution of (11) can be obtained by solving the
SDP program (13).
14

<!-- page 15 -->
Corollary 3.8. For the problem (11), suppose that â„¦ is compact, there exist Â¯v âˆˆ Rm and Â¯Î¾ âˆˆ RN
such that F0 +Pm
i=1 Â¯viFi +PN
t=1
Â¯Î¾tMt â‰» 0, and max (12) = min (13). Let (Â¯y, Â¯Î¾) âˆˆ Rs(m,d) Ã— RN be
a minimizer for (13). Then, (Â¯ye1, . . . ,Â¯yem) âˆˆ Rm is a minimizer for (11).
Proof. Consider the moment problems
min
ÂµâˆˆPâ„¦
Eâ„¦
Âµ[g(Ï‰)] (14)
and
min
ÂµâˆˆD
Eâ„¦
Âµ[g(Ï‰)], (15)
where D = {1 v âˆˆ X : v âˆˆ â„¦} âŠ‚ P â„¦ is the set of all Dirac measures supported on â„¦, giving
min (14) â‰¤ min (15). Note that min (15) = min (11) because Eâ„¦
1 v[g(Ï‰)] = g(v) for all v âˆˆ â„¦.
The interior point condition (Assumption 3.2) for the moment problem (14) requires that 1 âˆˆ
int{Eâ„¦
Âµ[1] : Âµ âˆˆ C}, where C = {Âµ âˆˆ X : Âµ âª°B 0}. Since C = cone(Pâ„¦) and Eâ„¦
P [1] = 1 for any
P âˆˆ P â„¦, a direct verification shows that {Eâ„¦
Âµ[1] : Âµ âˆˆ C} = R+. So, Assumption 3.2 is satisfied
automatically.
By Corollary 3.7 with Â¯ y0 = 1, we have that bÂµ := 1 (Â¯ye1 ,...,Â¯yem) is a minimizer for (14) and
(Â¯ye1, . . . ,Â¯yem) âˆˆ â„¦. As Eâ„¦
bÂµ[g(Ï‰)] = min (14) â‰¤ min (15) and bÂµ âˆˆ D, so bÂµ is also a minimizer
for (15). Noting that Eâ„¦
bÂµ[g(Ï‰)] = g(Â¯ye1, . . . ,Â¯yem), one conclude that (Â¯ye1, . . . ,Â¯yem) is a minimizer
for the polynomial optimization problem (11).
4 Applications to Newsvendor and Revenue Maximization
We devote this section to presenting SDP reformulations and numerical results for the Newsvendor
and revenue maximization problems.
Generalized Newsvendor Problems . A company orders n goods, where n â‰¥ 1. For each of
the i-th goods, the unit cost of upfront ordering is $ ci with 0 < c i < 1. For a fixed order quantity
0 â‰¤ xi â‰¤ Ri, where Ri refers to the capacity, the company wishes to estimate a worst-case (upper)
bound for the cost of ordering to meet the random demands. If the demand Ï‰i exceeds the
upfront order xi, the company incurs a unit back-ordering cost of $1, or a total back-ordering cost
of $( Ï‰i âˆ’ xi), otherwise, the cost of back-ordering is $0. This results in a total ordering cost of
cixi + max{Ï‰i âˆ’ xi, 0}.
Assume that the demand for each goods is independent and the support is Qn
i=1 â„¦i, where â„¦ i =
[Ï‰i, Ï‰i] with 0 â‰¤ Ï‰i < Ï‰i, i = 1 , . . . , n. Then, the multi-product Newsvendor model reduces
to single-product Newsvendor models, and the worst-case ordering cost can be found by solving
max
ÂµâˆˆPi
{cixi + Eâ„¦i
Âµ [max{Ï‰i âˆ’ xi, 0}]} for all i = 1 , . . . , n, where Pi consists of various moment
constraints.
We focus on the single-product Newsvendor model and suppress the index i âˆˆ { 1, . . . , n}. For
a fixed order quantity 0 â‰¤ x â‰¤ R, the worst-case ordering cost can be found from the following
15

<!-- page 16 -->
moment problem,
max
ÂµâˆˆP
cx + Eâ„¦
Âµ[max{Ï‰ âˆ’ x, 0}], (16)
where â„¦ = [Ï‰, Ï‰] = {v âˆˆ R : F0 + vF1 âª° 0}, Ï‰ < Ï‰, with F0 =

âˆ’Ï‰ 0
0 Ï‰

and F1 =

1 0
0 âˆ’1

.
Another interpretation of (16) at c = 0 comes from pricing a European call option [3, 22]. The
random variable Ï‰ represents the price of the underlying stock, and the fixed value x is regarded
as the strike price. This gives the expected price of Eâ„¦
Âµ[max{Ï‰ âˆ’ x, 0}], and thus a sharp upper
bound max
ÂµâˆˆP
Eâ„¦
Âµ[max{Ï‰ âˆ’ x, 0}] of the price.
Numerically tractable reformulations for (16) are available for a limited choice ofP. For example,
[27] provides a closed-form formula when P specifies known values for the mean and variance;
recently, [12] presents a closed-form formula when P specifies known values for mean plus one
other moment of any order; [3] reformulates (16) as an SDP when P contains the first J mo-
ments; and [33] reformulates (16) as a conic program when the random demand variable follows a
distribution defined by robust statistics. For other related models with tractable reformulations,
see [8].
We study the problem (16) when P1 = {Âµ âˆˆ Pâ„¦ : Eâ„¦
Âµ[Ï‰] â‰¤ Î³1, Eâ„¦
Âµ[Ï‰2] â‰¤ Î³2} and P2 = P1 âˆ© {Âµ âˆˆ
Pâ„¦ : Eâ„¦
Âµ[Ï‰4] â‰¤ Î³3}, where Pâ„¦ is the set of probability measures on â„¦. The set P1 contains upper
bounds for the mean and the second-order moment which is related to the variance, while P2
contains an additional bound for the fourth-order moment which is related to the kurtosis of a
distribution. The mean, variance, and kurtosis of the random demand distribution are usually
accessible in real-world applications.
The standard minimization form for the Newsvendor problem is
min (17) := min
ÂµâˆˆP
Eâ„¦
Âµ
h
min
k=1,2
gk
1(Ï‰)
i
, g 1
1(v) = x âˆ’ v, g 2
1(v) = 0, (17)
and the worst-case expected ordering cost is equal to [ cx âˆ’ min (17)].
The associated SOS optimization problem for (17) when P = P1 is
max (18) := max
Î»âˆˆR2
+Ã—R
Z,Z â€²âˆˆS2
+
âˆ’ Î»1Î³1 âˆ’ Î»2Î³2 âˆ’ Î»3 (18)
s.t. [x + Î»3 âˆ’ tr(ZF0)] + [âˆ’1 + Î»1 âˆ’ tr(ZF1)]v + Î»2v2 âˆˆ Î£2
2(v),
[Î»3 âˆ’ tr(Z â€²F0)] + [Î»1 âˆ’ tr(Z â€²F1)]v + Î»2v2 âˆˆ Î£2
2(v).
Its equivalent SDP representation is
max (19) := max
Î»âˆˆR2
+Ã—R
Z,Z â€²âˆˆS2
+
âˆ’ Î»1Î³1 âˆ’ Î»2Î³2 âˆ’ Î»3 (19)
s.t.

x + Î»3 âˆ’ tr(ZF0) 1
2[âˆ’1 + Î»1 âˆ’ tr(ZF1)]
1
2[âˆ’1 + Î»1 âˆ’ tr(ZF1)] Î»2

âª° 0,

Î»3 âˆ’ tr(Z â€²F0) 1
2[Î»1 âˆ’ tr(Z â€²F1)]
1
2[Î»1 âˆ’ tr(Z â€²F1)] Î»2

âª° 0.
16

<!-- page 17 -->
Proposition 4.1. Assume that (17) admits a minimizer for P = P1 and (Î³1, Î³2, 1) âˆˆ
int{{(Eâ„¦
Âµ[Ï‰], Eâ„¦
Âµ[Ï‰2], Eâ„¦
Âµ[1]) : Âµ âˆˆ X, Âµ âª°B 0} + R2
+ Ã— {0}}. Then, min (17) = max (19).
Proof. By Theorem 3.4, min (17) = max (18). Next, following the approach in Proposition C.1,
the constraint [x+Î»3 âˆ’tr(ZF0)]+ v[âˆ’1+ Î»1 âˆ’tr(ZF1)]+ Î»2v2 âˆˆ Î£2
2(v) is equivalent to the existence
of Q âˆˆ S2
+ such that tr(QB0) = x+ Î»3 âˆ’tr(ZF0), tr(QB1) = âˆ’1+ Î»1 âˆ’tr(ZF1), and tr(QB2) = Î»2,
where B0 =

1 0
0 0

, B1 =

0 1
1 0

, and B2 =

0 0
0 1

. This gives
Q =

x + Î»3 âˆ’ tr(ZF0) 1
2[âˆ’1 + Î»1 âˆ’ tr(ZF1)]
1
2[âˆ’1 + Î»1 âˆ’ tr(ZF1)] Î»2

âˆˆ S2
+.
Similar arguments apply for the other constraint, and hence max (18) = max (19).
Now, we illustrate how one can recover an optimal measure for (17) via the following SDP
min (20) := min
y1,y2âˆˆR3
z1,z2âˆˆR
z1 + z2 (20)
s.t. y 1
1 + y2
1 â‰¤ Î³1, y 1
2 + y2
2 â‰¤ Î³2, y 1
0 + y2
0 = 1,
xy1
0 âˆ’ y1
1 â‰¤ z1, 0 â‰¤ z2,
yk
0 F0 + yk
1 F1 âª° 0,

yk
0 yk
1
yk
1 yk
2

âª° 0, k = 1, 2.
Proposition 4.2. Assume the same conditions as in Proposition 4.1 and max (19) = min (20).
Let (Â¯yk, Â¯zk) âˆˆ R3 Ã— R with Â¯yk = (Â¯yk
0 , Â¯yk
1 , Â¯yk
2) be a minimizer for (20) with Â¯yk
0 > 0, k = 1, 2. Denote
buk := 1
Â¯yk
0
Â¯yk
1 âˆˆ R, k = 1, 2. Then, Â¯y1
01 bu1 + Â¯y2
01 bu2 is a minimizer for (17) for P = P1.
Proof. This follows from Corollary 3.7.
Note that the probability measure optimal for the standard minimization form (17) will also be
optimal for the original Newsvendor maximization problem (16).
In a similar vein, we examine the Newsvendor model with P = P2. Its associated SOS optimiza-
tion problem is given by
max (21) := max
Î»âˆˆR3
+Ã—R
Z,Z â€²âˆˆS2
+
âˆ’ Î»1Î³1 âˆ’ Î»2Î³2 âˆ’ Î»3Î³3 âˆ’ Î»4 (21)
s.t. [x + Î»4 âˆ’ tr(ZF0)] + [âˆ’1 + Î»1 âˆ’ tr(ZF1)]v + Î»2v2 + Î»3v4 âˆˆ Î£2
4(v),
[Î»4 âˆ’ tr(Z â€²F0)] + [Î»1 âˆ’ tr(Z â€²F1)]v + Î»2v2 + Î»3v4 âˆˆ Î£2
4(v).
17

<!-- page 18 -->
Its equivalent SDP representation is
max (22) := max
Î»âˆˆR3
+Ã—R
Z,Z â€²âˆˆS2
+,Q,Qâ€²âˆˆS3
+
âˆ’ Î»1Î³1 âˆ’ Î»2Î³2 âˆ’ Î»3Î³3 âˆ’ Î»4 (22)
s.t. tr(QB0) = x + Î»4 âˆ’ tr(ZF0), tr(Qâ€²B0) = Î»4 âˆ’ tr(Z â€²F0),
tr(QB1) = âˆ’1 + Î»1 âˆ’ tr(ZF1), tr(Qâ€²B1) = Î»1 âˆ’ tr(Z â€²F1),
tr(QB2) = tr(Qâ€²B2) = Î»2,
tr(QB3) = tr(Qâ€²B3) = 0,
tr(QB4) = tr(Qâ€²B4) = Î»3,
where
B0 =
ï£«
ï£­
1 0 0
0 0 0
0 0 0
ï£¶
ï£¸ , B1 =
ï£«
ï£­
0 1 0
1 0 0
0 0 0
ï£¶
ï£¸ , B2 =
ï£«
ï£­
0 0 1
0 1 0
1 0 0
ï£¶
ï£¸ , B3 =
ï£«
ï£­
0 0 0
0 0 1
0 1 0
ï£¶
ï£¸ , B4 =
ï£«
ï£­
0 0 0
0 0 0
0 0 1
ï£¶
ï£¸ .
(23)
Proposition 4.3. Assume that (17) admits a minimizer for P = P2 and (Î³1, Î³2, Î³3, 1) âˆˆ
int{{(Eâ„¦
Âµ[Ï‰], Eâ„¦
Âµ[Ï‰2], Eâ„¦
Âµ[Ï‰4], Eâ„¦
Âµ[1]) : Âµ âˆˆ X, Âµ âª°B 0} + R3
+ Ã— {0}}. Then, min (17) = max (22).
Proof. The proof is similar to Proposition 4.1.
We recover an optimal solution for (17) using the following SDP,
min (24) := min
y1,y2âˆˆR5
z1,z2âˆˆR
z1 + z2 (24)
s.t. y 1
1 + y2
1 â‰¤ Î³1, y 1
2 + y2
2 â‰¤ Î³2, y 1
4 + y2
4 â‰¤ Î³3, y 1
0 + y2
0 = 1,
xy1
0 âˆ’ y1
1 â‰¤ z1, 0 â‰¤ z2,
yk
0 F0 + yk
1 F1 âª° 0,
ï£«
ï£­
yk
0 yk
1 yk
2
yk
1 yk
2 yk
3
yk
2 yk
3 yk
4
ï£¶
ï£¸ âª° 0, k = 1, 2.
Proposition 4.4. Assume the same conditions as in Proposition 4.3 and max (22) = min (24).
Let (Â¯yk, Â¯zk) âˆˆ R5 Ã— R with Â¯yk = (Â¯yk
0 , Â¯yk
1 , Â¯yk
2 , Â¯yk
3 , Â¯yk
4) be a minimizer for (24) with Â¯yk
0 > 0, k = 1, 2.
Denotebuk := 1
Â¯yk
0
Â¯yk
1 âˆˆ R, k = 1, 2. Then, Â¯y1
01 bu1 + Â¯y2
01 bu2 is a minimizer for (17) for P = P2.
Proof. This follows from Corollary 3.7.
We illustrate how the worst-case costs of ordering [ cx âˆ’ max (19)] and [cx âˆ’ max (22)] change by
varying the order quantity x âˆˆ [0, R] = [0, 10] for P1 and P2. The SDPs are modelled by CVX
MATLAB [11] and solved by Mosek [28]. Set â„¦ = [ Ï‰, Ï‰] = [0, 100] for the support, c = 0.1 for the
unit cost of upfront ordering, and Î³1 = Î³2 = Î³3 = 1 for the moment bounds. Figure 2 illustrates
the cost of ordering ( $) for P = P1 (blue) and P = P2 (orange).
18

<!-- page 19 -->
Figure 2: Costs of ordering ( $) for P = P1 (blue) and P = P2 (orange).
The curves in Figure 2 exhibit â€œUâ€ shapes. Before the turning points ( x âˆˆ [0, 1.5811] blue,
x âˆˆ [0, 1.3337] orange), the more upfront orders at cost c = $0.1 the company makes, the more it
saves from back-ordering at cost $1. For larger upfront ordering quantities ( x âˆˆ [1.5811, 10] blue,
x âˆˆ [1.3337, 10] orange) that can largely cover the demand, back-ordering is not needed. The
slope of the curves in these regions is approximately c = 0.1.
Notice that P2 is a subset of P1, Figure 2 confirms that the costs of ordering for P1 (blue) are
at least the costs for P2 (orange), and this leads to a higher minimum cost for P1. This could be
justified that, by knowing more information about the demand, i.e., P = P2, the future demand
can be better predicted, and thus the ordering strategy could be better determined.
The minimum costs of ordering are achieved at the turning points of the curves. Specifically, the
minimum costs are $0.3162 and $0.1778 if the company orders x = 1.5811 and x = 1.3337 units
of goods under the ambiguity constraints P = P1 (blue) and P = P2 (orange), respectively.
For P1 with order quantity x = 1.5811, an optimal probability measure is 0 .11 3.1623 + 0.91 0.0003.
It refers to the discrete distribution with P(Ï‰ = 3.1623) = 0.1 and P(Ï‰ = 0.0003) = 0.9. For P2
with x = 1.3337, an optimal probability measure is 0 .11 1.7782 + 0.91 0.0200.
Revenue Maximization. A merchant supplies a random quantity Ï‰ âˆˆ [0, R] âŠ† R of goods and
sells them to n customers. Each customer offers a different price based on the supply quantity.
The goods can be sold to one customer exclusively at a time, and the merchant wishes to maximize
revenue by selling the goods to the customer who offers the highest price.
This problem of revenue maximization can be formulated as
max
ÂµâˆˆPâ„¦
n
Eâ„¦
Âµ
h
max
k=1,...,n
hk(Ï‰)
i
: Eâ„¦
Âµ[Ï‰] â‰¤ Î³1, Eâ„¦
Âµ[Ï‰2] â‰¤ Î³2
o
, (25)
where hk is the offer price from the k-th customer, k = 1 , . . . , n, and â„¦ = [0 , R], R > 0. The
support can be expressed equivalently as a spectrahedron â„¦ = {v âˆˆ R : F0 + vF1 âª° 0} with
F0 =

0 0
0 R

and F1 =

1 0
0 âˆ’1

.
The standard minimization form is
min (26) := min
ÂµâˆˆPâ„¦
n
Eâ„¦
Âµ[g(Ï‰)] : Eâ„¦
Âµ[Ï‰] â‰¤ Î³1, Eâ„¦
Âµ[Ï‰2] â‰¤ Î³2
o
, (26)
where g is given byg(v) = âˆ’ max
k=1,...,n
hk(v). The maximum expected revenue is equal to [âˆ’ min (26)].
19

<!-- page 20 -->
Denoting fk(v) = âˆ’hk(v), one has g(v) = min
k=1,...,n
fk(v). Here, for each k = 1, . . . , n, we describe
the offer price by
fk(v) =
(
Î±k(v âˆ’ bk)2 + Î²k(v âˆ’ bk)4 + ck, if 0 â‰¤ v â‰¤ bk,
ck, if v > b k, (27)
where Î±k, Î²k, bk â‰¥ 0 and ck â‰¤ âˆ’Î±kb2
k âˆ’ Î²kb4
k. Note that g is a piecewise SOS-convex function with
the following representation:
g(v) = min
k=1,...,2n
max
â„“=1,2
gk
â„“ (v), (28)
where gk
1(v) = gk
2(v) = Î±k(v âˆ’bk)2+Î²k(v âˆ’bk)4+ck, gn+k
1 (v) = âˆ’(Î±kbk +Î²kb3
k)v+(Î±kb2
k +Î²kb4
k +ck),
and gn+k
2 (v) = ck, for k = 1, . . . , n. For details of this representation, see Appendix A.
In fact, (âˆ’fk) where fk is given as in Eqn. (27), or hk in problem (25), corresponds to a combined
quadratic-quartic utility function that is concave, non-decreasing, and smooth [9]. The coefficients
(âˆ’Î±k) and ( âˆ’Î²k) capture the rate at which the customer increases the offer price based on the
supply; bk is the maximum quantity of goods the k-th customer is willing to purchase, beyond
which the customer will no longer be willing to increase the price; and ck which excludes negative
offer prices is the maximum price.
The function in Eqn. (27) reduces to the quadratic model in [13] by setting Î²k = 0, k = 1, . . . , n,
where an approximating upper bound for the maximum revenue is calculated through a convex
program. Our model covers more diverse purchasing behaviours described by the quartic feature.
Suppose that there are three customers with parameters Î±1 = 1, Î±2 = 1, Î±3 = 1
10, Î²1 = 1, Î²2 = 1
16,
Î²3 = 1
100, b1 = 1, b2 = 2, b3 = 4, and c1 = âˆ’5, c2 = âˆ’7, c3 = âˆ’7.5. The supply quantity ranges
between [0 , R] = [0 , 4]. As shown in Figure 3, the piecewise SOS-convex function g is neither
convex nor concave and is not smooth.
Figure 3: Left: negative offer prices (solid lines) and negative revenue (dotted lines). Right:
piecewise SOS-convex function in Eqn. (28).
20

<!-- page 21 -->
Associate (26) with the following SOS optimization problem with n = 3 customers,
max
Î»âˆˆR2
+Ã—R,Î´kâˆˆ[0,1]
Zk,Zâ€²
kâˆˆS2
+,k=1,...,n
âˆ’ Î»1Î³1 âˆ’ Î»2Î³2 âˆ’ Î»3
s.t. [Î±kb2
k + Î²kb4
k + ck + Î»3 âˆ’ tr(ZkF0)] + [âˆ’2Î±kbk âˆ’ 4Î²kb3
k + Î»1 âˆ’ tr(ZkF1)]v+
[Î±k + 6Î²kb2
k + Î»2]v2 âˆ’ 4Î²kbkv3 + Î²kv4 âˆˆ Î£2
4(v), k = 1, . . . , n,
[Î´kÎ±kb2
k + Î´kÎ²kb4
k + ck + Î»3 âˆ’ tr(Z â€²
kF0)] + [âˆ’Î´kÎ±kbk âˆ’ Î´kÎ²kb3
k + Î»1 âˆ’ tr(Z â€²
kF1)]v+
Î»2v2 âˆˆ Î£2
4(v), k = 1, . . . , n,
and its equivalent SDP representation
max (29) := max
Î»âˆˆR2
+Ã—R,Î´kâˆˆ[0,1]
Zk,Zâ€²
kâˆˆS2
+,Qk,Qâ€²
kâˆˆS3
+,k=1,...,n
âˆ’ Î»1Î³1 âˆ’ Î»2Î³2 âˆ’ Î»3 (29)
s.t. for each k = 1, . . . , n,
tr(QkB0) = Î±kb2
k + Î²kb4
k + ck + Î»3 âˆ’ tr(ZkF0),
tr(QkB1) = âˆ’2Î±kbk âˆ’ 4Î²kb3
k + Î»1 âˆ’ tr(ZkF1),
tr(QkB2) = Î±k + 6Î²kb2
k + Î»2,
tr(QkB3) = âˆ’4Î²kbk, tr(QkB4) = Î²k,
tr(Qâ€²
kB0) = Î´kÎ±kb2
k + Î´kÎ²kb4
k + ck + Î»3 âˆ’ tr(Z â€²
kF0),
tr(Qâ€²
kB1) = âˆ’Î´kÎ±kbk âˆ’ Î´kÎ²kb3
k + Î»1 âˆ’ tr(Z â€²
kF1),
tr(Qâ€²
kB2) = Î»2, tr(Qâ€²
kB3) = tr(Qâ€²
kB4) = 0,
where B0, . . . , B4 are given in Eqn. (23).
Proposition 4.5. Assume that (26) admits a minimizer and (Î³1, Î³2, 1) âˆˆ int{{(Eâ„¦
Âµ[Ï‰], Eâ„¦
Âµ[Ï‰2], Eâ„¦
Âµ[1]) :
Âµ âˆˆ X, Âµ âª°B 0} + R2
+ Ã— {0}}. Then, min (26) = max (29).
Proof. The proof is similar to Proposition 4.1.
Similar to the Newsvendor application, higher-order moments can be incorporated, and the re-
sulting SDP reformulation can be obtained.
The maximum revenue is equal to [ âˆ’ max (29)]. When the first and second-order moments of the
supply quantity are at most 2, i.e., Î³1 = Î³2 = 2, the maximum expected revenue is $6.6495.
21

<!-- page 22 -->
We recover an optimal measure via the SDP below,
min
ykâˆˆR5,zkâˆˆR
k=1,...,2n
2nX
k=1
zk (30)
s.t.
2nX
k=1
yk
1 â‰¤ Î³1,
2nX
k=1
yk
2 â‰¤ Î³2,
2nX
k=1
yk
0 = 1,
[Î±kb2
k + Î²kb4
k + ck]yk
0 âˆ’ [2Î±kbk + 4Î²kb3
k]yk
1 + [Î±k + 6Î²kb2
k]yk
2 âˆ’
4Î²kbkyk
3 + Î²kyk
4 â‰¤ zk, k = 1, . . . , n,
[Î±kb2
k + Î²kb4
k + ck]yn+k
0 âˆ’ [Î±kbk + Î²kb3
k]yn+k
1 â‰¤ zn+k, k = 1, . . . , n,
ckyn+k
0 â‰¤ zn+k, k = 1, . . . , n,
yk
0 F0 + yk
1 F1 âª° 0,
ï£«
ï£­
yk
0 yk
1 yk
2
yk
1 yk
2 yk
3
yk
2 yk
3 yk
4
ï£¶
ï£¸ âª° 0, k = 1, . . . ,2n.
Solving (30) for n = 3 gives Â¯y1 = (Â¯y1
0, Â¯y1
1, Â¯y1
2, Â¯y1
3, Â¯y1
4) = (0 , 0, 0, 0, 0), Â¯y2 = (1 , 1.4142, 2, 2.8284, 4),
Â¯y3 = (0, 0, 0, 0, 0), Â¯y4 = (0, 0, 0, 0, 1.5563), Â¯y5 = (0, 0, 0, 0, 1.5871), Â¯y6 = (0, 0, 0, 0, 1.4684), Â¯zk = 0,
k = 1 , 3, 4, 5, 6, and Â¯z2 = âˆ’6.6495. Clearly, Â¯yk
0 â‰¥ 0 and K = {k âˆˆ { 1, . . . ,6} : Â¯yk
0 > 0} = {2}.
Moreover, Â¯zk â‰¥ 0 for all k /âˆˆ K. For h1(v) = v, it is satisfied that P4
Î±=0 Â¯yk
Î±(h1)Î± = Â¯yk
1 = 0 for
all k /âˆˆ K, and for h2(v) = v2, P4
Î±=0 Â¯yk
Î±(h2)Î± = Â¯yk
2 = 0 for all k /âˆˆ K. By Theorem 3.6, an
optimal probability measure for problem (26), and thus for (25) is 1 1.4142, which is the discrete
distribution P(Ï‰ = 1 .4142) = 1. This result can be interpreted by the fact that the maximum
expected revenue $6.6495 can be achieved when the merchant supplies 1 .4142 units of goods.
We want to remark that the assumption Â¯yk
0 > 0, k = 1, . . . , r, in Corollary 3.7 is sufficient but not
necessary for an optimal solution to be Pr
k=1 Â¯yk
0 1 buk. Indeed, with r = 2n = 6, the above revenue
problem offers an example that Â¯yk
0 > 0, k = 1, . . . , r, is not a necessary condition.
Figure 4 (left) shows how the maximum expected revenue changes with respect to the mean bound
Î³1. The bound for the second-order moment is set to be Î³2 = Î³2
1. The curve exhibits an increasing
trend. For a large mean bound Î³1, the company can target the customer offering a higher price,
and thus the revenue is higher. Figure 4 (right), on the contrary, fixesÎ³1 = 2 and increases Î³2. The
curve exhibits an upward shape, which can be justified by the fact that the larger the second-order
moment (and thus the variance), the wider the customers the company can sell products to, and
thus the higher the revenue.
22

<!-- page 23 -->
Figure 4: Maximum expected revenue by varying Î³1 (left) and Î³2 (right).
5 Conclusion and Outlook
We showed how to derive Sum-of-Squares (SOS) reformulations for important classes of infinite-
dimensional moment optimization problems involving piecewise SOS-convex functions. We also
showed how to recover an optimal probability measure from the associated Semi-Definite Program
(SDP) reformulation.
It is worth noting that the class of piecewise SOS-convex functions as given in Definition 2.2
facilitates tractable representations of the corresponding moment problem (Theorem 3.4) and it
is rich enough to cover a broad class of functions encountered in optimization. Our definition 2.2
of piecewise functions may be extended to more general settings where one piece of SOS-convex
polynomial is given at one partition of Rm, in line with the piecewise linear-quadratic function
studied in [31, Definition 10.20]. It would be intriguing to explore the applicability and tractability
of these generalized classes of piecewise functions.
Further, our method in Theorem 2.4 suggests that the numerically tractable SOS representation
for non-negative piecewise SOS-convex functions over a projected spectrahedron can be extended
to more general settings. For instance, it would be of interest to examine similar representations
for piecewise SOS-convex functions over any non-convex sets whose convex hulls are semi-definite-
representable, leading to SDP reformulations or relaxations for broad classes of optimization prob-
lems.
As applications, we presented numerical results for a class of generalized Newsvendor and revenue
maximization problems with higher-order moments by solving their equivalent SDP reformula-
tions. Our approach opens new avenues for further research, such as conic program reformula-
tions for distributionally robust optimization problems [33, 36] involving piecewise SOS-convex
functions, and applications to practical models such as the lot-sizing and product management
problems in the face of uncertain conditions [2]. These problems will be examined in our forth-
coming studies.
Acknowledgment
The authors are grateful to the referees for their helpful comments and valuable suggestions which
have contributed to the final preparation of the paper.
23

<!-- page 24 -->
Declarations
Data availability statement. No data was used for the research described in the article.
Conflict of interest statement . The authors have no conflict of interest to declare that are
relevant to the content of this article.
Appendix A Piecewise SOS-Convex Representation
In this appendix, we present explicit piecewise SOS-convex representations for the examples men-
tioned in the Introduction (Section 1). We note that the representation is, in general, not unique.
â€¢ The truncated â„“1-norm, pÎµ(v) = min{1, Îµ|v|}, Îµ > 0, can be expressed aspÎµ(v) = min
k=1,2
max
â„“=1,2
gk
â„“ (v)
where g1
1(v) = g1
2(v) = 1, g2
1(v) = Îµv, g2
2(v) = âˆ’Îµv. Note that the truncated â„“1-norm is non-
convex and non-smooth. We plot max
â„“=1,2
g1
â„“ (v) versus max
â„“=1,2
g2
â„“ (v) in Figure 5 (left) and the
truncated â„“1-norm in Figure 5 (right).
Figure 5: Piecewise representation (left) and truncated â„“1-norm (right).
â€¢ The piecewise quadratic function, a, b â‰¥ 0, c âˆˆ R,
f(v) =
(
a(v âˆ’ b)2 + c, if 0 â‰¤ v â‰¤ b,
c, if v > b,
can be written as f(v) = min
k=1,2
max
â„“=1,2
gk
â„“ (v) where g1
1(v) = g1
2(v) = a(v âˆ’ b)2 + c, g2
1(v) =
âˆ’abv + ab2 + c, and g2
2(v) = c. We plot max
â„“=1,2
g1
â„“ (v) versus max
â„“=1,2
g2
â„“ (v) in Figure 6 (left) and
the full piecewise function in Figure 6 (right).
24

<!-- page 25 -->
Figure 6: Piecewise representation (left) and the piecewise quadratic function (right).
â€¢ The piecewise quadratic-quartic function from Section 4,
f(v) =
(
Î±(v âˆ’ b)2 + Î²(v âˆ’ b)4 + c, if 0 â‰¤ v â‰¤ b,
c, if v > b,
with Î±, Î², b â‰¥ 0, c âˆˆ R, can be expressed as f(v) = min
k=1,2
max
â„“=1,2
gk
â„“ (v), where g1
1(v) = g1
2(v) =
Î±(v âˆ’ b)2 + Î²(v âˆ’ b)4 + c, g2
1(v) = âˆ’(Î±b + Î²b3)v + (Î±b2 + Î²b4 + c), and g2
2(v) = c. See Figure
7 for illustrations.
Figure 7: Piecewise representation (left) and the piecewise quadratic-quartic function (right).
â€¢ The Huber loss function [33] with parameter Îµ > 0,
HÎµ(v) =
(
1
2 v2, if |v| â‰¤ Îµ,
Îµ|v| âˆ’ 1
2 Îµ2, otherwise,
can be expressed equivalently as HÎµ(v) = min
k=1,2
max
â„“=1,2,3,4
gk
â„“ (v), where g1
â„“ (v) = 1
2 v2, â„“ = 1, . . . ,4,
g2
1(v) = Îµv âˆ’ 1
2 Îµ2, g2
2(v) = âˆ’Îµv âˆ’ 1
2 Îµ2, g2
3(v) = 1
2 Îµv, and g2
4(v) = âˆ’ 1
2 Îµv. See Figure 8 for
illustrations.
25

<!-- page 26 -->
Figure 8: Piecewise representation (left) and Huber loss function (right).
Appendix B Conic Duality in Topological Vector Spaces
This section presents duality results for infinite-dimensional conic linear programs that were used
in Section 3.
Let X and X â€² be real topological vector spaces. They are paired if a bilinear form âŸ¨Â·, Â·âŸ© : X â€² Ã—X â†’
R is defined. For the linear map A : X â†’ RJ, assume that for any Î» âˆˆ RJ, there is a unique
xâ€² âˆˆ X â€² satisfying Î»âŠ¤Ax = âŸ¨xâ€², xâŸ© for all x âˆˆ X. See [32, Assumption A1] for a discussion of this
assumption. For the continuous linear map A : X â†’ RJ, the adjoint mapping Aâˆ— : RJ â†’ X â€² is
defined as âŸ¨Aâˆ—Î», xâŸ© = Î»âŠ¤Ax for any x âˆˆ X, Î» âˆˆ RJ. See [4, 35] for more details on the convexity
of sets and functions.
We consider the problem
min
xâˆˆX
âŸ¨v, xâŸ© (CP)
s.t. x âˆˆ C, âˆ’Ax + b âˆˆ K,
where C âŠ‚ X and K âŠ‚ RJ are convex cones that are closed in the respective topologies, A : X â†’
RJ is a linear map, and v âˆˆ X â€², b âˆˆ RJ.
Associate (CP) with the set D = {x âˆˆ X : âˆ’Ax + b âˆˆ K} and the dual problem
max
Î»âˆˆRJ
âˆ’ Î»âŠ¤b (CD)
s.t. v + Aâˆ—Î» âˆˆ C+, Î» âˆˆ K+,
where C+ = {xâ€² âˆˆ X â€² : âŸ¨xâ€², xâŸ© â‰¥ 0, âˆ€x âˆˆ C} and K+ = {w âˆˆ RJ : wâŠ¤x â‰¥ 0, âˆ€x âˆˆ K}.
We present a strong duality theorem under a more general constraint qualification known as the
Generalized-Sharpened strong Conical Hull Intersection Property (G-S strong CHIP [5]). A pair
of sets {C, D} is said to satisfy the G-S strong CHIP at xâˆ— âˆˆ (C âˆ© D) whenever (C âˆ© D âˆ’ xâˆ—)+ =
(C âˆ’ xâˆ—)+ + âˆªÎ»âˆˆK+{âˆ’Aâˆ—Î» : Î»âŠ¤(âˆ’Axâˆ— + b) = 0}.
Theorem B.1 (Strong duality under G-S strong CHIP). Suppose that xâˆ— is a minimizer
for the problem (CP), the linear form âŸ¨u, Â·âŸ© is continuous at xâˆ— for each u âˆˆ X â€², the linear map
A : X â†’ RJ is continuous, the convex cones C and K are closed in the respective topologies, and
that the pair {C, D} satisfies the G-S strong CHIP at xâˆ—. Then, min (CP) = max (CD), and the
26

<!-- page 27 -->
maximum of (CD) is attained.
Proof. Firstly, we note that, by construction, weak duality holds, that is, min (CP) â‰¥ max (CD).
To see min (CP) â‰¤ max (CD), let xâˆ— be a minimizer of (CP). By the necessary optimality
conditions [35], 0 âˆˆ âˆ‚(âŸ¨v, Â·âŸ©)(xâˆ—) âˆ’ (C âˆ© D âˆ’ xâˆ—)+ = {v} âˆ’ (C âˆ© D âˆ’ xâˆ—)+. This means v âˆˆ
(C âˆ©Dâˆ’xâˆ—)+, and thus v âˆˆ (C âˆ’xâˆ—)++âˆªÎ»âˆˆK+{âˆ’Aâˆ—Î» : Î»âŠ¤(âˆ’Axâˆ—+b) = 0} by the G-S strong CHIP
assumption. Therefore, there exist u âˆˆ (C âˆ’ xâˆ—)+ and Â¯Î» âˆˆ K+ satisfying u = v + Aâˆ—Â¯Î» âˆˆ (C âˆ’ xâˆ—)+,
Â¯Î»âŠ¤(âˆ’Axâˆ— + b) = 0, and
âŸ¨v + Aâˆ—Â¯Î», x âˆ’ xâˆ—âŸ© â‰¥ 0, for all x âˆˆ C. (31)
Since C is a convex cone, we have âŸ¨v + Aâˆ—Â¯Î», xâŸ© â‰¥ 0 for all x âˆˆ C, and so v + Aâˆ—Â¯Î» âˆˆ C+. In
particular, Â¯Î» is feasible for (CD). Moreover, letting x = 0 âˆˆ C in Eqn. (31) gives âŸ¨v + Aâˆ—Â¯Î», xâˆ—âŸ© =
âŸ¨v, xâˆ—âŸ© + Â¯Î»âŠ¤Axâˆ— â‰¤ 0. But Â¯Î»âŠ¤Axâˆ— = Â¯Î»âŠ¤b, which implies âŸ¨v, xâˆ—âŸ© + Â¯Î»âŠ¤b â‰¤ 0. Hence, âˆ’Â¯Î»âŠ¤b â‰¥ âŸ¨v, xâˆ—âŸ©,
and so,
max (CD) = max{âˆ’Î»âŠ¤b : Î» âˆˆ K+, v + Aâˆ—Î» âˆˆ C+} â‰¥ âˆ’ Â¯Î»âŠ¤b â‰¥ âŸ¨v, xâˆ—âŸ© = min (CP).
Therefore, we see that min (CP) = max (CD) and the maximum of (CD) is attained at Â¯Î».
The G-S strong CHIP is the weakest constraint qualification guaranteeing strong (Lagrangian)
duality for convex optimization. For details, see [5] and other references therein. As we see below,
strong duality under the interior point condition [32] is a consequence of Theorem B.1.
Corollary B.2 (Strong duality under interior point condition). Suppose that xâˆ— is a min-
imizer for the problem (CP), the linear form âŸ¨u, Â·âŸ© is continuous at xâˆ— for each u âˆˆ X â€², the linear
map A : X â†’ RJ is continuous, the convex cones C and K are closed in the respective topologies.
If b âˆˆ int(A(C) + K), then, min (CP) = max (CD), and the maximum of (CD) is attained.
Proof. The conclusion will follow from Theorem B.1 if we show that b âˆˆ int(A(C) + K) implies
(C âˆ© D âˆ’ xâˆ—)+ = (C âˆ’ xâˆ—)+ + âˆªÎ»âˆˆK+{âˆ’Aâˆ—Î» : Î»âŠ¤(âˆ’Axâˆ— + b) = 0}.
Note that the inclusion ( C âˆ’ xâˆ—)+ + âˆªÎ»âˆˆK+{âˆ’Aâˆ—Î» : Î»âŠ¤(âˆ’Axâˆ— + b) = 0 } âŠ† (C âˆ© D âˆ’ xâˆ—)+ holds
by construction. Conversely, let u âˆˆ (C âˆ© D âˆ’ xâˆ—)+. By definition, this means âŸ¨u, x âˆ’ xâˆ—âŸ© â‰¥ 0 for
all x âˆˆ (C âˆ© D). Hence, âŸ¨u, xâŸ© â‰¥ âŸ¨ u, xâˆ—âŸ© for all x âˆˆ (C âˆ© D), and xâˆ— is a minimizer of the convex
optimization problem min {âŸ¨u, xâŸ© : x âˆˆ C, âˆ’Ax + b âˆˆ K}. By [32], there exists Î» âˆˆ K+ with
âŸ¨u, xâˆ—âŸ© = âˆ’Î»âŠ¤b and Aâˆ—Î» + u âˆˆ C+. Since âˆ’Axâˆ— + b âˆˆ K and Î» âˆˆ K+, we have Î»âŠ¤(âˆ’Axâˆ— + b) â‰¥ 0.
On the other hand, Î»âŠ¤(âˆ’Axâˆ— + b) = âŸ¨âˆ’Aâˆ—Î» âˆ’ u, xâˆ—âŸ© â‰¤ 0 as xâˆ— âˆˆ C and Aâˆ—Î» + u âˆˆ C+. These
together force Î»âŠ¤(âˆ’Axâˆ— + b) = 0. In addition, âŸ¨Aâˆ—Î» + u, x âˆ’ xâˆ—âŸ© = âŸ¨Aâˆ—Î» + u, xâŸ© â‰¥ 0 for all x âˆˆ C.
This is equivalent toâŸ¨Aâˆ—Î»+u, xâŸ© â‰¥ 0 for all x âˆˆ (C âˆ’xâˆ—), which further implies Aâˆ—Î»+u âˆˆ (C âˆ’xâˆ—)+.
Hence, u âˆˆ (C âˆ’ xâˆ—)+ + âˆªÎ»âˆˆK+{âˆ’Aâˆ—Î» : Î»âŠ¤(âˆ’Axâˆ— + b) = 0}, and the proof is complete.
Appendix C SDP Representation of SOS Problems
It is known that the SOS optimization problem (D) can be expressed equivalently as an SDP. This
appendix provides technical results for Section 3.
27

<!-- page 28 -->
A monomial over v âˆˆ Rm of degree Â¯d is vÎ± = vÎ±1
1 vÎ±2
2 . . . vÎ±m
m with Â¯d = Pm
i=1 Î±i, and Î± =
(Î±1, . . . , Î±m) âˆˆ ({0} âˆª N)m is a multi-index. The canonical basis is
y(v) := (1, v1, . . . , vm, v2
1, v1v2, . . . , v2
m, . . . , v
Â¯d
1, . . . , v
Â¯d
m)âŠ¤,
which is of dimension s(m, Â¯d) :=
 m+ Â¯d
Â¯d

.
Let f be a real polynomial with an even degree d = 2 Â¯d written as f(v) =P
Î±âˆˆN (f)Î±vÎ±, where (f)Î±
is the Î±-th coefficient of f and N = {Î± âˆˆ ({0} âˆª N)m :Pm
i=1 Î±i â‰¤ d}. By [24, Proposition 2.1], f
is SOS if and only if there exists Q âˆˆ Ss(m,d/2)
+ such that f(v) = y(v)âŠ¤Qy(v). This expresses the
coefficients of f(v) as linear equations of the entries in Q. If we write y(v)y(v)âŠ¤ =P
Î±âˆˆN BÎ±vÎ±
for appropriate matrices BÎ± âˆˆ Ss(m,d/2), Î± âˆˆ N , checking whether f is SOS amounts to finding
Q âˆˆ Ss(m,d/2)
+ such that tr(QBÎ±) = (f)Î± for all Î± âˆˆ N .
The following proposition shows that the SOS problem (D) and the SDP program (R) share the
same optimal values.
Proposition C.1. Let gk
â„“ , â„“ = 1, . . . , L, k = 1, . . . , r, hj, j = 1, . . . , J, be defined as in Theorem
3.4. Then, max (D) = max (R).
Proof. The SOS constraints
LX
â„“=1
Î´k
â„“ gk
â„“ (v) +
JX
j=1
Î»jhj(v) + Î»J+1 âˆ’ tr(ZkF0) âˆ’
mX
i=1
vitr(ZkFi) âˆˆ Î£2
d(v), k = 1, . . . , r,
of (D) are equivalent to the existence of Qk âˆˆ Ss(m,d/2)
+ , k = 1, . . . , r, such that
LX
â„“=1
Î´k
â„“ (gk
â„“ )Î± +
JX
j=1
Î»j(hj)Î± + Î»J+1(1)Î± âˆ’ tr(ZkFÎ±) = tr(QkBÎ±), for all Î± âˆˆ N , k = 1, . . . , r,
where F0 := F0 âˆˆ SÎ½, Fei := Fi âˆˆ SÎ½, i = 1, . . . , m, and FÎ±, Î± âˆˆ N \ { 0, e1, . . . , em}, is the zero
matrix. Thus, the conclusion follows.
References
[1] A. A. Ahmadi and P. A. Parrilo. A complete characterization of the gap between convexity and SOS-convexity.
SIAM Journal on Optimization , 23(2):811â€“833, 2013. https://arxiv.org/pdf/1111.4587.pdf.
[2] A. Ben-Tal, L. El Ghaoui, and A. Nemirovski. Robust Optimization, volume 28. Princeton University Press,
Princeton, 2009.
[3] D. Bertsimas and I. Popescu. On the relation between option and stock prices: a convex optimization approach.
Operations Research, 50(2):358â€“374, 2002.
[4] J. F. Bonnans and A. Shapiro. Perturbation Analysis of Optimization Problems . Springer Science & Business
Media, New York, 2013.
[5] N. Chieu, V. Jeyakumar, G. Li, and H. Mohebi. Constraint qualifications for convex optimization without
convexity of constraints: new connections and applications to best approximation. European Journal of
Operational Research, 265(1):19â€“25, 2018.
28

<!-- page 29 -->
[6] E. de Klerk, D. Kuhn, and K. Postek. Distributionally robust optimization with polynomial densities: theory,
models and algorithms. Mathematical Programming, 181:265â€“296, 2020.
[7] E. de Klerk and M. Laurent. A survey of semidefinite programming approaches to the generalized problem
of moments and their error analysis. In World Women in Mathematics 2018 , pages 17â€“56, Switzerland, 2019.
Springer.
[8] E. Delage and Y. Ye. Distributionally robust optimization under moment uncertainty with application to
data-driven problems. Operations Research, 58(3):595â€“612, 2010.
[9] H. U. Gerber and G. Pafum. Utility functions: from risk theory to finance. North American Actuarial Journal,
2(3):74â€“91, 1998.
[10] J. Goh and M. Sim. Distributionally robust optimization and its tractable approximations. Operations
Research, 58(4):902â€“917, 2010.
[11] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 2.0 beta. http:
//cvxr.com/cvx, 2020. Accessed: 11 Aug 2023.
[12] J. Guo, S. He, B. Jiang, and Z. Wang. A unified framework for generalized moment problems: a novel
primal-dual approach. https: // arxiv. org/ pdf/ 2201. 01445. pdf, 2022.
[13] S. Han, M. Tao, U. Topcu, H. Owhadi, and R. M. Murray. Convex optimal uncertainty quantification. SIAM
Journal on Optimization , 25(3):1368â€“1387, 2015.
[14] G. A. Hanasusanto, V. Roitch, D. Kuhn, and W. Wiesemann. A distributionally robust perspective on
uncertainty quantification and chance-constrained programming.Mathematical Programming, 151:35â€“62, 2015.
[15] J. W. Helton and J. Nie. Structured semidefinite representation of some convex sets. In 2008 47th IEEE
Conference on Decision and Control , pages 4797â€“4800. IEEE, 2008.
[16] J. W. Helton and J. Nie. Semidefinite representation of convex sets. Mathematical Programming, 122:21â€“64,
2010.
[17] Q. Y. Huang and V. Jeyakumar. A distributional Farkasâ€™ lemma and moment optimization problems with
no-gap dual semi-definite programs. Optimization Letters, pages 1â€“16, 2024.
[18] V. Jeyakumar, G. M. Lee, J. H. Lee, and Q. Y. Huang. Sum-of-squares relaxations in robust DC optimization
and feature selection. Journal of Optimization Theory and Applications , 200(1):308â€“343, 2024.
[19] V. Jeyakumar and G. Li. Exact SDP relaxations for classes of nonlinear semidefinite programming problems.
Operations Research Letters, 40(6):529â€“536, 2012.
[20] V. Jeyakumar, G. Li, and J. Vicente-PÂ´ erez. Robust SOS-convex polynomial optimization problems: exact
SDP relaxations. Optimization Letters, 9:1â€“18, 2015.
[21] V. Jeyakumar and J. Vicente-PÂ´ erez. Dual semidefinite programs without duality gaps for a class of convex
minimax programs. Journal of Optimization Theory and Applications , 162:735â€“753, 2014.
[22] J. B. Lasserre. A semidefinite programming approach to the generalized problem of moments. Mathematical
Programming, 112:65â€“92, 2008.
[23] J. B. Lasserre. Convexity in semialgebraic geometry and polynomial optimization. SIAM Journal on Opti-
mization, 19(4):1995â€“2014, 2009.
[24] J. B. Lasserre. Moments, Positive Polynomials and Their Applications , volume 1. World Scientific, London,
2009.
[25] J. B. Lasserre. An Introduction to Polynomial and Semi-Algebraic Optimization , volume 52. Cambridge
University Press, Cambridge, 2015.
29

<!-- page 30 -->
[26] H. A. Le Thi, X. T. Vo, and T. Pham Dinh. Feature selection for linear SVMs under uncertain data: robust
optimization based on difference of convex functions algorithms. Neural Networks, 59:36â€“50, 2014.
[27] A. W. Lo. Semi-parametric upper bounds for option prices and expected payoffs. Journal of Financial
Economics, 19(2):373â€“387, 1987.
[28] MOSEK ApS. The mosek optimization toolbox for matlab manual. version 10.1. https://docs.mosek.com/
10.0/toolbox/index.html#, 2024. Accessed: 23 Apr 2024.
[29] T. Netzer and D. Plaumann. Geometry of Linear Matrix Inequalities . Springer, 2023.
[30] R. T. Rockafellar and S. Uryasev. Optimization of conditional value-at-risk. Journal of Risk , 2:21â€“42, 2000.
[31] R. T. Rockafellar and R. J.-B. Wets. Variational Analysis, volume 317. Springer Science & Business Media,
2009.
[32] A. Shapiro. On duality theory of conic linear problems. Semi-Infinite Programming: Recent Advances, pages
135â€“165, 2001.
[33] W. Wiesemann, D. Kuhn, and M. Sim. Distributionally robust convex optimization. Operations Research,
62(6):1358â€“1376, 2014.
[34] H. Xu, Y. Liu, and H. Sun. Distributionally robust optimization with matrix moment constraints: Lagrange
duality and cutting plane methods. Mathematical Programming, 169:489â€“529, 2018.
[35] C. Zalinescu. Convex Analysis in General Vector Spaces . World Scientific, Singapore, 2002.
[36] J. Zhen, D. Kuhn, and W. Wiesemann. A unified theory of robust and distributionally robust optimization
via the primal-worst-equals-dual-best principle. Operations Research, 2023.
30